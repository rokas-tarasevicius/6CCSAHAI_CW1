{
    "data/raw/NNs.pdf": {
        "metadata": {
            "file_name": "NNs.pdf",
            "file_type": "pdf",
            "content_length": 16065,
            "language": "en",
            "extraction_timestamp": "2025-11-27T18:30:44.819502+00:00",
            "timezone": "utc"
        },
        "content": "Simple Neural    Feedforward networks for\nNetworks and     simple classification\nNeural\nLanguage\nModels\n---\nUse cases for feedforward networks\n\nLet's consider 2 (simplified) sample tasks:\n 1.  Text classification\n 2.  Language modeling\n\nState of the art systems use more powerful neural\nclassifiers like BERT/MLM classifiers, but simple models\nwill introduce some important ideas\n\n                                                 43\n---\nClassification: Sentiment Analysis\n\nWe could do exactly what we did with logistic\nregression\nInput layer are binary features as before\nOutput layer is 0 or 1                   U   \u03c3\n\n                                         W\n\n                                         x\u2081    x\u2099\n---\n+ or \u2212 to a review document doc. We\u2019ll represent each input ob\nfeatures x     ...x  of the input shown in the following table; Fig. 5.2\nSentiment Features\n 1             6\nin a sample mini test document.\n\n Var                 Definition                             Value i\n x1                  count(positive lexicon) \u2208 doc)         3\n x                   count(negative lexicon) \u2208 doc)         2\n x2                  \u21e2 1  if \u201cno\u201d \u2208 doc                     1\n          3          0    otherwise\n x                   count(1st and 2nd pronouns \u2208 doc)      3\n x4                  \u21e2 1  if \u201c!\u201d \u2208 doc                      0\n          5          0    otherwise\n x6                  log(word count of doc)                 ln(66)\n\n                                                          45\n---\nFeedforward nets for simple classification\n\n Logistic      W \u03c3    2-layer               U    \u03c3\n Regression                      feedforward\n\n               x\u2081     x\u2099         network    W\n               f\u2081    f\u2082    f\u2099               x\u2081     x\u2099\n                                            f\u2081    f\u2082    f\u2099\nJust adding a hidden layer to logistic regression       46\n                     46\n\n\u2022     allows the network to use non-linear interactions between features\n\u2022     which may (or may not) improve performance.\n---\n Reminder: Multiclass Outputs\n\n What if you have more than two output classes?\n \u25e6  Add more output units (one for each class)\n \u25e6  And use a \u201csoftmax layer\u201d\n\nsoftmax(zi)  eZi  1\u2264i\u2264D      U\n             =2$\n             k\n\n                             W\n\n                             x\u2081               x\u2099\n                                                47\n---\nThe output layer\n\n\u0177 could have two nodes (one each for positive and\nnegative), or 3 nodes (positive, negative, neutral).\n\u2022     \u0177\u2081 estimated probability of positive sentiment\n\u2022     \u0177\u2082 probability of negative\n\u2022     \u0177\u2083 probability of neutral\n---\nent, \u02c6                             \u02c6\n     y2 the probability of negative and y3 the probability of neutral\nequations would be just what we saw above for a 2-layer network (a\n          Equations for NN classification with hand features\n ntinue to use the s to stand for any non-linearity, whether sigmo\n ).\n\n          x  = [x1 , x2 , ...xd ]  (each xi is a hand-designed feature)\n          h  = s (Wx + b)\n          z  = Uh\n   \u02c6\n   y         = softmax(z)\n\n10 shows a sketch of this architecture. As we mentioned earlier, ad\nlayer to our logistic regression classifier allows the network to repr\n---\ndessert  wordcount               x1            h1            ^\n         =3                                    h2            y1  p(+)\n\nwas            positive lexicon  x2                          ^\n               words = 1                       h3            y2  p(-)\n\n                                                             ^\ngreat  count of \u201cno\u201d             x3            \u2026             y3  p(neut)\n            = 0                             hdh\n\nInput words                      x     W       h             U  y\n                            [d\u2a091]      [dh\u2a09d]  [dh\u2a091]  [3\u2a09dh]   [3\u2a091]\n\n                            Input layer        Hidden layer     Output layer\n                            d=3 features                        softmax\n---\nVectoring for parallelizing inference\n\nWe would like to efficiently classify the whole test\nset of m observations.\nSo we vectorize: pack all the input features into X\n\u2022     Each row x(i) of X is a row vector with all the\n      features for example x(i)\n\u2022     Feature dimensionality is d, X is [m x d]\n---\nBecause we are now modeling each input as a row vector rather than a colum\ntor, we also need to slightly modify Eq. 6.19. X is of shape [m \u21e5 d ] and W is\n e [d  Slight changes to equations\n    h \u21e5 d ], so we\u2019ll reorder how we multiply X and W and transpose W so th\n ectly multiply to yield a matrix H of shape [m \u21e5 dh ]. 1\nThe    Each input is now a row vector\n     bias vector b from Eq. 6.19 of shape [1 \u21e5 dh ] will now have to be replicat\na matrix of shape [m \u21e5 dh ]. We\u2019ll need to similarly reorder the next step a\nspose \u2022    X is of shape [m x d]  \u02c6\n       U. Finally, our output matrix Y will be of shape [m \u21e5 3] (or more ge\nly [m \u2022    W is of shape [d x d]\n       \u21e5 d ], where d is the number of output classes), with each row i of o\n           o \u02c6    o   h            \u02c6 (i)\n ut matrix Y consisting of the output vector y  .\u2018 Here are the final equations f\n      \u2022    We'll need to do some reordering and transposing\n puting the output class distribution for an entire test set:\n      \u2022    Bias vector b that used to be [1 x d\u2095] is now [m x d\u2095]\n                            H = s (XW| + b)\n                            Z = HU|\n                            \u02c6\n                            Y = softmax(Z)                       (6.2\n---\nSimple Neural    Feedforward networks for\nNetworks and     simple classification\nNeural\nLanguage\nModels\n---\nSimple Neural    Embeddings as input to\nNetworks and     feedforward classifiers\nNeural\nLanguage\nModels\n---\n Even better: representation learning\n\nThe real power of deep learning comes   U \u03c3\nfrom the ability to learn features from\nthe data                                W\nInstead of using hand-built human-\nengineered features for classification  x\u2081    x\u2099\nUse learned representations like        e\u2081    e\u2082    e\u2099\nembeddings!\n\n                                                    55\n---\nEmbedding matrix E\n\n\u2022     An embedding is a vector of dimension [1 x d]\n      that represents the input token.\n\u2022     An embedding matrix E is a dictionary, one row\n      per token of vocab V\n\u2022     E has shape [|V| \u00d7 d]\n\u2022     Embedding matrices are central to NLP; they\n      represent input text in LLMs and all NLP tools\n---\nText classification from embeddings\n\n\u2022     Given tokenized input: dessert was great\n\u2022     Select the embedding vectors from E:\n     1.  Convert BPE tokens into vocabulary indices\n      \u2022  w = [3, 9824, 226]\n     2.  Use indexing to select the corresponding rows from E\n      \u2022  row 3, row 4000, row 10532\n---\n                                  Another way to think about selecting token embeddings from the\nAnother way to think of indexing from E\n                          matrix is to represent input tokens as one-hot vectors of shape [1 \u21e5 |V\n       one-hot vector     one dimension for each word in the vocabulary. Recall that in a one-h\n                          the elements are 0 except one, the element whose dimension is the w\n\n\u2022     Treat each in the vocabulary, which has value 1. So if the word \u201cdessert\u201d has in\n                         input word as one-hot vector\n                          vocabulary, x3 = 1, and xi = 0 8i = 3, as shown here:\n   \u2022   If dessert is index 3:     [0 0 1 0 0 0 0 ... 0 0 0 0]\n                                  1 2 3 4 5 6 7 ...      ... |V|\n\n\u2022     Multiply it by Multiplying by a one-hot vector that has only one non-zero element x\n                                  E to select out the embedding for\n                          selects out the relevant row vector for word i, resulting in the embeddin\n      dessert as depicted in Fig. 6.11.                  d\n\n       3                  |V|     \u2715 3 3  |V|              = 3 1  d             d  d\n     1 0 0 1 0 0 0 0 \u2026 0 0 0 0      1 0 0 1 0 0 0 0 \u2026 0 E  \u2715     E             =  1\n                                         0 0 0\n\n                                      |V|                     |V|\n                          Figure 6.11  Selecting the embedding vector for word V by multiplying th\n---\nCollecting embeddings from E for N words\n\n                              d\n                    |V|                 d\n\n   0 0 1 0 0 0 0 \u2026 0 0 0 0    \u2715  =\n   0 0 0 0 0 0 0 \u2026 0 0 1 0       E\n   1 0 0 0 0 0 0 \u2026 0 0 0 0\n\nN  0 0 0 0 1 0 \u2026                 |  |    N\n   0 \u2026 0 0 0 0                   V\n\nGiven window of N tokens, represented by N [1 \u00d7 d]\nembeddings\nNeed to return a single class (e.g., pos or neg)\n---\nText comes in different lengths\nGiven window of N tokens, represented by N [1 \u00d7 d]\nembeddings, return a single class (e.g., pos or neg)\n1.  Concatenate all the inputs into one long vector of\n    shape [1\u00d7dN], i.e. input is length N of longest review\n   \u2022     If shorter then pad with zero embeddings\n   \u2022     Truncate if you get longer reviews at test time\n2.  Pool the inputs into a single short [1 \u00d7 d] vector. A\n    single \"sentence embedding\" (the same\n    dimensionality as a word) to represent all the\n    words. Less info, but very efficient and fast.\n---\n   For example, for a text with N input words/tokens w1 , ..., wN , we want to turn\n  A pooling function is a way to turn a set of embeddings into a single embeddin\nhe N row embeddings e(w ), ..., e(w ) (each of dimensionality d ) into a single\n    Pooling          1              N\n  For example, for a text with N input words/tokens w1 , ..., wN , we want to tu\n mbedding also of dimensionality d .\ne N row embeddings e(w1 ), ..., e(wN ) (each of dimensionality d ) into a sing\n   There are various ways to pool. The simplest is mean-pooling: taking the mean\n bedding also of dimensionality d .\n y summing the embeddings and then dividing by N :\n    Intuition: exact position not so important for sentiment.\n  There are various ways to pool. The simplest is mean-pooling: taking the mea\n summing the embeddings and then dividing by N :\n                                      N\n    We'll just do some                X\n                          sort of averaging of all the vectors.\n                          xmean = 1    e(wi )                       (6.21)\n    Mean pooling:                   N N\n                                    1 X\n ere are the              xmean = N i=1 e(wi )                      (6.2\n                equations for this classifier assuming mean pooling:\n                                      i=1\n re are the       x = mean(e(w ), e(w ), . . . , e(w ))\n                equations for this classifier assuming mean pooling:\n                                    1    2           n\n                  h = s (xW + b)\n                 x  = mean(e(w1 ), e(w2 ), . . . , e(wn ))\n                  z = hU\n                 h  = s (xW + b)\n    \u02c6\n    y               = softmax(z)                                    (6.22)\n---\nPooling                           p(+)  p(-) p(neut)                  Output probabilities\n                                  ^     ^    ^\n                                  y1    y2   y3         y          [1\u2a093]  Output layer softmax\n\n                                                          U [d\u02b0\u2a093]          weights\n\n                                h1    h2    h3  \u2026 hdh   h [1\u2a09d\u2095]          Hidden layer\n\n                                                                   W [d\u2a09\u1d48\u2095] weights\n                                                        x          [1\u2a09d]  Input layer\n\n                                          +  pooling                      pooled embedding\n           embedding for \u201cdessert\u201d\n           embedding for \u201cwas\u201d                                        N\u2a09d   embeddings\n           embedding for \u201cgreat\u201d\n\n           E                         E                  E             |V|\u2a09d  E matrix\n           1    3                   |V|  1  524  |V|    1  902     |V|    N\u2a09|V|  shared across words\n           0 0  1               0 0                                              one-hot vectors\n                                         0 0  0 1  0 0\n           \u201cdessert\u201d = V3                \u201cwas\u201d = V524   0 0  0  1  0 0\n                                                        \u201cgreat\u201d = V902\n\n           dessert                            was            great         Input words\n---\nSimple Neural    Embeddings as input to\nNetworks and     feedforward classifiers\nNeural\nLanguage\nModels\n---\nSimple Neural    Neural language modeling with\nNetworks and     feedforward networks\nNeural\nLanguage\nModels\n---\nNeural Language Models (LMs)\n\nLanguage Modeling: Calculating the probability of the\nnext word in a sequence given some history.\n\u2022     We've seen N-gram based LMs\n\u2022     But neural network LMs far outperform n-gram\n      language models\nState-of-the-art neural LMs are based on more\npowerful neural network technology like Transformers\nBut simple feedforward LMs introduce many of the\nimportant concepts\n                                                  65\n---\nvast amounts of energy to train, and are less interpretable than\n    Simple feedforward Neural Language Models\n for some smaller tasks an n-gram language model is still the righ\nforward neural language model is a feedforward network that\n e  t  Task: predict next word w\n       a representation of some number of previous words (w  , w\n       given prior words w t    , w , w , \u2026    t \u22121\n s a probability distribution over possible next words. Thus\u2014lik\n               t-1                  t-2  t-3\n  the Problem: Now we\u2019re dealing with sequences of\n       feedforward neural LM approximates the probability of a w\n       arbitrary length.\n rior context  P(wt |w1:t \u22121 ) by approximating based on the N \u2212 1\n       Solution: Sliding windows (of fixed length)\n\n       P(wt |w1 , . . . , wt \u22121 ) \u2248 P(wt |wt \u2212N +1 , . . . , wt \u22121 )\n\nwing examples we\u2019ll use a 4-gram example, so we\u2019ll show a neur\n                                                                    66\n---\n    Inference in a Feedforward Language Model\n\n    p(wt=aardvark|w\u209c\u208b\u2083,w\u209c\u208b\u2082,w\u209c\u208b\u2081)     p(wt=do|\u2026) p(wt=fish|\u2026)  p(wt=zebra|\u2026)\n\n    ^  \u2026                           ^      \u2026      ^    \u2026 ^  \u2026 ^\noutput layer y  y1                 y34           y42  y35102   y|V|  1\u2a09|V|\n   softmax                                  U                  dh\u2a09|V|\n\n       hidden layer h    h1  h2    h3  \u2026 hdh                                      1\u2a09dh\n\n                                   W                                Nd\u2a09dh\n\n    embedding layer e                                                             1\u2a09Nd\nE is shared              E         E                             E  |V|\u2a09d\nacross words             1  35  |V|  1  992  |V|  1                 451  |V|      N\u2a09|V|\nInput layer\n  one-hot                0 0  1  0 0    0 0  0 1  0 0    0 0     0  1    0 0\n  vectors                \u201cfor\u201d = V35    \u201call\u201d = V992     \u201cthe\u201d = V451\n\n    ...\n    \u2026 and thanks                 for         all         the                ?     \u2026\n\n                              wt-3           wt-2        wt-1               wt         67\n---\n hot   Feedforward language model equations\n       input vectors for each input context word, are:\n\n       e        = [Ex\u209c\u2212\u2083 ; Ex\u209c\u2212\u2082 ; Ex\u209c\u2212\u2081 ]\n       h        = s (We + b)\n             z  = Uh\n   \u02c6\n   y            = softmax(z)                          (6.2\n       So \u0177    is the probability of the next word w being\nsemicolons to mean concatenation of vectors, so we form the\n       V 42                               t\ne by     = fish\n       concatenating the 3 embeddings for the three context vecto\n       42\nthis idea of using neural networks to do language modeling in\n---\nWhy Neural LMs work better than N-gram LMs\n\nTraining data:\nWe've seen: I have to make sure that the cat gets fed.\nNever seen: dog gets fed\nTest data:\nI forgot to make sure that the dog gets ___\nN-gram LM can't predict \"fed\"!\nNeural LM can use similarity of \"cat\" and \"dog\"\nembeddings to generalize and predict \u201cfed\u201d after dog\n---\nSimple Neural    Neural language modeling with\nNetworks and     feedforward networks\nNeural\nLanguage\nModels\n\n",
        "quiz": [
            {
                "question_text": "What is the primary purpose of a feedforward neural network in the context of simple classification?",
                "answers": [
                    {
                        "text": "To perform simple classification tasks by processing input data through layers to produce output classifications",
                        "is_correct": true,
                        "explanation": "The concept description and context explicitly state that feedforward neural networks are used for simple classification tasks, processing input data through layers to produce classifications."
                    },
                    {
                        "text": "To generate new data samples based on learned patterns",
                        "is_correct": false,
                        "explanation": "This describes a generative model, not the primary purpose of a feedforward neural network for classification."
                    },
                    {
                        "text": "To predict continuous values for regression problems",
                        "is_correct": false,
                        "explanation": "This describes regression tasks, not the classification tasks that feedforward networks are used for in this context."
                    },
                    {
                        "text": "To analyze and interpret the structure of neural networks",
                        "is_correct": false,
                        "explanation": "This describes a meta-analysis task, not the primary purpose of a feedforward neural network for classification."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the two sample tasks considered for feedforward networks in the provided context?",
                "answers": [
                    {
                        "text": "Text classification and language modeling",
                        "is_correct": true,
                        "explanation": "The content explicitly lists these two tasks as sample tasks for feedforward networks."
                    },
                    {
                        "text": "Image recognition and speech synthesis",
                        "is_correct": false,
                        "explanation": "These tasks are not mentioned in the provided context."
                    },
                    {
                        "text": "Data clustering and anomaly detection",
                        "is_correct": false,
                        "explanation": "These tasks are not listed as sample tasks for feedforward networks in the provided context."
                    },
                    {
                        "text": "Time series forecasting and reinforcement learning",
                        "is_correct": false,
                        "explanation": "These tasks are not mentioned in the provided context."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the output layer in a simple classification task using logistic regression?",
                "answers": [
                    {
                        "text": "A single node with a sigmoid activation function",
                        "is_correct": true,
                        "explanation": "The content context specifies that for a simple classification task using logistic regression, the output layer is a single node with a sigmoid activation function."
                    },
                    {
                        "text": "Multiple nodes with softmax activation function",
                        "is_correct": false,
                        "explanation": "This is used for multiclass classification, not simple binary classification with logistic regression."
                    },
                    {
                        "text": "A single node with a ReLU activation function",
                        "is_correct": false,
                        "explanation": "ReLU is not used in logistic regression; sigmoid is the correct activation function for binary classification."
                    },
                    {
                        "text": "A single node with a linear activation function",
                        "is_correct": false,
                        "explanation": "Linear activation is not used in logistic regression; sigmoid is the correct activation function for binary classification."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What does the variable x1 represent in the sentiment features table?",
                "answers": [
                    {
                        "text": "The count of positive lexicon words in the document",
                        "is_correct": true,
                        "explanation": "The concept description explicitly defines x1 as the count of positive lexicon words in the document."
                    },
                    {
                        "text": "The count of negative lexicon words in the document",
                        "is_correct": false,
                        "explanation": "This is the definition of x2, not x1, as per the sentiment features table."
                    },
                    {
                        "text": "The presence of the word 'no' in the document",
                        "is_correct": false,
                        "explanation": "This is the definition of x3, not x1, as per the sentiment features table."
                    },
                    {
                        "text": "The log of the word count of the document",
                        "is_correct": false,
                        "explanation": "This is the definition of x6, not x1, as per the sentiment features table."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What does the variable x2 represent in the sentiment features table?",
                "answers": [
                    {
                        "text": "A binary indicator of whether the word 'no' appears in the document",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that x2 is 1 if 'no' is in the document, and 0 otherwise."
                    },
                    {
                        "text": "The count of negative lexicon words in the document",
                        "is_correct": false,
                        "explanation": "The count of negative lexicon words is represented by x2, not x2."
                    },
                    {
                        "text": "The count of first and second pronouns in the document",
                        "is_correct": false,
                        "explanation": "The count of first and second pronouns is represented by x3, not x2."
                    },
                    {
                        "text": "A binary indicator of whether an exclamation mark appears in the document",
                        "is_correct": false,
                        "explanation": "The presence of an exclamation mark is represented by x4, not x2."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the purpose of adding a hidden layer to logistic regression?",
                "answers": [
                    {
                        "text": "To allow the network to use non-linear interactions between features",
                        "is_correct": true,
                        "explanation": "The content explicitly states that adding a hidden layer to logistic regression allows the network to use non-linear interactions between features."
                    },
                    {
                        "text": "To increase the number of input features",
                        "is_correct": false,
                        "explanation": "The content does not mention that adding a hidden layer increases the number of input features."
                    },
                    {
                        "text": "To reduce the computational complexity of the model",
                        "is_correct": false,
                        "explanation": "The content does not suggest that adding a hidden layer reduces computational complexity."
                    },
                    {
                        "text": "To enable the model to handle more output classes",
                        "is_correct": false,
                        "explanation": "The content states that adding more output units handles more classes, not adding a hidden layer."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the output layer in a neural network classification task with hand features?",
                "answers": [
                    {
                        "text": "A softmax layer with multiple nodes, one for each class",
                        "is_correct": true,
                        "explanation": "The output layer in a neural network classification task with hand features uses a softmax layer with multiple nodes, one for each class, to output probabilities for each class."
                    },
                    {
                        "text": "A single node with a sigmoid activation function",
                        "is_correct": false,
                        "explanation": "This is incorrect because the output layer for classification tasks typically requires multiple nodes for each class, not a single node."
                    },
                    {
                        "text": "A hidden layer with non-linear activation",
                        "is_correct": false,
                        "explanation": "This is incorrect because the output layer is specifically designed to produce the final classification probabilities, not to perform non-linear transformations."
                    },
                    {
                        "text": "A linear layer with no activation function",
                        "is_correct": false,
                        "explanation": "This is incorrect because the output layer requires an activation function like softmax to convert the outputs into probabilities."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ],
        "summary": "The document \"NNs.pdf\" discusses the application of simple feedforward neural networks for text classification and language modeling. It introduces the concept of using neural networks for sentiment analysis, where input features are designed to represent aspects of a text document, and the network's output predicts the sentiment class. The document also explains how adding a hidden layer to logistic regression allows for non-linear interactions between features, potentially improving performance, and how to handle multiclass outputs using a softmax layer."
    },
    "data/raw/LLMs.pdf": {
        "metadata": {
            "file_name": "LLMs.pdf",
            "file_type": "pdf",
            "content_length": 16593,
            "language": "en",
            "extraction_timestamp": "2025-11-27T18:31:17.575137+00:00",
            "timezone": "utc"
        },
        "content": "Large       Introduction to Large Language\nLanguage    Models\nModels\n---\nLarge language models\n\nComputational agents that can interact\nconversationally with people using natural language\nLLMS have revolutionized the field of NLP and AI\n---\nLanguage models\n\n\u2022       Remember the simple n-gram language model\n   \u2022     Assigns probabilities to sequences of words\n   \u2022     Generate text by sampling possible next words\n   \u2022     Is trained on counts computed from lots of text\n\u2022       Large language models are similar and different:\n   \u2022     Assigns probabilities to sequences of words\n   \u2022     Generate text by sampling possible next words\n   \u2022     Are trained by learning to guess the next word\n---\n Fundamental intuition of large language models\n\nText contains enormous amounts of knowledge\nPretraining on lots of text with all that knowledge is\n what gives language models their ability to do so\n much\n---\nWhat does a model learn from pretraining?\n\n\u2022     With roses, dahlias, and peonies, I was\n      surrounded by flowers\n\u2022     The room wasn't just big it was enormous\n\u2022     The square root of 4 is 2\n\u2022     The author of \"A Room of One's Own\" is Virginia\n      Woolf\n\u2022     The doctor told me that he\n---\n    What is a large language model?\n    A neural network with:\n    Input: a context or prefix,\n    Output: a distribution over possible next words\n\n                                                   p(w|context)\n                               output              all     .44\n\n                                                the        .33\n                                               your        .15\n    Transformer (or other decoder)             that        .08\n\n input                                ?\ncontext  So  long  and    thanks  for\n---\nLLMs can generate!\n\nA model that gives a probability distribution over next words can generate\nby repeatedly sampling from the distribution\n\n                  p(w|context)\n                  output                  all    .44\n\n                            the                  .33\n                                         your    .15\nTransformer (or other decoder)    that           .08\n                                  \u2026               \u2026\n\nSo  long  and  thanks  for  all                  p(w|context)\n                                   output        the   .77\n\n                                                 your  .22\n                                              our      .07\n          Transformer (or other decoder)       of      .02\n                                                 \u2026     \u2026\n\n          So  long  and  thanks    for  all  the\n---\nThree architectures for large language models\nw w                                          w\n\nw w w w w\n\nw  w     w w w   w  w  w w w   w  w w\n\n   Decoder                        Encoder-Decoder\nDecoders         Encoders      Encoder-decoders\n                    Encoder\n\nGPT, Claude,     BERT family,  Flan-T5, Whisper\nLlama            HuBERT\nMixtral\n---\nDecoders                                    w w w w        w\n\n                                            w  w  w w      w  w\nWhat most people think of when we say LLM Decoder\n\u2022     GPT, Claude, Llama, DeepSeek, Mistral\n\u2022     A generative model\n\u2022     It takes as input a series of tokens, and iteratively\n      generates an output token one at a time.\n\u2022     Left to right (causal, autoregressive)\n---\nEncoders              w     w w w w\n\n\u2022  Masked Language Models (MLMs)\n                      w     w  w w w  w  w  w  w     w\n\u2022  BERT family              Decoder      Encoder\n\n\u2022  Trained by predicting words from surrounding\n   words on both sides\n\u2022  Are usually finetuned (trained on supervised data)\n   for classification tasks.\n---\nEncoder-Decoders    w w w\nw w  w  w w\n\n\u2022          w  w  w  w w  w  w  w  w w  w  w  w\n    Trained to map from one sequence to another\n\u2022   Very      Decoder       Encoder       Encoder-Decoder\n           popular for:\n   \u2022     machine translation (map from one language to\n         another)\n   \u2022     speech recognition (map from acoustics to words)\n---\nLarge       Introduction to Large Language\nLanguage    Models\nModels\n---\nLarge       Pretraining Large Language\nLanguage    Models\nModels\n---\n    Three stages of training in LLMs\n\n    Instruction Data    Preference Data\n\nPretraining    Label sentiment of this sentence:    Human: How can I embezzle money?\n    The movie wasn\u2019t that great\n    Data    Summarize: Hawaii Electric urges       Assistant: Embezzling is a\n        caution as crews replace a utility pole    felony, I can't help you\u2026\n             overnight on the highway from\u2026         Assistant: Start by creating\n\n    Translate English to Chinese:                   fake expense reports...\n    When does the flight arrive?\n\n    1. Pretraining    2. Instruction    3. Preference\n                         Tuning            Alignment\n\nPretrained    Instruction    Aligned LLM\n   LLM         Tuned LLM\n---\nPretraining\n\nThe big idea that underlies all the amazing\nperformance of language models\n\nFirst pretrain a transformer model on enormous\namounts of text\nThen apply it to new tasks.\n---\nSelf-supervised training algorithm\n\nWe train them to predict the next word!\n1. Take a corpus of text\n2. At each time step t\n i.      ask the model to predict the next word\n ii.     train the model using gradient descent to minimize the\n         error in this prediction\n\n \"Self-supervised\" because it just uses the next word as the\n label!\n---\nIntuition of language model training: loss\n\n\u2022         Same loss function: cross-entropy loss\n     \u2022     We want the model to assign a high probability to true\n           word w\n     \u2022     = want loss to be high if the model assigns too low a\n           probability to w\n\u2022         CE Loss: The negative log probability that the model\n          assigns to the true next word w\n     \u2022     If the model assigns too low a probability to w\n     \u2022     We move the model weights in the direction that assigns a\n           higher probability to w\n---\n                    L   = \u2212        y [w] log \u02c6\nl to minimize the   CE             t             yt [w]\n                    error in predicting the true next word in the training sequ\n       Cross                 w\u2208V\n                 -entropy loss for language modeling\ncross-entropy as the loss function.\necall that the cross-entropy loss measures the difference between a pred\n of language modeling, the correct distribution        yt comes from kn\nbility distribution and the correct distribution.\n       CE loss: difference between the correct probability distribution and the predicted\n . This is represented as a one-hot vector corresponding to the v\n       distribution       X\nentry for the actual next word is 1, and all the other entries are\n                       L  = \u2212      y [w] log \u02c6\nentropy loss           CE          t    yt [w]                                           (\n                       for language modeling is determined by the proba\n                               w\u2208V\n igns to the correct next word (all other words get multiplied by\n ase   The correct distribution y knows the next word, so is 1 for the actual next\n   of language modeling, the correct distribution y comes from knowin\nthe CE loss in (10.5)        t                    t\n       word and 0 for the   can be simplified as the negative log prob\nord. This is                others.\n                 represented as a one-hot vector corresponding to the vocabu\n       So in this sum, all terms get multiplied by zero except one: the logp the\n igns to the next word in the training sequence.\nthe entry for the actual next word is 1, and all the other entries are 0. T\n       model assigns to the correct next word, so:\n ss-entropy loss for language modeling is determined by the probability\n                       L    ( \u02c6                   \u02c6\nl assigns to the       CE    yt , yt ) = \u2212 log yt [wt +1 ]\n                       correct next word (all other words get multiplied by zero\n t the CE loss in (10.5) can be simplified as the negative log probabilit\n---\nTeacher forcing\n\n\u2022     At each token position t, model sees correct tokens w1:t,\n     \u2022  Computes loss (\u2013log probability) for the next token wt+1\n\u2022     At next token position t+1 we ignore what model predicted\n      for w\u209c\u208a\u2081\n     \u2022  Instead we take the correct word w\u209c\u208a\u2081, add it to context, move on\n---\nTraining a transformer language model\n\nTrue next token  long        and        thanks        for        all           \u2026\n\nCE Loss          \u2212log ylong  \u2212log yand  \u2212log ythanks  \u2212log yfor  \u2212log yall     \u2026\nper token\n\n                 \u0177  back     \u0177  back    \u0177  back       \u0177  back    \u0177  back\n                    prop        prop       prop          prop       prop\n\nLLM                                                                            \u2026\n\nInput tokens  So  long  and  thanks  for    \u2026\n---\n LLMs are mainly trained on the web\n\nCommon crawl, snapshots of the entire web produced by\n the non- profit Common Crawl with billions of pages\nColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156\n billion tokens of English, filtered\n What's in it? Mostly patent text documents, Wikipedia, and\n news sites\n---\nThe Pile: a pretraining corpus\nComposition of the Pile by Category\nacademics    web                   books\n             Academic Internet Prose DialogueMisc\n\n                                                                 Bibliotik\n\nPile-CC                                                          PG-19       BC2\n\nPubMed Central    ArXiv\n                                                                             Subtitles\n                                                                                      dialog\n\n                  PMA    StackExchange                           Github      IRC EP\nFreeLaw           USPTO  Phil  NIH OpenWebText2    Wikipedia     DM Math     HN  YT\n---\nFiltering for quality and safety\n\nQuality is subjective\n\u2022     Many LLMs attempt to match Wikipedia, books, particular\n      websites\n\u2022     Need to remove boilerplate, adult content\n\u2022     Deduplication at many levels (URLs, documents, even lines)\nSafety also subjective\n\u2022     Toxicity detection is important, although that has mixed results\n\u2022     Can mistakenly flag data written in dialects like African American\n      English\n---\n Reexamining \"Fair Use\" in the Age of\n AI There are problems with scraping from the web\n Generative Al claims to produce new language and images, but when those ideas are based on copyrighted\n material, who gets the credit? A new paper from Stanford University looks for answers.\n Jun 5, 2023 | Andrew Myersfin 0\n\n                                                                                                       Authors Sue OpenAI Claiming Mass Copyright\n                                                                                                       Infringement of Hundreds of Thousands of Novels\n\n The Times Sues OpenAI and Microsoft\nOver A.I. Use of Copyrighted Work\nMillions of articles from The New York Times were used to train\n chatbots that now compete with it, the lawsuit said.\n---\nThere are problems with scraping from the web\n\nCopyright: much of the text in these datasets is copyrighted\n\u2022     Not clear if fair use doctrine in US allows for this use\n\u2022     This remains an open legal question across the world\nData consent\n\u2022     Website owners can indicate they don't want their site crawled\nPrivacy:\n\u2022     Websites can contain private IP addresses and phone numbers\nSkew:\n\u2022     Training data is disproportionately generated by authors from the\n      US which probably skews resulting topics and opinions\n---\nLarge       Pretraining Large Language\nLanguage    Models\nModels\n---\nLarge       Evaluating Large Language\nLanguage    Models\nModels\n---\n   P(w ) = P(w )P(w |w )P(w |w  ) . . . P(w |w\n We\u2019ve been talking about predicting one word at a time, computing the proba\n       1:n    1  2  1  3  1:2               n  1:n\u2212\nof the next token w from the prior context: P(w |w ). But of course as we\n    Better        n\n              LMs are better at predicting text\n hapter 3 the     iY      i  <i\n                chain rule allows us to move between computing the probability\n next token and =       P(wi |w<i )\n                computing the probability of a whole text:\n    Reminder of the chain rule:\n       P(w         i=1\n                1:n ) = P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P(wn |w1:n\u22121 )\npute the probability of text just by multiplying the cond\n                        n\n ch                = Y P(wi |w<i )\n    token in the text. The resulting (log) likelihood of a\n                        i=1\n mparing how good two language models are on that text\ne can compute the probability of text just by multiplying the conditional pr\n        So given a text w1:n we could just compare the log likelihood from two LMs:\nities for each token in the text. The resulting (log) likelihood of a text is a us\n                                   n\n tric for comparing how good two language   Y\n        log likelihood(w          ) = models are on that text:\n                               1:n  log            P(wi |w<i )\n                                            n\n                  log likelihood(w  ) = log Y i=1\n                               1:n           P(wi |w<i )\n---\nBut raw log-likelihood has problems\n\nProbability depends on size of test set\n\u2022  Probability gets smaller the longer the text\n\u2022  We would prefer a metric that is per-word,\n   normalized by length\n---\nPerplexity is normalized for length\n\nPerplexity is the inverse probability of the test set,\nnormalized by the number of words\n (The inverse comes from the original definition of\n perplexity from cross-entropy rate in information theory)\n\n Probability range is [0,1], perplexity range is [1,\u221e]\n---\n          set), normalized by the test set length in tokens. For a test set of n toke\n    Perplexity\nAs we     perplexity is\n          first saw in Chapter 3, one way to evaluate language models is\n  well they predict unseen text. Intuitively, good models are those that\n                              Perplexity          (w ) =      P (w )\u2212 1\n    So just as for n-gram grammars, we use perplexity                 n\n                                                  1:n         to measure how\n robabilities to unseen data (are less     q                  q    1:n\n                                           surprised when encountering the\n    well the LM predicts unseen text                     = s       1\n    The perplexity of a model \u03b8 on an unseen test             n\ntiate this intuition by using perplexity to measure set is the inverse\n                                                           the quality of a\n    probability that \u03b8 assigns to the test set,                  Pq (w1:n )\n el. Recall from page   ?? that the perplexity of    normalized by the test\n    set length.                                      a model q on an unseen\n erse probability that  q assigns to the test set, normalized by the test\n          To visualize how perplexity can be computed as a function of the proba\na test  For a test set of n tokens w       the perplexity is :\n        set of  n tokens w  , the perplexity is\n          LM computes for each new word, we can use the chain rule to expand th\n                            1:n    1:n\n          tion of probability of the test set: \u2212 1\n                Perplexity (w1:n ) = P (w1:n ) n\n                           q          q                   v\n                                     s                    u n\n                                           1              uY          1\n                                   =                      t\n                                   Perplexity   (w  ) =\n                                        n  q       1:n    n      P (w(10.7)\n                                           P (w1:n )             q    i |w<i )\n                                           q                  i=1\n---\nPerplexity\n\n\u2022     The higher the probability of the word sequence, the lower\n      the perplexity.\n\u2022     Thus the lower the perplexity of a model on the data, the\n      better the model.\n\u2022     Minimizing perplexity is the same as maximizing\n      probability\n\nAlso: perplexity is sensitive to length/tokenization so best used\nwhen comparing LMs that use the same tokenizer.\n---\nMany other factors that we evaluate, like:\n\nSize\nBig models take lots of GPUs and time to train, memory to store\nEnergy usage\nCan measure kWh or kilograms of CO2 emitted\nFairness\nBenchmarks measure gendered and racial stereotypes, or decreased\nperformance for language from or about some groups.\n\n",
        "quiz": [
            {
                "question_text": "What type of agents are large language models?",
                "answers": [
                    {
                        "text": "Computational agents that can interact conversationally with people using natural language",
                        "is_correct": true,
                        "explanation": "The concept description explicitly defines large language models as computational agents that can interact conversationally with people using natural language."
                    },
                    {
                        "text": "Simple n-gram language models",
                        "is_correct": false,
                        "explanation": "Simple n-gram language models are mentioned as a different type of language model, not large language models."
                    },
                    {
                        "text": "Neural networks that generate images",
                        "is_correct": false,
                        "explanation": "The content context does not mention anything about generating images; it focuses on natural language interaction."
                    },
                    {
                        "text": "Programs that translate code between programming languages",
                        "is_correct": false,
                        "explanation": "The content context does not mention anything about translating code between programming languages; it focuses on natural language interaction."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "How do language models generate text?",
                "answers": [
                    {
                        "text": "By assigning probabilities to sequences of words and sampling possible next words",
                        "is_correct": true,
                        "explanation": "The concept description states that language models generate text by assigning probabilities to sequences of words and sampling possible next words."
                    },
                    {
                        "text": "By using a fixed set of predefined sentences",
                        "is_correct": false,
                        "explanation": "The concept description does not mention using a fixed set of predefined sentences for text generation."
                    },
                    {
                        "text": "By randomly selecting words from a dictionary",
                        "is_correct": false,
                        "explanation": "The concept description specifies that text generation involves sampling from a probability distribution over possible next words, not random selection from a dictionary."
                    },
                    {
                        "text": "By copying text from a database of stored phrases",
                        "is_correct": false,
                        "explanation": "The concept description does not mention copying text from a database; it focuses on generating text based on probabilities and sampling."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are large language models trained to do?",
                "answers": [
                    {
                        "text": "Interact conversationally with people using natural language",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that large language models are computational agents that can interact conversationally with people using natural language."
                    },
                    {
                        "text": "Perform complex mathematical calculations",
                        "is_correct": false,
                        "explanation": "The concept description does not mention that large language models are trained to perform complex mathematical calculations."
                    },
                    {
                        "text": "Generate images based on textual descriptions",
                        "is_correct": false,
                        "explanation": "The concept description does not mention that large language models are trained to generate images based on textual descriptions."
                    },
                    {
                        "text": "Control physical robots in real-time",
                        "is_correct": false,
                        "explanation": "The concept description does not mention that large language models are trained to control physical robots in real-time."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the fundamental intuition behind large language models?",
                "answers": [
                    {
                        "text": "Text contains enormous amounts of knowledge and pretraining on lots of text gives language models their ability to perform well",
                        "is_correct": true,
                        "explanation": "This is the fundamental intuition behind large language models as described in the content."
                    },
                    {
                        "text": "Large language models are trained by learning to guess the next word in a sequence",
                        "is_correct": false,
                        "explanation": "While this is a characteristic of large language models, it is not the fundamental intuition behind them."
                    },
                    {
                        "text": "Large language models use a simple n-gram model to assign probabilities to sequences of words",
                        "is_correct": false,
                        "explanation": "Large language models are more advanced than simple n-gram models, as they use neural networks to assign probabilities."
                    },
                    {
                        "text": "Large language models are primarily used for classification tasks",
                        "is_correct": false,
                        "explanation": "Large language models are used for a variety of tasks, including text generation, not just classification."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What do large language models learn from pretraining?",
                "answers": [
                    {
                        "text": "Large language models learn to guess the next word in a sequence.",
                        "is_correct": true,
                        "explanation": "The concept description states that large language models are trained by learning to guess the next word."
                    },
                    {
                        "text": "Large language models learn to count the frequency of words in a text.",
                        "is_correct": false,
                        "explanation": "The concept description mentions n-gram models that count word frequencies, but large language models learn to guess the next word, not just count frequencies."
                    },
                    {
                        "text": "Large language models learn to assign probabilities to individual words.",
                        "is_correct": false,
                        "explanation": "The concept description specifies that large language models assign probabilities to sequences of words, not individual words."
                    },
                    {
                        "text": "Large language models learn to generate text by sampling possible next words.",
                        "is_correct": false,
                        "explanation": "While large language models do generate text by sampling possible next words, the question specifically asks what they learn from pretraining, which is to guess the next word."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the output of a large language model?",
                "answers": [
                    {
                        "text": "A distribution over possible next words",
                        "is_correct": true,
                        "explanation": "The concept description states that a large language model outputs a distribution over possible next words."
                    },
                    {
                        "text": "A sequence of predefined responses",
                        "is_correct": false,
                        "explanation": "The concept description does not mention predefined responses; it focuses on generating probabilities for next words."
                    },
                    {
                        "text": "A set of predefined rules for conversation",
                        "is_correct": false,
                        "explanation": "The concept description does not mention predefined rules; it focuses on probabilistic generation of text."
                    },
                    {
                        "text": "A list of possible topics for discussion",
                        "is_correct": false,
                        "explanation": "The concept description does not mention topics for discussion; it focuses on generating probabilities for next words."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the three architectures for large language models?",
                "answers": [
                    {
                        "text": "Decoder, Encoder, Encoder-Decoder",
                        "is_correct": true,
                        "explanation": "The content explicitly lists these as the three architectures for large language models."
                    },
                    {
                        "text": "Transformer, Recurrent Neural Network, Convolutional Neural Network",
                        "is_correct": false,
                        "explanation": "These are different types of neural network architectures, not the specific architectures for large language models."
                    },
                    {
                        "text": "Generative, Discriminative, Hybrid",
                        "is_correct": false,
                        "explanation": "These terms describe types of models but are not the specific architectures mentioned for large language models."
                    },
                    {
                        "text": "GPT, BERT, T5",
                        "is_correct": false,
                        "explanation": "These are examples of models that use the architectures, not the architectures themselves."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the three stages of training in large language models?",
                "answers": [
                    {
                        "text": "Pretraining, Instruction Tuning, Preference Alignment",
                        "is_correct": true,
                        "explanation": "The three stages of training in large language models are explicitly listed as Pretraining, Instruction Tuning, and Preference Alignment in the provided content."
                    },
                    {
                        "text": "Training, Testing, Deploying",
                        "is_correct": false,
                        "explanation": "These are general stages of model development but not specifically the stages of training in large language models as described in the content."
                    },
                    {
                        "text": "Data Collection, Model Building, Evaluation",
                        "is_correct": false,
                        "explanation": "These are general steps in creating a model but not the specific stages of training in large language models as outlined in the content."
                    },
                    {
                        "text": "Fine-tuning, Validation, Optimization",
                        "is_correct": false,
                        "explanation": "These are steps in model refinement but not the specific stages of training in large language models as described in the content."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ],
        "summary": "The document \"LLMs.pdf\" introduces large language models (LLMs), which are computational agents that interact conversationally using natural language and have revolutionized NLP and AI. LLMs assign probabilities to word sequences, generate text by sampling next words, and are trained by learning to predict the next word in a sequence. The fundamental intuition is that pretraining on vast amounts of text imbues these models with extensive knowledge, enabling them to perform a wide range of tasks. The document also explains the architecture of LLMs, which typically involve a neural network that takes a context or prefix as input and outputs a distribution over possible next words, and discusses how these models can generate text by repeatedly sampling from this distribution."
    }
}