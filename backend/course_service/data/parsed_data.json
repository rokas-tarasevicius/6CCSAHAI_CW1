{
    "data/raw/Week9 - LGT.pdf": {
        "metadata": {
            "file_name": "Week9 - LGT.pdf",
            "file_type": "pdf",
            "content_length": 54823,
            "language": "en",
            "extraction_timestamp": "2025-11-25T23:24:03.859953+00:00",
            "timezone": "utc"
        },
        "content": "                    KING'S\n   Applications of  .\n                    College\n   LLM \u2013 Part I:    LONDON\n   Retrieval\n   Augmented\n   Generation\n   (RAG)\n\n   Week 9 - LGT     BVSH HOVSE\n\nG  Dr Lin Gui\n   Lin.1.gui@kcl.ac.uk\n---\nLearning outcomes\n\n\u26ab By the end of this topic, you will be able to:\n\n  \u26ab  Understand the core concepts of Retrieval-Augmented Generation and how it\n     differs from standard LLM approaches.\n\n  \u26ab  Build and configure a basic RAG pipeline using embeddings, retrievers, and\n     generators.\n\n  \u26ab  Evaluate and optimize RAG performance through effective data preparation,\n     chunking, and retrieval strategies.\n\n2\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n3\n---\n                                                        RAG overview\n\n                                                                               \u26ab                                  When answering questions\n                                                                                                                  or generating text, it first\n                                                                                                                  retrieves relevant\n                                                                                                                  information from a large\n                                                                                                                  number of documents, and\n                                                                                                                  then LLMs generates\n                                                                                                                  answers based on this\n                                                                                                                  information.\n\n                                                                               \u26ab                                  By attaching an external\n                                                                                                                  knowledge base, there is\n                                                                                                                  no need to retrain the\n                                                                                                                  entire large model for each\n                                                                                                                  specific task.\n\n                                                                               \u26ab                                  The RAG model is\n                                                                                                                  especially suitable for\n\n                                           Input        Query                       Indexing                      knowledge-intensive tasks.\nUser                                        How do you evaluate the fact       Documents\n                                            that OpenAI's CEO, Sam Altman,              Chunks Vectors\n\n        Output                              by the board in just three days,\n                                            and then was rehired by the                 embeddings\n                                            company, resembling a real-life\n                                            version of \"Game of Thrones\" in             Retrieval\n\nwithout RAG                                                                    Relevant Documents\n\nfuture events. Currently, I do not have     LLM         Generation\n\nand rehiring of OpenAI's CEO ..            Question:                           Chunk 1: \"Sam Altman Returns to                                4\nwith RAG\n\nthe company's future direction and        based on the following information:     Chunk 2: \"The Drama Concludes? Sam\n                                                                                  Altman to Return as CEO of OpenAl,\nand turns reflect power struggles and     Chunk 2:                                Board to Undergo Restructuring\"\n                                          Chunk 3 :\nOpenAl...                                 Combine Context                         OpenAl Comes to an End: Who Won\n             Answer                       and Prompts                             and Who Lost?\"\n---\n   Symbolic Knowledge or Parametric Knowledge\n\n    \u26ab Ways to optimize LLMs.\n\n    \u26ab Prompt Engineering    This week\n\n    \u26ab Instruct / Fine-tuning\n\n    \u26ab  Retrieval-Augmented\n       Generation\n\n    Week 7    Week 8\n\nExternal Knowledge\n     Required\n    High     Modular RAG\n\n             multiple modules    Retriever Fine-tuning\nAdvanced RAG                     Collaborative Fine-tuning\n\noptimization                     All of the above\n Naive RAG                       RAG             Generator Fine-tuning\n\n   XoT Prompt     Prompt Engineering  Fine-tuning          5\n e.g. CoT, ToT\nFew-shot Prompt\n    Low           Standard Prompt                      Model Adaptation\n           Low                                       High  Required\n---\nRAG vs Fine-tuning\n\n Data Processing\n                ddling.    datasets, and limited datasets may not result\n\n 6\n\n higher latency.    retrieval, resulting in lower latency.\n---\nRAG Application\n\n\u26ab Scenarios where RAG is applicable:\n\n  \u26ab  Long-tail distribution of data\n\n  \u26ab  Frequent knowledge updates\n\n  \u26ab  Answers requiring verification and traceability\n\n  \u26ab  Specialized domain knowledge\n\n  \u26ab  Data privacy preservation\n\nQuestion Answering     Fact checking    Dialog systems    Summarisation\n\nMachine translation    Code generation    Sentiment Analysis    Commonsense\n                                                                reasoning\n\n                                                                           7\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  Foundation of information Retrieval\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 8\n---\n   Foundation of information Retrieval\n\n   \u26ab What is information Retrieval?\n\n     \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n        returns those items to the user, typically in list form sorted per computed\n        relevance#\n\n   \u26ab Three main questions in information retrieval:\n\n     \u26ab  How to map the text into features (Embedding method)\n\n     \u26ab  How to measure the similarity between features (IR Modelling)\n\n     \u26ab  How to do it efficiently (Indexing)\n\n[#] Qiaozhu Mei and Dragomir Radev, \u201cInformation Retrieval,\u201d The Oxford Handbook of Computational Linguistics,\n2\u207f\u1d48 edition, Oxford University Press, 2016.    9\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n10\n---\nDiscrete representation\n\n\u26ab  In discrete representation, for both query and document, we assign each word a\n   specific dimension. If a word appears query/document, then value of the\n   corresponding dimension is:\n\n    \u26ab  In Binary representation: 1\n\n    \u26ab  In TF (term frequency) based representation: t (how many times this word\n       appears within the query/documents)\n\n    \u26ab  In TF-IDF (inverse document frequency) based representation: tlog(n/x)\n\n       \u26ab  Here, t is term frequency, n is number of documents, x is the number of\n          documents which contains this term.\n\n11\n---\nDiscrete representation (example)\n\n\u26ab We have the following documents:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n\u26ab After pre-processing:\n\n  \u26ab  D1 = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d.\n\n  \u26ab  D2 = \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n  \u26ab  D3 = \u201cshipment\u201d, \u201cgold\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n                                                  12\n---\nDiscrete representation (example)\n\n \u26ab Building vocabulary:\n\n   \u26ab V = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d, \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n \u26ab  Detect the feature for each document. If the feature occurs, the corresponding\n    value is \u20181\u2019, otherwise \u20180\u2019 (binary feature):\n\n           shipment  gold  damage  fire          delivery  silver  arrive  truck\n     D1     1        1     1       1             0         0       0       0\n     D2     0        0     0       0             1         1       1       1\n     D3     1        1     0       0             0         0       1       1\n\n 13\n---\nDiscrete representation (example)\n\n\u26ab  Definition \u2013 term frequency (TF):\n\n    \u26ab  \ud835\udc61 - how many times the term appears in the document\n\n\u26ab  Example:\n\n    \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n    \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n    \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n              shipment  gold  damage  fire  delivery  silver    arrive  truck\n        D1     1        1     1       1     0              0     0      0\n        D2     0        0     0       0     1              2     1      1\n        D3     1        1     0       0     0              0     1      1\n\n                                                                             14\n---\nDiscrete representation (example)\n\n\u26ab Definition \u2013 inverse document frequency (IDF):\n\n  \u26ab  \ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc5b/\ud835\udc65) \u2013 n is number of documents, x is the number of documents which\n     contains this term\n\n\u26ab Example:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n          shipment     gold  damage  fire  delivery  silver     arrive  truck\n          0.176     0.176    0.477   0.477  0.477    0.477      0.176   0.176\n                       Inverse document frequency vector\n\n                                                                             15\n---\nDiscrete representation (example)\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1      1         1        1      1         0        0           0         0\n D2      0         0        0      0         1        2           1         1\n D3      1         1        0      0         0        0           1         1\n                           Term frequency matrix\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n        0.176     0.176    0.477   0.477    0.477     0.477      0.176     0.176\n                           Inverse document frequency vector\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1     0.176     0.176    0.477   0.477     0        0           0         0\n D2      0        0         0      0        0.477     0.954      0.176     0.176\n D3     0.176     0.176     0      0         0        0          0.176     0.176\n\n                            TF-IDF Matrix\n                                                                                16\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n17\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n18\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n19\n---\n   Dense Passage Retrieval\n\n   \u26ab  Encode questions and text passages into continuous vectors (embeddings) and\n      retrieve passages using vector similarity instead of keyword overlapping.\n\n   \u26ab  Train directly on question\u2013passage pairs, using in-batch negatives to improve\n      efficiency.\n\n                 Question q    Passage p                                In each batch, there are multiple\n\n                 BERTQ         BERTp                Passage encoder     question\u2013answer pairs, both\n      Question encoder                                                  matched and unmatched. Matched\n                                                                        pairs should have similar\n\n                          OOOOOOOO  0OOOOOOO                            representations, while unmatched\n                                                                        pairs should have representations\n                          $h_q$                                         that are far apart.\n                                                Training phase\n\n      Similarity score: dot product  (q, =  Fine-tune two encoders\nhttps://aclanthology.org/2020.emnlp-main.550.pdf                                                 20\n---\n   ReContriever\n\n   \u26ab  What if we don\u2019t have annotated data (Matched and unmatched QA-pair).\n\n   \u26ab  Using pseudo-examples: For each passage/document p, create an augmented\n      version p\u2032. Then treat (p, p\u2032) as a positive pair:\n\n       \u26ab  Masking words (random word masking)\n\n       \u26ab  Span deletion\n\n       \u26ab  Back-translation Sentence\n\n       \u26ab  Reordering Adding noise\n\n       \u26ab  Perturbations Cropping (taking a subset of sentences)\n\nhttps://aclanthology.org/2023.findings-acl.695.pdf    21\n---\n  Using API\n\n   \u26ab  There are many APIs could do this job, for example, Mistral AI:\n\n                       YMISTRAL EMBED API\n\n                       8OPEN IN COLAB\n\n   \u26ab  Example: link    How to Generate Embeddings\n                       To generate text embeddings using Mistral Al's embeddings APl, we can make a request to the APl endpoint and specify the\n                       embedding model mistra1-embed , along with providing a list of input texts. The APl will then return the corresponding\n                       embeddings as numerical vectors, which can be used for further analysis or processing in NLP applications.\n\n   \u26ab Some other options:    PYTHON TYPESCRIPT      CURL                                                           OUTPUT\n                            import os\n                            from mistralai import Mistral\n          Sentence Bert     api_key = os.environ[\"MISTRAL_API_KEY\"]\n     \u26ab                      model = \"mistral-embed\"\n                            client = Mistral(api_key=api_key)\n\n     \u26ab    SimCSE            embeddings_batch_response = client.embeddings.create(\n                            model=model,\n                            inputs=[\"Embed this sentence.\", \"As well as this one.\"],\n\n     \u26ab    \u2026\u2026                )\n                            The output is an embedding object with the embeddings and the token usage information.\n\n                            Let's take a look at the length of the first embedding:\n\n                            PYTHON TYPESCRIPT CURL\n                            len(embeddings batch response.data[0].embedding)\n\nhttps://docs.mistral.ai/capabilities/embeddings    22\n---\nIR Modelling\n\n\u26ab What is information Retrieval?\n\n  \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n     returns those items to the user, typically in list form sorted per computed\n     relevance\n\n\u26ab Three main questions in information retrieval:\n\n  \u26ab  How to map the text into features (Embedding method)\n\n  \u26ab  How to measure the similarity between features (IR Modelling)\n\n  \u26ab  How to do it efficiently (Indexing)\n\n23\n---\nIR Modelling\n\n\u26ab  In IR modelling, we use different metric to measure the similarity/distance\n   between the query and given documents. The target is to find the top-k relevant\n   documents based on the given query.\n\n    \u26ab  Cosine similarity (for both Discrete & Continuous representation)\n\n    \u26ab  Jaccard distance (for Discrete representation only)\n\n    \u26ab  BM25 (for Discrete representation only)\n\n24\n---\nCosine similarity\n\n\u26ab Cosine similarity\n                           \u03c3\ud835\udc5b \ud835\udc65\ud835\udc56 \ud835\udc66\ud835\udc56\n                   \ud835\udc36\ud835\udc5c\ud835\udc60 \ud835\udc65, \ud835\udc66 = \u03c3\ud835\udc5b  \ud835\udc56=1 \u03c3\ud835\udc5b\n                                   \ud835\udc56=1(\ud835\udc65\ud835\udc56 )2  \ud835\udc56=1(\ud835\udc66\ud835\udc56 )2\n\n\u26ab Considering\n  \u2212  D1 = [1,1,1,1,0,0,0,0]\n  \u2212  D3 = [1,1,0,0,0,0,1,1]\n                                 \ud835\udc36\ud835\udc5c\ud835\udc60(D1,D3)=1/2\n\n25\n---\nJaccard similarity\n\n\u26ab Only considering if there is over lapping or not. We don\u2019t care about the value.\n\n\u26ab For example:            C1    sim(cl,c2)    C1  C2\n\n  \u26ab  \ud835\udc45\ud835\udc65 = [2,0,3,3]     C2\n\n  \u26ab  \ud835\udc45\ud835\udc66 = [1,1,0,5]     C3                    JACCARD SIMILARITY\n                                                  2    0.5\n                                              4  2+1+1\n\n\u26ab Jaccard similarity: \ud835\udc60\ud835\udc56\ud835\udc5a \ud835\udc65, \ud835\udc66 = \ud835\udc79\ud835\udc99\u2229\ud835\udc79\ud835\udc9a\n                                    \ud835\udc79\ud835\udc99\u222a\ud835\udc79\ud835\udc9a\n\n                                               26\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              hyperparameters         Average length of all docs\n                                                                                  27\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b      \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1      \ud835\udc56               1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n     Maybe\u2026a bit confusing                            Average length of all docs\n     Can you speak in English?  hyperparameters\n                                                                                  28\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b             \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1             \ud835\udc56        1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n Relax\u2026it is pretty simple actually    hyperparameters    Average length of all docs\n\n                                                                                  29\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:  IDF term in Q (is it an important word?)\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| )\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              The \u2018percentage\u2019 of querying words in D\n\n                                                                                  30\n---\nIndexing\n\n\u26ab Next question, how to do it efficiently (Indexing)\n\n\u26ab  Suppose we have 1k queries, and there are 1 billion documents in knowledge\n   based, how many times of comparison we need?\n\n\u26ab  1k x 1b\n    It is a really huge number.\n    In real world scenario, it could be even larger\n    If there is only one important task in information retrieval, it must be\n    \u201cindexing\u201d\n\n31\n---\nIndexing - Discrete representation\n\n\u26ab  Inverted index\n\n\u26ab  Since the discrete representation is sparse (most dims are zero), we can build\n   inverted index. For each word, we build a link list to store all the documents\n   contain this word.\n\n\u26ab  For the given query, the complexity is now only related to the #unique words in\n   the query. (In most queries, the size is just few words)\n   doclD                          geo-scopelD              geo-scopelD   docID\n     1                            Europe                   Europe        1 2 7\n     2                                Europe               France        3\n     3                                France               Portugal      5\n     4                                England              England       4\n     5                                Portugal             Quebec        6\n     6                                Quebec               Spain         8\n     7                                Europe\n     8                                Spain\n                     Forward Index                         Inverted Index\n                                                                                 32\n---\n   Indexing - Continuous representation\n\n   \u26ab  In continuous representation, it might be a bit complex. There is no sparse\n      representation anymore.\n\n   \u26ab  We can use the following method to speed up the searching.\n\n       \u26ab  Vector compression \u2013 reduce the size of vectors\n\n       \u26ab  Hierarchical clustering \u2013 in each layer only search the nearest cluster\n                                          Clustering the documents first, and then,\n                                          Only consider the nearest centroid during the searching\n          voronoi cells  xq  Centroids                        voronoi cells  xq    Centroids\n                         o\n                             o                                                     o\n                  e\n                         o\n                  o          \u00a9    o                           9                  o\n                                  -\n                       o                                           o\n         Pause (k)\n\nhttps://www.pinecone.io/learn/series/faiss/faiss-tutorial/                                       33\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n34\n---\nNaive RAG\n\n\u26ab  Step 1 \u2013 indexing\n\n    \u26ab  Divide the document into even chunks, each chunk being a piece of the\n       original text.\n\n    \u26ab  Using the encoding model to generate an embedding for each chunk.\n\n    \u26ab  Store the Embedding of each block in the vector database.\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  Retrieve the k most relevant documents using vector similarity search.\n\n\u26ab  Step 3 \u2013 Generation\n\n    \u26ab  The original query and the retrieved text are combined and input into a LLM\n       to get the final answer\n                                                                             35\n---\n  Naive RAG\n\n    \u26ab Step 1 \u2013 indexing\n\n    \u26ab Step 2 \u2013 Retrieval\n\n    \u26ab Step 3 \u2013 Generation\n\n                                    Offline\n\nDocuments  Document Chunks  Vector Database\n    8                                      36\n   User  Query    Related DocumentChunks\n                            Frozen\n    Augmented Prompt        LLM\n---\nAdvanced RAG\n\n  \u26ab Step 1 \u2013 indexing\n\n  \u26ab + index optimization\n\n  \u26ab + pre-retrieval process\n\n  \u26ab Step 2 \u2013 Retrieval\n\n  \u26ab +post-retrieval process\n\n  \u26ab Step 3 \u2013 Generation\n\n  URLS  PDFs  Database\n     Documents               Document Chunks       Vector Database\n                             Fine-grained Data Cleaning\n                             Sliding Window /Small2Big\n                             Add File Structure\n                             Query Rewrite/Clarifcation\n  User    Query              Retriever Router                          37\n                              Pre-retrieval    Related Document Chunks\n\n  Prompt              LLM                        Rerank  Filter  Prompt Compression\n                                                         Post-retrieval\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Sliding windows\n\n    \u26ab  + index optimization       Fine-grained segmentation\n\n    \u26ab  + pre-retrieval process    Adding metadata\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n38\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Sliding windows\n\n      \u26ab  + index optimization       Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process    Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n                                    Document\n\n         0                200 100   300 200  400 300  500\n\n  Split the doc into chunks, and ensure there is over lapping between chunks (WHY?)\n                                                                                   39\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                                      Sliding windows\n\n      \u26ab  + index optimization          Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                            Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n\n                          Document     Section 1             Paragraph 1.1\n\n                                       Section 2             Paragraph 1.2\n\n                          Searching on fine-grained text     Paragraph 1.3\n                                                                           40\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                               Sliding windows\n\n      \u26ab  + index optimization                        Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                     Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation                             Web page    Publishing date\n\n                                                                    Title\n     The metadata is the aspects of each chunk.\n     It will help both retriever and generator to                Parents node\n     improve the performance.\n\n  41\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process    Summarization\n\n\u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n    \u26ab  +post-retrieval process    Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n\n42\n---\nAdvanced RAG\n\n   \u26ab  Step 1 \u2013 indexing                    Retrieve routes\n\n       \u26ab  + index optimization\n\n       \u26ab  + pre-retrieval process          Summarization\n\n   \u26ab  Step 2 \u2013 Retrieval                   Rewriting\n\n       \u26ab  +post-retrieval process          Confidence judgment\n\n   \u26ab  Step 3 \u2013 Generation                  Instead of one flat \u201cretrieve chunks by embeddings\u201d step, you can:\n\n                                           Searching doc first\n\n  Retrieve routes = multiple retrieval     Query    Document    Chunk\n  paths that a RAG system can choose                            Searching chunks\n  from, depending on query intent, data                         within the doc\n  type, or document structure.\n                                                                                                             43\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing                       Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process             Summarization\n\n\u26ab  Step 2 \u2013 Retrieval                      Rewriting\n\n    \u26ab  +post-retrieval process             Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n                                           Document\n                                                           Summarise first\n                                  Query    Summarisation\n\n                                  Searching in smmarisation\n                                  instead of full documents\n                                                                          44\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing               Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process     Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval              Rewriting\n\n      \u26ab  +post-retrieval process     Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n        Benefits:                    Query\n        a more explicit query             Rewrite the query first\n        a more keyword-rich query    Rewrite the\n        a more structured query      query      Searching\n        multiple diverse sub-queries\n                                     Searching by re-written query\n                                                                  45\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process    Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n      \u26ab  +post-retrieval process    Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n  Query     Document             LLM            Confidence checking    Output\n\n            Confirm the Confidence before output\n             By similarity scores\n             By LLM Confidence scores                                        46\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing           Re-order\n\n    \u26ab  + index optimization    Filter content retrieval\n\n    \u26ab  + pre-retrieval process\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n47\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing           Re-order\n\n      \u26ab  + index optimization    Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation         Evidence#1  Evidence#2  Evidence#3  Question\n\n  LLMs is sensitive with the input order\n  The early input chunks has higher weights\n  How to organize the searched evidence for final output is\n  important\n\n  48\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Re-order\n\n      \u26ab  + index optimization       Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process    Evidence#1  Evidence#2  Evidence#3  Question\n\n  \u26ab  Step 3 \u2013 Generation\n\n                                                           Evidence#1  Evidence#3  Question\n\n  To avoid possible hallucination, filtering the irrelevant\n  evidences.\n\n                                                                                           49\n---\nModular RAG\n\n   Na\u00ef\n\u26ab  ve RAG\n\n      Read           Retrieve    Generate\n\n\u26ab  DSP\n\n   Demonstrate       Search      Predict    Generate\n\n\u26ab  Rewrite-Retrieve-Read\n\n      Rewrite        Retrieve    Read\n\n\u26ab  Retrieve-then-read\n\n   Retrieve          Read        Generate\n\n50\n---\n   Different RAG Paradigms\n\n    Modules\n    8 C 8 E2    Search\nUser Query  Documents    User Query  Documents    Routing    Predict\n\n    Indexing    Query Routing    Indexing    Rewrite  RAG  Rerank\n\n  Read\n                               Fusion\n Memory\n\n    \\Post-Retrieval    Patterns\n \u2192|l|+                               51\nSummary                        Retrieve\n\n    Output    Output\n\n    Naive RAG    Advanced RAG    Modular RAG\n---\nKey problems in RAG\n\n \u26ab  How to retrieve\n\n \u26ab  When to retrieve\n\n \u26ab  How to use the retrieved information\n\n 52\n---\nHow to retrieve\n\n \u26ab By using the information on different structuration levels\n\n \u26ab  Token level         It excels in handling long-tail and cross-domain issues with high\n                        computational efficiency, but it requires significant storage.\n\n \u26ab  Phrase level\n\n \u26ab  Chunk level         The search is broad, recalling a large amount of information, but with\n                        low accuracy, high coverage but includes much redundant information.\n\n \u26ab  Entity level\n\n \u26ab  Knowledge level     Richer semantic and structured information, but the retrieval efficiency\n                        is lower and is limited by the quality of KG.\n\n 53\n---\n   When to retrieve\n\n    \u26ab Two questions:\n\n    \u26ab When we need to retrieve information to support the QA\n\n    \u26ab How many times we need to retrieve the information\n\n    \u26ab Solution#1: Conducting once search during the reasoning process.\n\n    High efficiency, but low relevance of the\n    retrieved documents\n\n    Retrieved document d.\n              Jobs cofounded     Jobs was raisedd;    Jobs is thex    apple\n  Retriever    Apple in his      by adopted...        CEO of        pearnot\n             parents' garage\n Document    Input                 Steve Jobs         Jobs is the     apple    apple\nRetrieval    Reformulation       passed away...       CEO of        pearnot    pearnot    54\nTest Context X    Black-box      Jobs cofoundedJobs is the            apple\n Jobs is the    LM                  Apple...          CEO of           pear\n   CEO of _                                           Ensemble          not\n    Apple\n---\n  When to retrieve\n\n   \u26ab  Two questions:\n\n   \u26ab  When we need to retrieve information to support the QA\n\n   \u26ab  How many times we need to retrieve the information\n\n   \u26ab Solution#2: Adaptively conduct the search.\n\n   Balancing efficiency and information\n   might not yield the optimal solution\n\n Search results:Dx               Retriever\n [1]:Search results:Dq2\n [2]:[1]:Search results:Dq3\n [2]:[1]: ...\nP     [2]: ..                             x\n     x Generate a summary about Joe Biden.\nAy1 Joe Biden attended           q2                55\n Q2[Search(Joe Biden University)]\n y2tthe University of Pennsylvania, where he earned\n q3[Search(Joe Biden degree)]    q3\n y3 a law degree.\n---\n When to retrieve\n\n  \u26ab  Two questions:\n\n  \u26ab  When we need to retrieve information to support the QA\n\n  \u26ab  How many times we need to retrieve the information\n\n  \u26ab Solution#3: Retrieve once for every N tokens generated.\n\n  A large amount of information with low\n  efficiency and redundant information.\n\n    Masked Language Modelling:\n    Bermuda Triangle is in the    western part\n  <MASK> of the Atlantic Ocean.\nPretraining    Atlas\nFew-shot\n          Fact checking:\nBermuda Triangle is in the western                                False    56\n      part of the Himalayas.            The Bermuda\n                                    Triangle is an urban\n                                    legend focused on a\n                                      loosely-defined\n       Question answering:             region in the       Western part of the\n  Where is the Bermuda Triangle?    western part of the    North Atlantic Ocean\n                                       North Atlantic\n                                           Ocean.\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 57\n---\nKey Technologies\n\n\u26ab  Data indexing optimization\n\n\u26ab  Structured Corpus\n\n\u26ab  Retrieval Source Optimization\n\n\u26ab  KG as a Retrieval Data Source\n\n\u26ab  Query Optimization\n\n\u26ab  Embedding Optimization\n\n\u26ab  Fine-tuning on RAG\n\n58\n---\n Data indexing optimization\n\n  \u26ab Chunk optimization\n\n  \u26ab Small-2-big: Embedding at sentence level expand the window during generation\n\n  process.\n\n                   Embed Sentence \u2192 Link to Expanded Window\n\n                              Continuous observation of the Atlantic meridional\n                              overturning circulation (AMOC) has improved the\n                              understanding of its variability (Frajka-Williams et al.,    What the LLM Sees\n                              2019), but there is low confidence in the quantification\n                              of AMOC changes in the 20th century because of low\n                   Embeddingagreement in quantitative reconstructed and simulated\n                              trends. Direct observational records since the\n                   Lookup     mid-2000s remain too short to determine the relative\nQuestion:                     contributions of internal variability, natural                                59\n                              forcing and anthropogenic forcing to AMOC change\nWhat are the                  (high confidence). Over the 21st century, AMOC wil\nconcerns                      very likely decline for all SSP scenarios but will not\n                              involve an abrupt collapse before 2100. 3.2.2.4 Sea Ice\nsurrounding the               Changes\nAMOC?                         Sea ice is a key driver of polar marine life, hosting        What the LLM Sees\n                              unique ecosystems and affecting diverse marine\n                              organisms and food webs through its impact on light\n                              penetration and supplies of nutrients and organic\n                              matter (Arrigo, 2014)\n---\nData indexing optimization\n\n\u26ab  Chunk optimization\n\n\u26ab  Sliding window: sliding chunk covers the entire text, avoiding semantic ambiguity.\n\n                          Maintain overlap for\n                          contextual continuity\n\nLoaded large\ndocument  Dividing into  Merging units into\n          compact units  larger chunks\n\n60\n---\nData indexing optimization\n\n \u26ab  Chunk optimization\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 61\n---\nStructured Corpus\n\n \u26ab  Adding meta-data: adding meta-data in the query searching to improve retrieval\n    accuracy, provide context during chunking, and enables filtering\n\n                             Indexed documents\n                                                                        Filtered subset of\n                                                                        documents    Most relevant\n    Did we implement any new                                                           documents\n       policies in 2021?     year: 2020, content: ...\n\n                             year: 2020, content:                       year: 2021, content.:..\n    year = 2021              year: 2021, content: .    Select relevant  year: 2021, content...    Vector similarity    year: 2021, content:.\n                                                       documents                                  search\n                             year: 2021, content:..                     year: 2021, content...                       year: 2021, content:...\n    Metadata filter          year: 2021, content: .\n\n Filter the irrelevant docs\n\n                           Ensure each chunk contains the metadata\n\n                                                                  62\n---\nRetrieval Source Optimization\n\n \u26ab  Adding meta-data\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 63\n---\nKG as a Retrieval Data Source\n\n\u26ab  Extract entities from the user's input query, then construct a subgraph to form\n   context, and finally feed it into the large model for generation.\n\n    \u26ab  Use LLM (or other models) to extract key entities from the question.\n\n    \u26ab  Retrieve subgraphs based on entities, delving to a certain depth, such as 2\n       hops or even more.\n\n    \u26ab  Utilize the obtained context to generate answers through LLM.Two-stage\n       method: Retrieve documents through summaries, then retrieve text blocks\n       from the documents.\n\n64\n---\n  KG as a Retrieval Data Source\n\n  \u26ab  Extract entities from the user's input query, then construct a subgraph to form\n     context, and finally feed it into the large model for generation.\n     P Meta Summary Entities        x k    Layer[i+1]\n                                    Summary Entities    Summarization by LLM  Layer[i]\n                                    Normal Entities     GMM Clustering        Layer[i-1]\n     Documents                      Hilndex: Indexing with Hierarchical Knowledge\n\n                                    6Communities        Global                                            Community Report\n                  Query             Key Entity          Bridge\n                                    Reasoning Paths                              Reasoning Paths                          Generation by LLM\n                                                        xk\n                                    Hatten KG             Local    Key Entity\n                                                                  Descriptions\n                                                        HiRetrieval: Retrieval with Hierarchical Knowledge\n\nhttps://arxiv.org/pdf/2503.10150                                                                                                           65\n---\n   Query Optimization\n\n    \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n       the Query can yield better retrieval results.\n\n\u26ab  Rewrite query:\n                 Input     Input\n                           Black-box LLM\n\n                 Retriever    Rewriter\n\nInput    Example\nSmall PrLM  Input:\n            What profession does Nicholas Ray and\nRewriter            Elia Kazan have in common?\n\n                                  Query             Quuery              Query: Nicholas Ray profession\n                     Documents    Web Search        Web Search            Query: Elia Kazan profession\n                                  Retriever         Retriever           Elia Kazan was an American film and\n                                                                         theatre director, producer,\n       Black-box LLM                                Documents            screenwriter and actor, described\n           Reader                 Documents                               Nicholas Ray American author and\n                                                                          director, original name Raymond\n                                                                          Nicholas Kienzle, born August 7,\n                     Output       Black-box LLM     Black-box LLM         1911, Galesville, Wisconsin, U.S.\n                                  Reader            Reader               Correct (reader                   director\n                                  Output            Reward Output        Hit (retriever\n                     (a) Retrieve-then-read (b)Rewrite-retrieve-read    (c) Trainable rewrite-retrieve-read\n\n https://arxiv.org/pdf/2305.14283    66\n---\n Query Optimization\n\n  \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n     the Query can yield better retrieval results.\n\n  \u26ab Clarify the query:                                        Ambiguous Question (AQ)\n                                                    \"What country has the most medals         o  M\n                                                             in Olympic history?\"             88\n\n                                                      Tree of Clarifications\n                                                      Question                       Pruned   Information\n                                                   Clarification                              Retrieval\n                                                                       *\n                                                   DQ1            DQ2   DQ3\n                                                                       \"What country has\n                                                                       the most total medals\n                                                                       in Olympic history?\"\n                                                   Question       Question\n                                                   Clarification  Clarification\n                                                   *                   *                      Passages\n                                                   DQ11 DQ12 DQ13 DQ21 DQ22 DQ23 DQ24\n                                                   \"What country has the  \"What country has\n                                                   most medals in winter  the most gold medals\n                                                     Olympic history?\"  in Olympic history?\"\n\n                                                                                     Long Form Answer\n\n                                                     Answer                          \"The United States has the\n                                                   Generation                        most total medals. .\n                                                                                        Norway has won most\n\nhttps://aclanthology.org/2023.emnlp-main.63.pdf                                      medals in winter Olympic.\"    67\n---\n Embedding Optimization\n\n  \u26ab Better embedding always indicate a better retrieval results:\n\n             \u26ab    Selecting a more suitable embedding method\n                                                                0Retriever &  HotpotQA      Dataset\n                  Fine-tuning the embedding model               Framework      EM F1        2Wiki      NQ   WebQ\n             \u26ab                                                  [BM25                       EM F1 EM F1 EM F1\n                                                                               |25.4, 37.2[16.6 21.1[26.0 32.8 [22.2 31.2\n                                                                2+SuRe      38.8 53.523.8.31.036.6 47.934 4 48.5\n                                                                 +EmbQA (ours) 42.0 55.8|27.4 36.642.2 54.438.2 52.1\n                                                                DPR            20.6 21.7[10.8 13.5[25.0 34.2[23.8 34.4\n                                                                 +SuRe         25.0 31.9 14.2 16.038.8 52.336.0 49.6\n                                                                 +EmbQA (ours) |29.8 36.3 16.8 21.0    38.0 52.0\n                                                                                                   43.0 54.4\n                                                                 Contriever   [22.6 35.4\n                                                                                     [16.6 20.7[25.8 32.8\n                                                                                                             25.2 34.2\n                                                                 +SuRe          33.8 50.6 21.0 29.3 39.0 52.834.4 48.5\n                                                                 +EmbQA (ours) 36.6 52.7 26.4 34.2 42.2 53.6\n                  Try different embedding methods in the RAG    [BM25         [21.2 29.2               36.0 49.6\n                                                                20+SuRe        32.2 46.1 [13.8 21.7 18.8 25.319.0 26.1\n                                                                                            17.8 30.1\n                                                                                                    35.2 45.131.6 45.7\n                                                                  +EmbQA (ours) 34.8 44.3 18.6 30.5 35.8 46.035.8 48.1\n                                                                [DPR               7.8 11.0 3.8 4.5[22.2,26.718.8 27.7\n                                                                +Sure          15.0 21.8 6.4 8.540.0 51.8\n                                                                 +EmbQA (ours) 16.2 23.3 7.6 9.6             32.6 47.7\n                                                                                                   |40.2 49.433.4 46.0\n                                                                Contriever     19.4 28.6[13.6 20.7[21.8 27.4117.8,244\n                                                                 +SuRe         28.0 41.6 17.2 25.4 39.8 51.630.2 45.0\n                                                                 +EmbQA (ours)29.8 42.3 17.4 26.2 40.6 51.8 31.6 43.0\n                                                                [BM25          [28.6 37.1[20.2 24.1 [24.0 29.4 [22.6 31.4\n                                                                20+Sure        43.6 54.7 28.4 34.1 41.6 49.0 36.6 47.3\n                                                                 +EmbQA (ours) 44.6 55.628.8 33.8 42.4 49.2 38.2 48.7\n                                                                [DPR           8.8 9.8 5.6 7.1\n                                                                                                  [29.2 32.6[25.6 31.1\n                                                                 +Sure         21.8 27.3 12.2 16.1\n                                                                                                   45.4 54.6\n                                                                                                             38.4 49.6\n                                                                 +EmbQA (ours) 22.6 29.1 13.8 17.345.8 54.7\n  AD                                                                                                      38.6 50.1\n  Facts                                                         Contriever     27.0 34.0[17.6 20.0 26.6 31.9 21.0 29.1\n\n0                                                                +Sure         38.8 50.323.8 30.4 44.0 52.9 36.4 48.1\n                                                                +EmbQA (ours)39.0 50.2\n            General-Purpose                                                              24.4 30.9 45.2 50.5 37.0 48.6\n     C-Pack  Text Embedding                                                                                              68\nhttps://arxiv.org/pdf/2503.01606\n\n  C-MTEB  C-MTP  C-TEM  Recipe\n---\nEmbedding Optimization\n\n \u26ab Better embedding always indicate a better retrieval results:\n\n   \u26ab  Selecting a more suitable embedding method\n\n   \u26ab  Fine-tuning the embedding model\n\n      An in-context learning based method to generate prompt\n\n                                                                                       generate Query-Doc pair    Fine-tuning with pseudo data\n\n 1           A few query and\n             relevant document\n             examples\n             for each doc     You are an award\n             in documents     winning relevance                                 GPT-x\n                              expert. Suggest          Large Language Model     BARD\n                              relevant queries for                              Flan-T5\nDocuments                     this article $article                                                               69\n                              queries:                 Synthetic queries for documents\n             LLM Query Generation Prompt\n                                                       \"Labeled data\"                  9\n---\nFine-tuning on RAG\n\n\u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n  \u26ab  Retriever fine-tuning\n\n  \u26ab  Generator fine-tuning\n\n70\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n      \u26ab  Retriever fine-tuning\n\n      \u26ab  Generator fine-tuning\n\n                              A small LM\n\n    Using the attention scores\n    annotate which documents\n    the LM \u201cprefers\u201d.\n\n                                                DecSource LM\n                                                    Fusion-in-Decoder\n                                                Enc Enc. Enc\n    Source Task                                     Q+D1 Q+D2Q+DN\n                                                    Retrieve\n                                                      N Docs\n  Positives         Negatives                   Pre-Trained Retriever    71\nGround Truth U    ANCE Sampling\n    -Top-K FiDAtt                               Target LMs Target Tasks\n    https://aclanthology.org/2023.acl-long.136.pdf\n                                     Generic                GCMETRY\n                                     Plug-In                  WAKT\n   Augmentation-Adapted Retriever                           RISTORY\n                                                            LITERATRE\n                                                            SCIENCE\n                                                              MATH\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n    \u26ab Retriever fine-tuning\n\n    \u26ab Generator fine-tuning\n\n    Product Search                 Premium hiking bag.            Unstructured Data                                             Structured Data (Negative)\n    Black large capacity hiking    Size: Max Color: Black                                     Structured Data (Positive)\n    bag, made of canvas.           Material: canvas               These erasable mood pencils     [MASK1] changing [MASK2]       Tire Specifications: Material:\n\nCode Search\nGiven two numbers, the\n\nlargest number is returned\nQuery\n]\nPretrained Language Model\n                               are made of quality wood       [MASK3] with [MASK4]            Rubber Tire Size: 16X6.50-8\n                               and color temperature          Function: removing wrong        Tire Type: Tubeless Rim\ndef compare(a, b):             coating, have non-fading        writing Material: [MASK2]      Width: 5.375\" Tread Depth:\nreturn max(a, b)          colors.                              Changing Size: 18 x 0.5 cm     7.1mm\" Pattern: P332\nStructured Data (Positive)\n Structured Data (Negative)                                               T5\n                               Structured Data Alignment (Loss: C sDA)      Masked Entity Prediction (Loss: LP)\n\n    Training    Push Away                           Prediction                                          erasers\n                                                    [MASK1] Color\n                Ground Truth                        [MASK2] mood\n                                Align               [MASK3] pencils,\n    .                                               [MASK4]\n\n    Embedding Space    Optimized Embedding Space    Add an entity prediction loss in the fine-tuning           72\n---\nK\nING'S\nCollege\nLONDON\n\nThank you\n\nDr Lin Gui\nLin.1.Gui@kcl.ac.uk\nwww.kcl.ac.uk/people/lin-gui\n\n"
    }
}