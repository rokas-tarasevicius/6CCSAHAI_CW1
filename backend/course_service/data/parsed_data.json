{
    "data/raw/vector25aug-v2.pdf": {
        "metadata": {
            "file_name": "vector25aug-v2.pdf",
            "file_type": "pdf",
            "content_length": 68177,
            "language": "en",
            "extraction_timestamp": "2025-11-29T21:52:09.907635+00:00",
            "timezone": "utc"
        },
        "content": "Vector     Word Meaning\nSemantics &\nEmbeddings\n---\nWhat do words mean?\n\nN-gram or text classification methods we've seen so far\n\u25e6  Words are just strings (or indices wi in a vocabulary list)\n\u25e6  That's not very satisfactory!\nIntroductory logic classes:\n\u25e6  The meaning of \"dog\" is DOG; cat is CAT\n   \u2200x DOG(x) \u27f6 MAMMAL(x)\nOld linguistics joke by Barbara Partee in 1967:\n\u25e6  Q: What's the meaning of life?\n\u25e6  A: LIFE\nThat seems hardly better!\n---\nDesiderata\n\nWhat should a theory of word meaning do for us?\nLet's look at some desiderata\nFrom lexical semantics, the linguistic study of word\nmeaning\n---\n   Lemmas and senses\n          lemma\n          mouse (N)\n\nsense     1. any of numerous small rodents...\n          2. a hand-operated device that controls\n          a cursor...    Modified from the online thesaurus WordNet\n\n  A sense or \u201cconcept \u201d is the meaning component of a word\n  Lemmas can be polysemous (have multiple senses)\n---\nRelations between senses: Synonymy\n\nSynonyms have the same meaning in some or all\ncontexts.\n\u25e6  filbert / hazelnut\n\u25e6  couch / sofa\n\u25e6  big / large\n\u25e6  automobile / car\n\u25e6  vomit / throw up\n\u25e6  water / H\u20820\n---\nRelations between senses: Synonymy\n\nNote that there are probably no examples of perfect\nsynonymy.\n\u25e6  Even if many aspects of meaning are identical\n\u25e6  Still may differ based on politeness, slang, register, genre,\n   etc.\n---\nRelation: Synonymy?\n\nwater/H\u20820\n\"H\u20820\" in a surfing guide?\nbig/large\nmy big sister != my large sister\n---\nThe Linguistic Principle of Contrast\n\nDifference in form \u00e0 difference in meaning\n---\nAbb\u00e9 Gabriel Girard 1718                       LA' JUSTESSE\n                                               DE LA\nRe: \"exact\" synonyms                           LANGUE FRANCOISE.\n                                               o v\n\"jc ne crois pas qu'il y ait de                LES DIFFERENTES SIGNIFICATIONS\n                                               DES MOTS QUI PASSENT\nmot fynonime dans aucune                       POU.R\n \"                                             SYNONIMES\nLangue. Je le dis par con-                     PAr M.PAbb\u00c9 GIRARD C.D.M.D.D.B.\n [I do not believe that there      SPIRAT\n is a synonymous word in any\n language]                         A PARIS,\n                                   CheZ I.AURENT D'HOURY, IMprimeur-\n Lbraire, au bas de la rue de la Harpe,vis-\n d vis la rue S. Severin, au Saint Efprit.\n                                               M DCC.XVIII.\n                                   Avce Approbason & Frivilegs dus Roy.\n                        Thanks to Mark Aronoff!\n---\nRelation: Similarity\n\nWords with similar meanings. Not synonyms, but sharing\nsome element of meaning\n\ncar,  bicycle\ncow,  horse\n---\nAsk humans how similar 2 words are\n\nword1      word2          similarity\nvanish     disappear      9.8\nbehave     obey           7.3\nbelief     impression     5.95\nmuscle     bone           3.65\nmodest     flexible       0.98\nhole       agreement      0.3\n\n          SimLex-999 dataset (Hill et al., 2015)\n---\n    Relation: Word relatedness\n\nAlso called \"word association\"\nWords can be related in any way, perhaps via a semantic\nframe or field\n\n \u25e6  coffee, tea: similar\n \u25e6  coffee, cup: related, not similar\n---\nSemantic field\n\nWords that\n\u25e6  cover a particular semantic domain\n\u25e6  bear structured relations with each other.\n\n hospitals\n   surgeon, scalpel, nurse, anaesthetic, hospital\n restaurants\n   waiter, menu, plate, food, menu, chef\n houses\n   door, roof, kitchen, family, bed\n---\nRelation: Antonymy\n\nSenses that are opposites with respect to only one\nfeature of meaning\nOtherwise, they are very similar!\n    dark/light      short/long fast/slow  rise/fall\n    hot/cold        up/down      in/out\nMore formally: antonyms can\n \u25e6    define a binary opposition or be at opposite ends of a scale\n   \u25e6  long/short, fast/slow\n \u25e6    Be reversives:\n   \u25e6  rise/fall, up/down\n---\nConnotation (sentiment)\n\n\u2022 Words have affective meanings\n  \u2022     Positive connotations (happy)\n  \u2022     Negative connotations (sad)\n\u2022 Connotations can be subtle:\n  \u2022     Positive connotation: copy, replica, reproduction\n  \u2022     Negative connotation: fake, knockoff, forgery\n\u2022 Evaluation (sentiment!)\n  \u2022     Positive evaluation (great, love)\n  \u2022     Negative evaluation (terrible, hate)\n---\nConnotation\n                                          Osgood et al. (1957)\nWords seem to vary along 3 affective dimensions:\n\u25e6  valence: the pleasantness of the stimulus\n\u25e6  arousal: the intensity of emotion provoked by the stimulus\n\u25e6  dominance: the degree of control exerted by the stimulus\n\n                  Word          Score      Word         Score\n    Valence       love           1.000     toxic              0.008\n                  happy          1.000     nightmare          0.005\n    Arousal       elated         0.960     mellow             0.069\n                  frenzy         0.965     napping            0.046\n    Dominance     powerful       0.991     weak               0.045\n                  leadership     0.983     empty              0.081\n\n                                           Values from NRC VAD Lexicon (Mohammad 2018)\n---\nSo far\n\nConcepts or word senses\n\u25e6  Have a complex many-to-many association with words (homonymy,\n   multiple senses)\nHave relations with each other\n\u25e6  Synonymy\n\u25e6  Antonymy\n\u25e6  Similarity\n\u25e6  Relatedness\n\u25e6  Connotation\n---\nVector     Word Meaning\nSemantics &\nEmbeddings\n---\nVector     Vector Semantics\nSemantics &\nEmbeddings\n---\nComputational models of word meaning\n\nCan we build a theory of how to represent word\nmeaning, that accounts for at least some of the\ndesiderata?\nWe'll introduce vector semantics\n The standard model in language processing!\n Handles many of our goals!\n---\nLudwig Wittgenstein\n\nPI #43:\n\"The meaning of a word is its use in the language\"\n---\nLet's define words by their usages\n\nOne way to define \"usage\":\nwords are defined by their environments (the words around them)\n\nZellig Harris (1954):\nIf A and B have almost identical environments we say that they\nare synonyms.\n---\nWhat does recent English borrowing ongchoi mean?\n\nSuppose you see these sentences:\n    \u2022 Ong choi is delicious saut\u00e9ed with garlic.\n    \u2022 Ong choi is superb over rice\n    \u2022 Ong choi leaves with salty sauces\nAnd you've also seen these:\n    \u2022  \u2026spinach saut\u00e9ed with garlic over rice\n    \u2022  Chard stems and leaves are delicious\n    \u2022  Collard greens and other salty leafy greens\nConclusion:\n\u25e6 Ongchoi is a leafy green like spinach, chard, or collard greens\n  \u25e6 We could conclude this based on words like \"leaves\" and \"delicious\" and \"sauteed\"\n---\nOngchoi: Ipomoea aquatica \"Water Spinach\"\n\n  \u7a7a\u5fc3\u83dc\n  kangkong\n  rau mu\u1ed1ng\n  \u2026\n\n  Yamaguchi, Wikimedia Commons, public domain\n---\nIdea 1: Defining meaning by linguistic distribution\n\nLet's define the meaning of a word by its\ndistribution in language use, meaning its\nneighboring words or grammatical environments.\n---\nIdea 2: Meaning as a point in space (Osgood et al. 1957)\n3 affective dimensions for a word\n\u25e6    valence: pleasantness\n\u25e6    arousal: intensity of emotion\n\u25e6    dominance: the degree of control exerted\n              Word          Score      Word         Score\nValence       love           1.000     toxic             0.008\n              happy          1.000     nightmare         0.005\nArousal       elated         0.960     mellow            0.069  NRC VAD Lexicon\n              frenzy         0.965     napping           0.046  (Mohammad 2018)\nDominance     powerful       0.991     weak              0.045\n\u25e6             leadership     0.983     empty             0.081\nHence the connotation of a word is a vector in 3-space\n---\nIdea 1: Defining meaning by linguistic distribution\n\nIdea 2: Meaning as a point in multidimensional space\n---\nDefining meaning as a point in space based on distribution\n?\nEach word = a vector (not just \"good\" or \"w\u2084\u2085\")\nSimilar words are \"nearby in semantic space\"   drinks\nWe build this space automatically by           alcoholic\n                                         seeing which words are\nnearby in text       candy chocolate           cider\n\n                     cream\n                                         juice\n                     0     honey                    wine\n\n                     0  corn  rice\n\n \u2022                                       beef  fried  soup drink\n                                         potatoes\n                              wheat      foods   pork   cooking\n\n                                         vegetablesbread\n---\nWe define meaning of a word as a vector\n\nCalled an \"embedding\" because it's embedded into a\nspace (see textbook)\nThe standard way to represent meaning in NLP\n Every modern NLP algorithm uses embeddings as\n the representation of word meaning\nFine-grained model of meaning for similarity\n---\nIntuition: why vectors?\n\nConsider sentiment analysis:\n\u25e6   With words, a feature is a word identity\n  \u25e6  Feature 5: 'The previous word was \"terrible\"'\n  \u25e6  requires exact same word to be in training and test\n\u25e6   With embeddings:\n  \u25e6  Feature is a word vector\n  \u25e6  'The previous word was vector [35,22,17\u2026]\n  \u25e6  Now in the test set we might see a similar vector [34,21,14]\n  \u25e6  We can generalize to similar but unseen words!!!\n---\nWe'll discuss 2 kinds of embeddings\n\nSimple count embeddings\n\u25e6  Sparse vectors\n\u25e6  Words are represented by the counts of nearby words\n\nWord2vec\n\u25e6  Dense vectors\n\u25e6  Representation is created by training a classifier to predict whether a\n   word is likely to appear nearby\n\u25e6  Later we'll discuss extensions called contextual embeddings\n---\n    From now on:\n    Computing with meaning representations\n    Vector Semantics and\n    instead of string representations\n    Embeddings\n\n   C\u8005@\u00c2(|\uff0c\u00f3|\u800c\u00ffC Nets are for fish;\n                Once you get the fish, you can forget the net.\n   \u8a00\u8005@\u00c2(\u270f\uff0c\u00f3\u270f\u800c\u00ff\u8a00 Words are for meaning;\n                Once you get the meaning, you can forget the words\n                                      \u00d1P(Zhuangzi), Chapter 26\n\n         The asphalt that Los Angeles is famous for occurs mainly on its freeways. But\n in the middle of the city is another patch of asphalt, the La Brea tar pits, and this\nasphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-\n---\nVector     Vector Semantics\nSemantics &\nEmbeddings\n---\nVector     Count-based embeddings\nSemantics &\nEmbeddings\n---\n Remember the intuition: words with similar\n neighborhoods have similar meanings\nHow to measure a word's neighborhood?\nWord-context matrix (a kind of co-occurrence matrix)\n  \u25e6  each row represents a word in the vocabulary\n  \u25e6  each column represents how often each other word in the\n     vocabulary appears nearby\n---\nWord-Context Matrix\n\n            aardvark\n            abacus    zydeco\n            adept\n                    affect\n                      agate     How often does\n\naardvark                  \u2026     agate occur\nabacus                          near abacus?\nadept\naffect\nagate\n\u2026\n\nzydeco\n---\nWord-Context Matrix\nWhat does \"nearby\" mean?\nFor right now let's say \"within 4 words\"\n---\n most common, however, to use smaller contexts, generally a window around the\nThe word-context matrix\n word, for example of 4 words to the left and 4 words to the right, in which case\n the cell represents the number of times (in some training corpus) the column word\nOne set of 4-word contexts\n occurs in such a \u00b14 word window around the row word. For example here is one\n example each of some words in their windows:\n\n is traditionally followed by       cherry       pie, a traditional dessert\n often mixed, such as               strawberry   rhubarb pie. Apple pie\n computer peripherals and personal  digital      assistants. These devices usually\n a computer. This includes          information  available on the internet\n If we then take every occurrence of each word (say strawberry) and count the con-\n text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n simplified subset of the word-word co-occurrence matrix for these four words com-\n puted from the Wikipedia corpus (Davies, 2015).\n Note in Fig. 6.5 that the two words cherry and strawberry are more similar to\n each other (both pie and sugar tend to occur in their window) than they are to other\n words like digital; conversely, digital and information are more similar to each other\n than, say, to strawberry. Fig. 6.6 shows a spatial visualization.\n---\nLet's consider a mini-matrix of 3 words.\n most common, however, to use smaller contexts, generally a window around the\n word, for example of 4 words to the left and 4 words to the right, in which case\nHow often do \"a\", \"computer\", and \"pie\n the cell represents the number of times (in some training corpus) the column word\noccur in the context of \"cherry\"?\n occurs in such a \u00b14 word window around the row word. For example here is one\n example each of some words in their windows:\n\n is traditionally followed by       cherry       pie, a traditional dessert\n often mixed, such as               strawberry   rhubarb pie. Apple pie\n computer peripherals and personal  digital      assistants. These devices usually\n a computer. This includes          information  available on the internet\n If we then take every occurrence of each word (say strawberry) and count the con-\n text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n simplified subset of the word-word co-occurrence matrix for these four words com-\n puted from the Wikipedia corpus (Davies, 2015).\n Note in Fig. 6.5 that the two words cherry and strawberry are more similar to\n each other (both pie and sugar tend to occur in their window) than they are to other\n words like digital; conversely, digital and information are more similar to each other\n than, say, to strawberry. Fig. 6.6 shows a spatial visualization.\n---\n  most common, however, to use smaller contexts, generally a window around the\n The word-          5.3             \u2022    S IMPLE COUNT- BASED EMBEDDINGS              7\n               context mini-matrix for just 4 words\n  word, for example of 4 words to the left and 4 words to the right, in which case\n  the cell represents the number of times (in some training corpus) the column word\n and 3 contexts\ncontext co-occurrence matrix is very large, because for each word in the vocabulary\n  occurs in such a \u00b14 word window around the row word. For example here is one\n(since |V |) we have to count how often it occurs with every other word in the vo-\n  example each of some words in their windows:\ncabulary, hence dimensionality |V | \u21e5 |V |. Let\u2019s therefore instead sketch the process\n               is traditionally followed by  cherry         pie, a traditional dessert\non a smaller scale. Imagine that we are going to look at only the 4 words, and only\nconsider       often mixed, such as          strawberry     rhubarb pie. Apple pie\n             the following 3 context words:     a, computer, and pie.       Furthermore let\u2019s\nassume computer peripherals and personal     digital        assistants. These devices usually\n we only count occurrences in the mini-corpus above.\n So before     a computer. This includes     information    available on the internet\n  If           looking at Fig. 5.2, compute by hand the counts for these 3 context\nwords for we then take every occurrence of each word (say strawberry) and count the con-\n             the four words cherry, strawberry, digital, and information.\n  text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n  simplified subset of the word-word co-occurrence matrix for these four words com-\n  puted from   a                                computer                    pie\n cherry        the Wikipedia corpus (Davies, 2015).\n               Note in Fig. 6.51that the two words     0                    1\nstrawberry     0                                      cherry and strawberry are more similar to\n  each other (both pie and sugar                       0                    2\n digital       0                    tend to occur in their window) than they are to other\n  words like digital; conversely, digital and          1                    0\ninformation    1                                      information are more similar to each other\n  than, say, to strawberry. Fig. 6.6 shows a 1                              0\nFigure 5.2     Co-occurrence vectors for four         spatial visualization.\n                                                words with counts from the 4 windows above,\n---\n         consider the following 3 context words: a, computer, and pie. Furthermore let\u2019s\n         assume we only count occurrences in the mini-corpus above.\n        The word-context mini-matrix for just 4 words\n          So before looking at Fig. 5.2, compute by hand the counts for these 3 context\n        and 3 contexts\n         words for the four words cherry, strawberry, digital, and information.\n\n                         a                       computer              pie\n         cherry          1                          0                  1\n         strawberry      0                          0                  2\n          digital        0                          1                  0\n         information     1                          1                  0\n\n        \u2022 Figure 5.2    Co-occurrence vectors for four words with counts from the 4 windows above,\n          This 4x3 matrix is a subset of full |V| x |V| matrix\n         showing just 3 of the potential context word dimensions. The vector for cherry is outlined in\n\n        \u2022 red. Note that a real vector would have vastly more dimensions and thus be even sparser.\n          Each word is represented by a row vector with\n          Hopefully your count matches what is shown in Fig. 5.2, so that each cell repre-\n          dimensionality [1 x |V|]\n         sents the number of times a particular word (defined by the row) occurs in a partic-\n        \u2022 ular context (defined by the word column).\n          With co-occurrence counts with each other word\n          Each row, then, is a vector representing a word.   To review some basic linear\nctor     algebra, a vector is, at heart, just a list or array of numbers. So cherry is represented\n---\n    most common, however, to use smaller contexts, generally a window around the\nhere is one example each of some words in their windows:\n    word, for example of 4 words to the left and 4 words to the right, in which case\n      is traditionally followed by      cherry           pie, a traditional dessert\n    the cell represents the number of times (in some training corpus) the column word\nA                  often mixed, such as strawberry       rhubarb pie. Apple pie\nselection from a larger word-context matrix\n    occurs in such a \u00b14 word window around the row word. For example here is one\ncomputer peripherals and personal       digital          assistants. These devices usually\n    example each of some words in their windows:\n            a computer. This includes   information      available on the internet\n      If we  is traditionally followed by cherry         pie, a traditional dessert\n            then take every occurrence of each word (say strawberry) and count the\ncontext            often mixed, such as   strawberry     rhubarb pie. Apple pie\n            words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a\n      computer peripherals and personal   digital        assistants. These devices usually\nsimplified subset of the word-word co-occurrence matrix for these four words com-\n             a computer. This includes    information    available on the internet\nputed from the Wikipedia corpus (Davies, 2015).\n    If we then take every occurrence of each word (say strawberry) and count the con-\n    text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n    simplified     aardvark     ...    computer    data  result        pie    sugar     ...\n       cherry     subset of the word-word co-occurrence matrix for these four words com-\n    puted from    0             ...       2         8     9            442     25       ...\n     strawberry    the Wikipedia corpus (Davies, 2015).\n     Note in Fig. 0             ...       0         0     1            60      19       ...\n      digital      6.5 that the two words cherry and strawberry are more similar to\n    each other (both 0          ...     1670       1683  85            5       4        ...\n    information    pie and sugar tend to occur in their window) than they are to other\n    words like    0             ...     3325       3982  378           5       13       ...\nFigure 6.6        digital; conversely, digital and information are more similar to each other\n    than,    Co-occurrence vectors for four words in the Wikipedia corpus, showing six of\nthe         say, to strawberry. Fig. 6.6 shows a spatial visualization.\n      dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\nred. Note that a real vector would have vastly more dimensions and thus be much sparser.\n---\ncomputer\n\n4000\n        information\n3000    [3982,3325]\n        digital\n2000    [1683,1670]\n\n1000\n\n1000 2000 3000 4000\ndata\n---\nThe word-context matrix\n\nWord context matrix is |V| x |V|\nThis could be 50,000 x 50,000\nMost of these numbers are zero!\nSo these are sparse vectors\nThere are efficient algorithms for storing and\ncomputing with sparse matrices\n---\nVector     Count-based embeddings\nSemantics &\nEmbeddings\n---\n           Cosine for computing word similarity\nVector\nSemantics &\nEmbeddings\n---\nhence of length |V |, or both with documents as dimensions as documents, of length\n|D|) and gives a measure of their similarity. By far the most common similarity\n Computing word similarity: Dot product and cosine\nmetric is the cosine of the angle between the vectors.\n The cosine\u2014like most measures for vector similarity used in NLP\u2014is based on\nthe dot product operator from linear algebra, also called the inner product:\n The dot product between two vectors is a scalar:\n N\n dot product(v, w) = v \u00b7 w = X vi wi = v1 w1 + v2 w2 + ... + vN wN          (6.7)\n i=1\n The dot product tends to be high when the two\nAs we will see, most metrics for similarity between vectors are based on the dot\n vectors have large values in the same dimensions\nproduct. The dot product acts as a similarity metric because it will tend to be high\njust when the two vectors have large values in the same dimensions. Alternatively,\n Dot product can thus be a useful similarity metric\nvectors that have zeros in different dimensions\u2014orthogonal vectors\u2014will have a\n between vectors\ndot product of 0, representing their strong dissimilarity.\n This raw dot product, however, has a problem as a similarity metric: it favors\nlong vectors. The vector length is defined as\n---\n  ill see, most metrics for similarity between vectors are based on the dot\nt. The dot product acts as a similarity metric because it will tend to be high\n           Problem with raw dot-product\n  n the two vectors have large values in the same dimensions. Alternatively,\n  that have zeros in different dimensions\u2014orthogonal vectors\u2014will have a\n  uct of Dot product favors long vectors\n           0, representing their strong dissimilarity.\n  s raw dot product, however, has a problem as a similarity metric: it favors\n           Dot product is higher if a vector is longer (has higher\n  ctors. The vector length is defined as\n           values in many dimension)\n           Vector length:        v\n                                 u N\n                                 uX\n                             |v| = t    v2                        (6.8)\n                                        i\n                                     i=1\n           Frequent words (of, the, you) have long vectors (since\n product is higher if a vector is longer, with higher values in each dimension.\n           they occur many times with other words).\n equent words have longer vectors, since they tend to co-occur with more\n  nd have higher co-occurrence values with each of them. The raw dot product\n  l be     So dot product overly favors frequent words\n           higher for frequent words. But this is a problem; we\u2019d like a similarity\n---\n             This raw dot product, however, has a problem as a similarity metric: it favors\nctor length  long vectors. The vector length is defined as\n  The cosine similarity metric between two vectors ~\n                                                                      v and ~\n    Alternative: cosine for                                            w thus can be computed\n                                                  computing word similarity\n :                                                          v\n                                                            u N\n                                                            uX\n                                                  |v| = t             v2            (6.8)\n                                                                      iN\n\n             The dot product is higher if                    i=1 X vi wi\n                                              a vector is longer, with higher values in each dimension.\n                                              ~\n                                              v \u00b7 ~\n               cosine(                            w          v        i=1 v\n                                ~\n                                v, ~\n                                 w) =                  =                            (6.10)\n             More frequent words have longer vectors, since they tend to co-occur with more\n                                              |              u              u\n                                              ~\n                                                   v||~\n             words and have                        w|           N            N\n                                higher co-occurrence values with each of them. The raw dot product\n                                                             uX uX\n             thus will be higher for frequent                t              2 t  2\n                                                   words. But this is a problem; we\u2019d like a similarity\n             metric that tells us how similar two words are           vi         wi\n                                                                regardless of their frequency.\n             We modify the dot                                  i=1              i=1\n                                   product to normalize for the vector length by dividing the\n      For some applications we pre-normalize each vector, by dividing it by its length,\n             dot product by the lengths of each of the two vectors. This normalized dot product\neating a     turns out to be the same as the cosine of the angle between the two vectors, following\n             unit vector of length 1. Thus we could compute a unit vector from ~\n               Based on the definition of the dot product between two vectors a                        a by\n             from the definition of the dot product between two vectors a and b:              and b\nviding it by |~\n             a|. For unit vectors, the dot product is the same as the cosine.\n      The cosine value ranges from 1 for vectors pointing in the same direction, through\n                                                   a \u00b7 b = |a||b| cos q\n or vectors that are orthogonal, to -1 for vectors pointing in opposite directions.\n                                                   |a \u00b7 b  = cos q                            (6.9)\nut raw frequency values are                        a||b|\n                                   non-negative, so the cosine for these vectors ranges\n---\nCosine as a similarity metric\n\n                                         1\n-1: vectors point in opposite directions 0.5\n+1: vectors point in same directions        50  400  150  200  250  300  350\n0: vectors are orthogonal                -0.5\n\n                                         -1\n\nBut since raw frequency values are non-negative, the\n50\ncosine for term-term matrix vectors ranges from 0\u20131\n---\n        0 for vectors that are orthogonal, to -1 for vectors pointing in opposite direction\nraw frequency values are non-negative, so the cosine for these vectors ranges\n rs        Let\u2019s see how the cosine computes which of the words cherry or digital is c\n        that are orthogonal, to -1 for vectors pointing in opposite directions.\n        But raw frequency values are non-negative, so the cosine for these vectors rang\n  0\u20131.  Cosine examples\n        in meaning to information, just using raw counts from the following shortened t\nequency values are non-negative, so the cosine for these vectors ranges\n        from 0\u20131.\n Let\u2019s see how the cosine computes which of the words cherry or digital is closer\n           Let\u2019s see how the cosine computes which of the words cherry or digital is clos\n eaning to information, just                                          pie  data  computer\n        in meaning to                   using raw counts from the following shortened table:\nsee how the cosine information, just using raw counts from the following shortened tabl\n                             computes which of the words cherry or digital is closer\n                                                  cherry              442     8   pie     data     computer\ng to       v \u2022 w  v      w       \u2211 N v wpie                     data       computer  2\n        information, just using raw counts from the following shortened table:\n                                                  i  i\n        cos(v, w) =  =   \u2022  =                i=1  digital             5    1683   1670\n                                                                   piecherry      442     8        2\n                     v w  v  w  \u2211 N               \u2211 N                     data   computer\n                                cherry               442           8       2\n                                             information              5    3982   3325\n                                     v 2                 w 2\n                                                  cherry           442     8      2\n                                             i             i          digital     5       1683     1670\n                                 i=1pie              data       computer\n                                digital              i=1\n                                                      5         1683       1670\n                                                  digital        5       1683    1670\n                          cherry     442              8               information 5       3982     3325\n                                                                4422\n                             information              5         3982 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                     information                 5         3325\n                                                         p               3982    3325\n        cos(cherry, information) =                                               p                      = .017\n                             digital         5       1683             21670\n                          information        5             442        + 82 + 22       52 + 39822 + 33252\n                                                     3982              3325\n                                                  442 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                                                442 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                        p                          5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\ns(cherry, information) =                             p                p          p                = .017\n        cos(cherry, information) =                       p                            p                = .017\n        cos(digital, information) =                                                                          = .\n                                         4422 + 82 + 22                    52 + 39822 + 33252\n                                                           4422 + 82 + 22        52 + 39822 + 33252\n                                        442 \u21e4 5 + 8 5\u00b2 + 1683\u00b2 + 1670\u00b2                   52 + 39822 + 33252\n                                                           \u21e4 3982 + 2 \u21e4 3325\n                                                  5 \u21e4 5 + 5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\n                             p                                  1683 \u21e4 3982 + 1670 \u21e4 3325\nrry, information) =                                  p          p                     p   = .017\n        cos(digital, information) =                                                                        = .996\ns(digital, information) =               p                                     p                        = .996\n           The model decides that information is way closer to digital than it is to cher\n                                4422 + 82 + 22                  52 + 39822 + 33252\n                                             2             52 + 16832 + 16702         52 + 39822 + 33252\n        result that seems                5        + 16832 + 16702                52 + 39822 + 33252\n                                sensible. Fig. 6.7 shows a visualization.\n                             51         5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\n tal, information) =         p                                        p                           = .996\n           The model decides that information is way closer to digital than it is to cherry,\n The model decides that information is way closer to digital than it is to cherry, a\n        result that seems          52 + 16832 + 16702                    52 + 39822 + 33252\n lt                                sensible. Fig. 6.7 shows a visualization.\n        that seems sensible. Fig. 6.7 shows a visualization.\n---\n   V Visualizing cosines\n    S                   E\n    (well, angles)\n    ECTOR EMANTICS AND   MBEDDINGS\n\n        \u2019\n        pie\n        \u2018\n        1:  500\n        Dimension  cherry\n                         digital    information\n\n                   500   1000  1500  2000  2500  3000\n\n                         Dimension 2: \u2018computer \u2019\nre 6.7   A (rough) graphical demonstration of cosine similarity, showing vec\n---\nVector       Cosine for computing word\nSemantics &  similarity\nEmbeddings\n---\nVector     TF-IDF\nSemantics &\nEmbeddings\n---\nBut raw frequency is a bad representation\n\n\u2022  The co-occurrence matrices we have seen represent each\n   cell by word frequencies.\n\u2022  Frequency is clearly useful; if sugar appears a lot near\n   apricot, that's useful information.\n\u2022  But overly frequent words like the, it, or they are not very\n   informative about the context\n\u2022  It's a paradox! How can we balance these two conflicting\n   constraints?\n---\n    fool                36     0.012\n    Two good            37     0\n    common solutions for word weighting\n    sweet               37     0\n ighting of the value for word t in document d , wt ,d thus combines\nth idf: tf-idf:  tf-idf value for word t in document d:\n\n                 wt ,d = tft ,d \u21e5 idft                     (6.13)\n\n idf weighting to the Shakespeare term-document matrix in Fig. 6.2.\n        Words like \"the\" or \"it\" have very low idf\nvalues for the dimension corresponding to the word         good have\n ince   PMI: (Pointwise mutual information)\n       this word appears in every document, the tf-idf algorithm\nd in any comparison                       #(% ,% )\n                        of the plays. Similarly, the word  fool, which\n        \u25e6 PMI !! , !\"   = $%& # % !#( \"\n the 37 plays, has a much lower           !  %\" )\n                                          weight.\n hting is by far the dominant way of weighting co-occurrence ma-\n          See if words like \"good\" appear more often with \"great\" than\n n retrieval, but also plays a role in many other aspects of natural\n          we would expect by chance\n akespeare\u2019s favorite adjectives, a fact probably related to the increased use of\n---\n     Term frequency (tf) in the tf-idf algorithm\n                       tft , d  = count(t , d )\n\n commonly we squash the raw frequency a bit, by using the lo\n cy  We could imagine using raw count:\n     instead. The intuition is that a word appearing 100 times\nn\u2019t make that word 100 times more likely to be relevant to the\n     tf\u209c,d = count(t,d)\n ment. We also need to do something special with counts of 0,\n he log of 0. 2\n     But instead of using raw count, we usually squash a bit:\n               (\n     tft , d   =  1 + log10 count(t , d )     if count(t , d ) > 0\n                  0                           otherwise\n\nuse log weighting, terms which occur 0 times in a document wou\n---\n      for discriminating those documents from the rest of the collection; terms that occur\nt     frequently across the entire collection aren\u2019t as helpful. The document frequency\n      df  Document frequency (df)\n      t   of a term t is the number of documents it occurs in. Document frequency is\n      not the same as the collection frequency of a term, which is the total number of\n      times the word appears in the whole collection in any document. Consider in the\n          df is the number of documents t occurs in.\n      collection of Shakespeare\u2019s 37 plays the two words Romeo and action. The words\n             t\n      have identical collection frequencies (they both occur 113 times in all the plays) but\n          (note this is not collection frequency: total count across\n      very different document frequencies, since Romeo only occurs in a single play. If\n          all documents)\n      our goal is to find documents about the romantic tribulations of Romeo, the word\n          \"Romeo\" is very distinctive for one Shakespeare play:\n      Romeo should be highly weighted, but not action:\n                         Collection Frequency  Document Frequency\n              Romeo      113                   1\n              action     113                   31\n          We emphasize discriminative words like Romeo via the inverse document fre-\nf     quency or idf term weight (Sparck Jones, 1972). The idf is defined using the frac-\n      tion N /df , where N is the total number of documents in the collection, and df is\n---\ner of documents in many collections, this measure\n      common as to be completely non-discriminative since they o\n      Inverse document frequency (idf)\n      good or sweet.3\ng function. The resulting definition for inverse\nus                                           Word         df     idf\n            \u2713        \u25c6                       Romeo        1      1.57\n      idf  = log  N                          salad        2      1.27\n                                             Falstaff     4         (6.13)\n      t           10  dft                    forest       12     0.967\n                                                                 0.489\n                                             battle       21     0.246\n e words in the Shakespeare corpus, ranging from\n      N is the total number of documents     wit          34     0.037\n hich occur in only one play like fool                    36     0.012\n      in the collection                      Romeo, to those that\n alstaff, to those which are very            good         37     0\n                                            common like fool or so\n                                             sweet        37     0\nn-discriminative since they occur in all 37 plays like\n---\nWhat is a document?\n\nCould be a play or a Wikipedia article\nBut for the purposes of tf-idf, documents can be\nanything; we often call each paragraph a document!\n---\n defined either by Eq. 6.11 or by Eq. 6.12) with id\n            Final tf-idf weighted value for a word\n                            wt , d           = tft , d \u21e5 idft\n           Raw counts:                           6.3            \u2022    W ORDS AND VECTORS  7\n            As You Like It   Twelfth Night    Julius Caesar    Henry V\n f-idf weighting to the Shakespeare term-documen\n            battle          1      0                7             13\n             good           114    80               62            89\n on          fool           36     58               1             4\n    Eq.                 6.12. Note that the tf-idf values for the\n             wit            20     15               2             3\n            Figure 6.2       The term-document matrix for four words in four Shakespeare plays. Each cell\n ord 6      \u2022  V            S      E\n           tf good have now all become 0; since this wor\n HAPTER     -idf: ECTOR          EMANTICS AND    MBEDDINGS\n            contains the number of times the (row) word occurs in the (column) document.\n idf algorithm leads it to be ignored. Similarly, the\n                        As You Like It  Twelfth Night           Julius Caesar  Henry V\n            represented as a count vector, a column in Fig. 6.3.\n            battle      0.246           0                       0.454          0.520\n vector     To review some basic linear algebra, a vector is, at heart, just a list or array of\n  f the 37 plays, has a much lower weight.\n            good        0               0                       0              0\n            numbers. So As You Like It is represented as the list [1,114,36,20] (the first column\n            fool        0.030           0.033                   0.0012         0.0019\n            vector in Fig. 6.3) and Julius Caesar is represented as the list [7,62,1,2] (the third\n            wit         0.085           0.081                   0.048          0.054\nr space     column vector).      A vector space is a collection of vectors, characterized by their\n            Figure 6.9       A tf-idf weighted term-document matrix for four words in four Shakespeare\n---\nVector     TF-IDF\nSemantics &\nEmbeddings\n---\nVector     Word2vec\nSemantics &\nEmbeddings\n---\nSparse versus dense vectors\n\nCount vectors (even if weighted by tf-idf)\n\u25e6  long (length |V|= 20,000 to 50,000)\n\u25e6  sparse (most elements are zero)\nAlternative: learn vectors which are\n\u25e6  short (length 50-1000)\n\u25e6  dense (most elements are non-zero)\n---\nSparse versus dense vectors\n\nWhy dense vectors?\n\u25e6   Short vectors may be easier to use as features in machine\n    learning (fewer weights to tune)\n\u25e6   Dense vectors may generalize better than explicit counts\n\u25e6   Dense vectors may do better at capturing synonymy:\n  \u25e6 car and automobile are synonyms; but are distinct dimensions\n    \u25e6  a word with car as a neighbor and a word with automobile as a\n       neighbor should be similar, but aren't\n\u25e6   In practice, they work better\n       56\n---\nCommon methods for getting short dense vectors\n\n\u201cNeural Language Model\u201d-inspired models\n\u25e6  Word2vec (skipgram, CBOW), GloVe\nSingular Value Decomposition (SVD)\n\u25e6  A special case of this is called LSA \u2013 Latent Semantic\n   Analysis\nAlternative to these \"static embeddings\":\n  \u2022     Contextual Embeddings (ELMo, BERT)\n  \u2022     Compute distinct embeddings for a word in its context\n  \u2022     Separate embeddings for each token of a word\n---\nSimple static embeddings you can download!\n\nWord2vec (Mikolov et al)\nhttps://code.google.com/archive/p/word2vec/\n\nGloVe (Pennington, Socher, Manning)\nhttp://nlp.stanford.edu/projects/glove/\n---\nWord2vec\nPopular embedding method\n\u2022     Very fast to train\n\u2022     Code available on the web\nIdea: predict rather than count\nWord2vec provides various options. We'll do:\nskip-gram with negative sampling (SGNS)\n---\n  Word2vec\nInstead of counting how often each word w occurs near \"apricot\"\n\u25e6   Train a classifier on a binary prediction task:\n  \u25e6  Is w likely to show up near \"apricot\"?\nWe don\u2019t actually care about this task\n  \u25e6  But we'll take the learned classifier weights as the word embeddings\nBig idea: self-supervision:\n  \u25e6  A word c that occurs near apricot in the corpus cats as the gold \"correct\n     answer\" for supervised learning\n  \u25e6  No need for human labels\n  \u25e6  Bengio et al. (2003); Collobert et al. (2011)\n---\nApproach: predict if candidate word c is a \"neighbor\"\n\n1.  Treat the target word t and a neighboring context word c\n    as positive examples.\n2.  Randomly sample other words in the lexicon to get\n    negative examples\n3.  Use logistic regression to train a classifier to distinguish\n    those two cases\n4.  Use the learned weights as the embeddings\n---\nSkip-Gram Training Data\n\nAssume a +/- 2 word window, given training sentence:\n\n\u2026lemon, a [tablespoon of apricot jam, a] pinch\u2026\nc1    c2 [target]      c3             c4\n---\nSkip-Gram Classifier\n\n(assuming a +/- 2 word window)\n\n\u2026lemon, a [tablespoon of apricot jam, a] pinch\u2026\n c1            c2 [target]          c3  c4\nGoal: train a classifier that is given a candidate (word, context) pair\n (apricot, jam)\n (apricot, aardvark)\n\u2026\nAnd assigns each pair a probability:\nP(+|w, c)\nP(\u2212|w, c) = 1 \u2212 P(+|w, c)\n---\nSimilarity is computed from dot product\n\nRemember: two vectors are similar if they have a high\ndot product\n\u25e6 Cosine is just a normalized dot product\nSo:\n\u25e6 Similarity(w,c) \u221d w \u00b7 c\nWe\u2019ll need to normalize to get a probability\n\u25e6 (cosine isn't a probability either)\n           64\n---\ndel   Turning dot products into probabilities\n      the probability that word  c is a real context word for target word w\n\n      Sim(    P(+|w, c) =       s (c \u00b7 w) =    1\n              w,c) \u2248 w \u00b7 c                     1 + exp (\u2212c \u00b7 w)\n                                                6.8  \u2022         W ORD 2 VEC\n      To turn this into a probability\nmoid function returns a number between 0 and 1, but to make it a prob\n del the probability that word   c is a real context word for target word w\n      We'll use the sigmoid from logistic regression:\n so need the total probability of the two possible events ( c is a context\nsn\u2019t a context word) to sum to 1. We thus estimate the probability that\neal context   P(+|w, c) =       s (c \u00b7 w) =    1\n      word for          w as:                  1 + exp (\u2212c \u00b7 w)\n\n      moid function returns a number between 0 and 1, but to make it a proba\n              P(\u2212|w, c) =         1 \u2212 P(+|w, c)\n lso need the total probability of the two possible events ( c is a context\nisn\u2019t a context word)   = s (\u2212c \u00b7 w) =         1\n                        to sum to 1. We thus estimate the probability that w\n                                               1 + exp (c \u00b7 w)\n---\n    How  P(\u2212|w, c) =                    1 \u2212 P(+|w, c)           6.8  \u2022  W ORD 2 VEC  19\n         Skip-Gram Classifier computes P(+|w, c)\n    We model the probability that word c is a real context           1\n                        = s (\u2212c \u00b7 w) =                         word for target word w as:     (6.29)\n                                                      1 + exp (c \u00b7 w)\n                     P(+|w, c) =        s (c \u00b7 w) = 1 + exp1                         (6.28)\nation 6.28 gives us                                            (\u2212c \u00b7 w)\n    This is             the probability for one word, but there are many context\n             for one context word, but we have lots of context words.\nds  The sigmoid function returns a number between 0 and 1, but to make it a probability\n    in the window. Skip-gram makes the simplifying assumption that all context\n    We'll assume independence and just multiply them:\n    we\u2019ll also need the total probability of the two possible events (c is a context word,\nds are independent, allowing us to just multiply their probabilities:\n    and c isn\u2019t a context word) to sum to 1. We thus estimate the probability that word c\n    is not a real context word for w as:\n                                                   L\n                     P(\u2212|w, c) =        1 \u2212 P(+|w, Y\n                        P(+|w, c1:L ) = c)            s (ci \u00b7 w)                              (6.30)\n                        = s (\u2212c \u00b7 w) = i=1                     1                     (6.29)\n                                                   1 + exp (c \u00b7 w)\n                                                  L\n    Equation 6.28 gives us the probability for    X\n                     log P(+|w, c                      ) one word, but there are many context\n    words in the window. Skip-gram makes   =          log s (c          \u00b7 w)                  (6.31)\n                                                  the simplifying assumption that all context\n                                        1:L                             i\n    words are independent, allowing us to just multiply their probabilities:\n                                                   i=1\n                    L\nmmary, skip-gram    Y\n                     trains a probabilistic classifier that, given a test target word\n---\nSkip-gram classifier: summary\n\nA probabilistic classifier, given\n\u2022     a test target word w\n\u2022     its context window of L words c1:L\nEstimates probability that w occurs in this window based\non similarity of w (embeddings) to c1:L (embeddings).\n\nTo compute this, we just need embeddings for all the\nwords.\n---\nThese embeddings we'll need: a set for w, a set for c\n\n1..d\naardvark  1\napricot\n\n\u2026         \u2026  W target words\n\n& =  zebra  |V|\n     aardvark  |V|+1\n     apricot\n                    C  context & noise\n     \u2026         \u2026       words\n\n     zebra     2V\n---\nVector     Word2vec\nSemantics &\nEmbeddings\n---\nVector       Word2vec: Learning the\nSemantics &  embeddings\nEmbeddings\n---\nWord2vec learns embeddings by starting with an initial set of embedding vecto\nSkip-Gram Training data\nand then iteratively shifting the embedding of each word w to be more like the em\nbeddings of words that occur nearby in texts, and less like the embeddings of word\nthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n... lemon,     a [tablespoon of apricot jam,      a] pinch ...\n           \u2026lemon, a [tablespoon of apricot jam,     a] pinch\u2026\n                c1 c1    c2  t       c3           c4\nThis example has a target word c2 [target]  c3       c4\n                             t (apricot), and 4 context words in the L = \u00b1\nwindow, resulting in 4 positive training instances (on the left below):\n positive examples +                        negative examples -\n t         c                    t           c           t        c\n apricot   tablespoon           apricot     aardvark    apricot  seven\n apricot   of                   apricot     my          apricot  forever\n                                apricot     where       apricot  dear\n apricot   jam                                                         71\n apricot   a                    apricot     coaxial     apricot  if\n---\nWord2vec learns embeddings by starting with an initial set of embedding vecto\nSkip-Gram Training data\nand then iteratively shifting the embedding of each word w to be more like the em\nbeddings of words that occur nearby in texts, and less like the embeddings of word\nthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n... lemon,     a [tablespoon of apricot jam,        a] pinch ...\n           \u2026lemon, a [tablespoon of apricot jam,       a] pinch\u2026\n                c1 c1    c2    t        c3          c4\nThis example has a target word c2 [target]    c3       c4\n                             t (apricot), and 4 context words in the L = \u00b1\nwindow, resulting in 4 positive training instances (on the left below):\n positive examples +         For              negative examples -\n t         c                       each positive\n                                   t          c           t        c\n                             example we'll grab k\n apricot   tablespoon              apricot    aardvark    apricot  seven\n                             negative examples,\n apricot   of                      apricot    my          apricot  forever\n                             sampling by frequency\n                                   apricot    where       apricot  dear\n apricot   jam                                                         72\n apricot   a                       apricot    coaxial     apricot  if\n---\n ord2vec learns embeddings by starting with an initial set of embedding vectors\n    Word2vec learns embeddings by starting with an initial set of embedding vecto\n d then Skip-Gram Training data\n         iteratively shifting the embedding of each word w to be more like the em-\n    and then iteratively shifting the embedding of each word w to be more like the em\n ddings of words that occur nearby in texts, and less like the embeddings of words\n    beddings of words that occur nearby in texts, and less like the embeddings of word\nat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n    that don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n . lemon, a [tablespoon of apricot jam,                   a] pinch ...\n    ... lemon,          a [tablespoon of apricot jam,                     a] pinch ...\n                    \u2026lemon, a [tablespoon of apricot jam,                 a] pinch\u2026\n                c1       c1  c2     t        c3           c4\n                             c1     c2       t           c3               c4\n  This example has a target word t           c2 [target]       c3         c4\n         This example has a         (apricot), and 4 context words in the L = \u00b12\n                              target word t (apricot), and 4 context words in the L = \u00b1\nindow, resulting in 4 positive training instances (on the left below):\n    window, resulting in 4 positive training instances (on the left below):\n          positive examples +\n    positive examples +                                        negative examples -\n    t     tc        c                            negative examples -\n                                        t        t  c          c     t      t  c     c\n                                        apricotapricot         aardvark     apricot  seven\n          apricot   tablespoon                      aardvark         apricot seven\n    apricot tablespoon                  apricotapricot         my           apricot  forever\n          apricot   of                              my               apricot forever\n    apricot of                                   apricot       where        apricot  dear\n          apricot   jam                 apricot     where            apricot dear    73\n    apricot jam                                  apricot       coaxial      apricot  if\n          apricot   a                   apricot     coaxial          apricot if\n    apricot a\n---\n        Word2vec: how to learn vectors\n\nGiven the set of positive and negative training instances,\nand an initial set of embedding vectors\nThe goal of learning is to adjust those word vectors such\nthat we:\n \u25e6  Maximize the similarity of the target word, context word pairs\n    (w , c\u209a\u2092\u209b) drawn from the positive data\n \u25e6  Minimize the similarity of the (w , cneg) pairs drawn from the\n    negative data.\n\n    8/24/25                                74\n---\n \u2022 Minimize the similarity of the (w, cneg ) pairs from the negative examples.\n If we consider one word/context pair (w, c  ) with its k noise words c  ...c  ,\nLoss function for one w with c               , c                  ...c\n                   pos                       pos  neg1               neg1     negk\n we can express these two goals as the following loss function L         negk\n                                                                  to be minimized\n (hence the \u2212); here the first term expresses that we want the classifier to assign the\nMaximize the similarity of the target with the actual context words,\n real context word cpos a high probability of being a neighbor, and the second term\nand minimize the similarity of the target with the k negative sampled\n expresses that we want to assign each of the noise words cnegi a high probability of\nnon-neighbor words.\n being a non-neighbor, all multiplied because we assume independence:\n                   [    k                         ]\n   LCE  = \u2212 log    P(+|w, cpos ) \u220f P(\u2212|w, cnegi )\n        [               i=1 k                          ]\n        = \u2212 log P(+|w, cpos ) + \u2211 log P(\u2212|w, cnegi )\n        [                    i=1                                  ]\n                             k\n        = \u2212 log P(+|w, cpos ) + \u2211 log \u23081 \u2212 P(+|w, cnegi )\u2309\n        [               ki=1                           ]\n        = \u2212 log s (cpos \u00b7 w) + \u2211 log s (\u2212cnegi \u00b7 w)                        (6.34)\n                        i=1\n---\nLearning the classifier\n\nHow to learn?\n\u25e6  Stochastic gradient descent!\n\nWe\u2019ll adjust the word weights to\n\u25e6  make the positive pairs more likely\n\u25e6  and the negative pairs less likely,\n\u25e6  over the entire training set.\n---\nIntuition of one step of gradient descent\n\naardvark\n                    move apricot and jam closer,\napricot              w  increasing c\u209a\u2092\u209b z w\nW\n\n                              \u201c\u2026apricot jam\u2026\u201d\n!           zebra\n         aardvark             move apricot and matrix apart\n\n   jam               c\u209a\u2092\u209b     decreasing cneg1 z w\n   C k=2 matrix      cneg1    move apricot and Tolstoy apart\n\n          Tolstoy    cneg2    decreasing cneg2 z w\n            zebra\n---\n   Reminder: gradient descent\n\n   \u2022 At each step\n     \u2022      Direction: We move in the reverse direction from the\nISTIC R     gradient of the loss function\n     EGRESSION\n     \u2022      Magnitude: we move the value of this gradient\n            ! ! (# $ ; & , ( ) weighted by a learning rate \u03b7\n\n     \u2022      !\"\n            Higher learning rate means move w faster\n\n              wt +1 = wt \u2212 h d L( f (x; w), y)\n                             dw\n---\n    pos                                         k\nesses that we want to assign each of        \u2211       \u2308    \u2309\n    = \u2212 log P(+|w, c                        the noise words c  a high probability o\n                      ) +                           log 1 \u2212 P(+|w, c  )\n    The derivatives of the loss                                neg\n           pos                                           function\ng a non-neighbor, all                                          i  negi\n \u2022  V  S                    multiplied because we assume independence:\n       ECTOR EMANTICS AND E MBEDDINGS\n             [            [                 ki=1         ]     ]\n   k\n   \u220f\n       L  = \u2212 log         P(+|w, c         )\u2211 P(\u2212|w, c   )\n          = \u2212 log s (c         \u00b7 w) +           log s (\u2212c  \u00b7 w)        (6.34\n       CE                      pos  pos                  neg\nof as an exercise at the end of the chapter):              neg\n                                                           i  i\n                                           i=1\n             [                             i=1 k                   ]\ns, we want to maximize the dot                  \u2211\n             \u2202 LCE                  product of the word with the actual contex\n             = \u2212 log P(+|w, c              ) +      log P(\u2212|w, c   )\n , and minimize the = [s (c         \u00b7 w) \u2212 1]w\n                       dot products of the word with the k negative sampled non\nbor words.   \u2202 cpos [          pos  pos         i=1               negi    ]\n             \u2202 L                                k\n e minimize                                     \u2211\n                this loss function using stochastic gradient descent.      Fig. 6.1\n                = CE  = [s (cneg \u00b7 w)]w                 \u2308                 \u2309\ns the        \u2202 c    \u2212 log P(+|w, c         ) +         log 1 \u2212 P(+|w, c   )\n          intuition of one step of learning.\n                neg                 pos                                   negi\n                       [                        i=1          k     ]\n             \u2202 L                         k                   X\n                 CE    = [s (c      \u00b7 w) \u2211\n             aardvark          pos         \u2212 1]cpos +             [s (cneg \u00b7 w)]cneg\n                = \u2212 log s (c        \u00b7 w) +       log s (\u2212c        \u00b7 w)\n                \u2202 w            pos  move apricot and neg                   i    i    (6.3\n                                                                  jam closer,\n                                            i=1              i=1  i\n                apricot        w            increasing c          z w\n---\n               \u2202 w   = [s (cpos \u00b7 w) \u2212 1]cpos +  [s (cnegi \u00b7 w)]cnegi\n    Update equation in SGD                       i=1\n he update equations going from time step t to t + 1 in stochastic gradient de\nre thus:\n    Start with randomly initialized C and W matrices, then incrementally do updates\n\n        ct +1  = ct      \u2212 h [s (ct  \u00b7 wt ) \u2212 1]wt\n        pos       pos    pos\n        ct +1  = ct      \u2212 h [s (ct  \u00b7 wt )]wt\n        neg       neg    \"  neg                     k                              #\n\n        wt +1  = wt \u2212 h  [s (cpos \u00b7 wt ) \u2212 1]cpos + X[s (cnegi \u00b7 wt )]cnegi\n                                                  i=1\nust as in logistic regression, then, the learning algorithm starts with randoml\n alized W and C matrices, and then walks through the training corpus using gra\nescent to move W and C so as to maximize the objective in Eq. 6.34 by makin\n---\nTwo sets of embeddings\n\nSGNS learns two sets of embeddings\nTarget embeddings matrix W\nContext embedding matrix C\nIt's common to just add them together,\nrepresenting word i as the vector wi + ci\n---\n Summary: How to learn word2vec (skip-gram)\n embeddings\nStart with V random d-dimensional vectors as initial\nembeddings\nTrain a classifier based on embedding similarity\n  \u25e6 Take a corpus and take pairs of words that co-occur as positive\n  examples\n  \u25e6 Take pairs of words that don't co-occur as negative examples\n  \u25e6 Train the classifier to distinguish these by slowly adjusting all\n  the embeddings to improve the classifier performance\n  \u25e6 Throw away the classifier code and keep the embeddings.\n---\nVector       Word2vec: Learning the\nSemantics &  embeddings\nEmbeddings\n---\nVector     Properties of Embeddings\nSemantics &\nEmbeddings\n---\nThe kinds of neighbors depend on window size\n\nSmall windows (C= +/- 2) : nearest words are syntactically\nsimilar words in same taxonomy\n\u25e6Hogwarts nearest neighbors are other fictional schools\n\u25e6Sunnydale, Evernight, Blandings\nLarge windows (C= +/- 5) : nearest words are related\nwords in same semantic field\n\u25e6Hogwarts nearest neighbors are Harry Potter world:\n\u25e6Dumbledore, half-blood, Malfoy\n---\nability to capture relational meanings. In an important early vector space model of\n   Analogical relations\ncognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model\nfor solving simple analogy problems of the form a is to b as a* is to what?. In such\n   The classic parallelogram model of analogical reasoning\nproblems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as\ngrape is to  , and must fill in the word vine. In the parallelogram model, illus-\n   (Rumelhart and Abrahamson 1973)    #  \u00bb  # \u00bb\ntrated in Fig. 6.15, the vector from the word apple to the word tree (= apple \u2212 tree)\n   To solve: \"apple         #  \u00bb\nis added to the vector for  is to tree as grape is to _____\"\n                            grape (grape); the nearest word to that point is returned.\n   Add tree \u2013 apple to grape to get vine\n                                      tree\n             apple\n\n   vine\n   grape\n---\n llelogram method received more modern attention because of\n       Analogical relations via parallelogram\n rd2vec or GloVe vectors ( Mikolov et al. 2013b, Levy and Gold\n gton et al. 2014). For example, the result       #\n                 # of the expression (kin\n       The parallelogram                        \u00bb  #  \u00bb  #  \u00bb\n                 #           method can solve analogies with\n is a vector close to        \u00bb\n                 #  \u00bb  queen. Similarly, Paris \u2212 France + Italy)\n       both sparse and dense embeddings (Turney and\n hat is close to Rome. The embedding model thus seems to be ex\n       Littman 2005, Mikolov et al. 2013b)\n tions of relations like MALE - FEMALE , or CAPITAL - CITY- OF, or\n       /    king \u2013 man + woman is close to queen\nIVE SUPERLATIVE, as shown in Fig. 6.16 from GloVe.\nr a         Paris \u2013 France + Italy is close to Rome\n       a:b::a*:b* problem, meaning the algorithm is given a, b, and\n , the parallelogram method is thus:\n            For a problem a:a*::b:b*, the parallelogram method is:\n                 \u02c6 \u2217                     \u2217\n                 b      = argmax distance(x, a  \u2212 a + b)\n                        x\n---\nStructure in GloVE Embedding space\n0.5                               heiress\n\n0.4\n            niece                        countess\n     0.3    aunt                         duchess\n            $ister\n     0.2                                 empress\n\n     0.1                          madam\n\n0           nephew    heir\n\n-0.1        uncle     ;woman             ue earl,\n                                         queen\n-0.2         brother                     /duke\n\n-0.3                                     emperor\n\n-0.4                    man     sir       king\n\n-0.5\n\n            -0.5  -0.4  -0.3   0.2  -0.1  0  0.1  0.2  0.3  0.4  0.5\n---\nCaveats with the parallelogram method\n\nIt only seems to work for frequent words, small\ndistances and certain relations (relating countries to\ncapitals, or parts of speech), but not others. (Linzen\n2016, Gladkova et al. 2016, Ethayarajh et al. 2019a)\n\nUnderstanding analogy is an open area of research\n(Peterson et al. 2020)\n---\nEmbeddings as a window onto historical semantics\n\nTrain embeddings on different decades of historical text to see meanings shift\n\n                      ~30 million books, 1850-1990, Google Books data\na daft gay (1900s)                 b spread                          C  solemn\nflaunting       sweet                                                   awful (1850s)\ntasteful              cheerful       broadcast (1850s) soW              awe majestic\n                       pleasant                seed                     dread       pensive\n            frolicsom\u2091             circulated                       SOWS         gloomy\n              witty gay (1950s)                  scatter\n                bright               broadcast (1900s)                      horrible\ngays        bisexual                 newspapers                             appalling terrible\n                homosexual           television                             awful (1900s)     wonderful\n gay (1990s)                         radio                                       awful (1990s)\n     lesbian                       bbcbroadcast (1990s)                          aulweird\n\n            William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic Word Embeddings Reveal\n            Statistical Laws of Semantic Change. Proceedings of ACL.\n---\nEmbeddings reflect cultural bias!\n\n   Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. \"Man is to computer\n   programmer as woman is to homemaker? debiasing word embeddings.\" In NeurIPS, pp. 4349-4357. 2016.\n\n Ask \u201cParis : France :: Tokyo : x\u201d\n \u25e6 x = Japan\n Ask \u201cfather : doctor :: mother : x\u201d\n \u25e6 x = nurse\n Ask \u201cman : computer programmer :: woman : x\u201d\n \u25e6 x = homemaker\nAlgorithms that use embeddings as part of e.g., hiring searches for\nprogrammers, might lead to bias in hiring\n---\nHistorical embedding as a tool to study cultural biases\n             Garg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes.\n             Proceedings of the National Academy of Sciences 115(16), E3635\u2013E3644.\n\n\u2022     Compute a gender or ethnic bias for each adjective: e.g., how\n      much closer the adjective is to \"woman\" synonyms than\n      \"man\" synonyms, or names of particular ethnicities\n      \u2022     Embeddings for competence adjective (smart, wise,\n            brilliant, resourceful, thoughtful, logical) are biased toward\n            men, a bias slowly decreasing 1960-1990\n      \u2022     Embeddings for dehumanizing adjectives (barbaric,\n            monstrous, bizarre) were biased toward Asians in the\n            1930s, bias decreasing over the 20\u1d57\u02b0 century.\n\u2022     These match the results of old surveys done in the 1930s\n---\nVector     Properties of Embeddings\nSemantics &\nEmbeddings\n\n",
        "summary": "Purpose of the Document:\nThe document explores the concept of word meaning and the use of vector embeddings to represent and compute semantic relationships between words. It discusses various linguistic principles, computational models, and techniques for capturing word meanings and their relationships.\n\nMain Ideas:\n\u2022 Exploration of word meaning and semantic relationships\n\u2022 Introduction to vector embeddings for representing word meanings\n\u2022 Discussion of linguistic principles and computational models for word semantics\n\u2022 Techniques for capturing and computing semantic relationships\n\nKey Concepts:\n\u2022 Word meaning\n\u2022 Vector embeddings\n\u2022 Semantic relationships\n\u2022 Linguistic principles\n\u2022 Computational models\n\u2022 Co-occurrence matrices\n\u2022 TF-IDF\n\u2022 Word2vec\n\u2022 Analogical relations\n\u2022 Cultural biases in embeddings\n\nKey Takeaways:\n\u2022 Word meaning can be represented using vector embeddings.\n\u2022 Semantic relationships between words can be captured and computed using various techniques.\n\u2022 Linguistic principles and computational models play a crucial role in understanding word meanings.\n\u2022 Co-occurrence matrices and TF-IDF are methods for representing word meanings.\n\u2022 Word2vec is a popular method for learning word embeddings.\n\u2022 Analogical relations can be captured using vector embeddings.\n\u2022 Embeddings can reflect and reveal cultural biases.",
        "quiz": [
            {
                "question_text": "What is the primary focus of lexical semantics?",
                "answers": [
                    {
                        "text": "The study of word meanings and their relationships",
                        "is_correct": true,
                        "explanation": "The concept description explicitly mentions that lexical semantics is the linguistic study of word meaning, focusing on relationships like synonymy, antonymy, and semantic fields."
                    },
                    {
                        "text": "The analysis of sentence structures and grammar",
                        "is_correct": false,
                        "explanation": "This option describes syntax or grammar, not lexical semantics, which focuses on word meanings and their relationships."
                    },
                    {
                        "text": "The examination of phonetic sounds and their meanings",
                        "is_correct": false,
                        "explanation": "This option describes phonetics or phonology, not lexical semantics, which deals with word meanings."
                    },
                    {
                        "text": "The study of how words are used in different contexts",
                        "is_correct": false,
                        "explanation": "While context is important, this option describes pragmatics, not lexical semantics, which specifically focuses on word meanings and their relationships."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is a lemma in the context of word meaning?",
                "answers": [
                    {
                        "text": "A lemma is a canonical form of a word that represents its meaning, which can have multiple senses.",
                        "is_correct": true,
                        "explanation": "The concept description defines a lemma as a canonical form of a word that can have multiple senses, which directly supports this answer."
                    },
                    {
                        "text": "A lemma is a type of word that only has one meaning.",
                        "is_correct": false,
                        "explanation": "The concept description explicitly states that lemmas can be polysemous, meaning they can have multiple senses, making this option incorrect."
                    },
                    {
                        "text": "A lemma is a synonym for a word in a different language.",
                        "is_correct": false,
                        "explanation": "The concept description does not mention anything about lemmas being synonyms in different languages, making this option incorrect."
                    },
                    {
                        "text": "A lemma is a part of speech that describes actions.",
                        "is_correct": false,
                        "explanation": "The concept description does not relate lemmas to parts of speech or actions, making this option incorrect."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What does it mean for a lemma to be polysemous?",
                "answers": [
                    {
                        "text": "A lemma has multiple senses or meanings.",
                        "is_correct": true,
                        "explanation": "The content explicitly states that a lemma can be polysemous, meaning it has multiple senses."
                    },
                    {
                        "text": "A lemma is always a single word with one specific meaning.",
                        "is_correct": false,
                        "explanation": "This contradicts the provided information that lemmas can have multiple senses."
                    },
                    {
                        "text": "Polysemy refers to words that are spelled the same but have different pronunciations.",
                        "is_correct": false,
                        "explanation": "This describes homonyms, not polysemous lemmas."
                    },
                    {
                        "text": "A lemma is a type of mathematical function used in vector spaces.",
                        "is_correct": false,
                        "explanation": "This is unrelated to the linguistic concept of a lemma as described in the content."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are synonyms in the context of word meaning?",
                "answers": [
                    {
                        "text": "Words that have the same meaning in some or all contexts",
                        "is_correct": true,
                        "explanation": "This is the definition of synonyms as described in the context."
                    },
                    {
                        "text": "Words that are opposites with respect to one feature of meaning",
                        "is_correct": false,
                        "explanation": "This describes antonyms, not synonyms."
                    },
                    {
                        "text": "Words that are related but not similar",
                        "is_correct": false,
                        "explanation": "This describes word relatedness, not synonymy."
                    },
                    {
                        "text": "Words that vary along affective dimensions like valence and arousal",
                        "is_correct": false,
                        "explanation": "This describes connotation, not synonymy."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the Linguistic Principle of Contrast?",
                "answers": [
                    {
                        "text": "Difference in form implies a difference in meaning",
                        "is_correct": true,
                        "explanation": "The Linguistic Principle of Contrast states that a difference in form implies a difference in meaning."
                    },
                    {
                        "text": "Words with similar meanings are always synonyms",
                        "is_correct": false,
                        "explanation": "The content explains that words with similar meanings are not necessarily synonyms, as they may differ based on context or other factors."
                    },
                    {
                        "text": "Synonyms have identical meanings in all contexts",
                        "is_correct": false,
                        "explanation": "The content notes that there are no examples of perfect synonymy, as meanings can differ based on politeness, slang, register, or genre."
                    },
                    {
                        "text": "Antonyms are completely unrelated in meaning",
                        "is_correct": false,
                        "explanation": "The content describes antonyms as opposites with respect to only one feature of meaning, indicating they share some similarities."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the difference between 'similarity' and 'synonymy' in word meaning?",
                "answers": [
                    {
                        "text": "Synonymy refers to words that have the same meaning in some or all contexts, while similarity refers to words that share some element of meaning but are not exact synonyms.",
                        "is_correct": true,
                        "explanation": "The concept description and content context clearly differentiate between synonymy and similarity."
                    },
                    {
                        "text": "Synonymy and similarity both refer to words that have identical meanings in all contexts.",
                        "is_correct": false,
                        "explanation": "The content context explicitly states that there are no perfect synonyms and that similarity involves shared elements of meaning, not identical meanings."
                    },
                    {
                        "text": "Similarity refers to words that are opposites with respect to one feature of meaning.",
                        "is_correct": false,
                        "explanation": "This describes antonymy, not similarity, as per the content context."
                    },
                    {
                        "text": "Synonymy involves words that are related in any way, perhaps via a semantic frame or field.",
                        "is_correct": false,
                        "explanation": "This describes word relatedness, not synonymy, as per the content context."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is a semantic field?",
                "answers": [
                    {
                        "text": "A set of words that cover a particular semantic domain and bear structured relations with each other",
                        "is_correct": true,
                        "explanation": "This is the definition of a semantic field as described in the content context."
                    },
                    {
                        "text": "A collection of words that are all synonyms of each other",
                        "is_correct": false,
                        "explanation": "This is incorrect because synonyms are words that have the same or nearly the same meaning, not necessarily a semantic field."
                    },
                    {
                        "text": "A group of words that are all antonyms of each other",
                        "is_correct": false,
                        "explanation": "This is incorrect because antonyms are words that have opposite meanings, not a semantic field."
                    },
                    {
                        "text": "A list of words that are all related to a specific topic but do not have structured relations",
                        "is_correct": false,
                        "explanation": "This is incorrect because a semantic field requires structured relations among the words."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are antonyms in the context of word meaning?",
                "answers": [
                    {
                        "text": "Words that have opposite meanings",
                        "is_correct": true,
                        "explanation": "Antonyms are defined as senses that are opposites with respect to only one feature of meaning, otherwise being very similar."
                    },
                    {
                        "text": "Words that have similar meanings",
                        "is_correct": false,
                        "explanation": "This describes synonyms, not antonyms."
                    },
                    {
                        "text": "Words that are related but not similar",
                        "is_correct": false,
                        "explanation": "This describes word relatedness, not antonymy."
                    },
                    {
                        "text": "Words that share a semantic field",
                        "is_correct": false,
                        "explanation": "This describes words within a semantic field, not antonyms."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the three affective dimensions along which words vary according to Osgood et al. (1957)?",
                "answers": [
                    {
                        "text": "Valence, arousal, and dominance",
                        "is_correct": true,
                        "explanation": "Osgood et al. (1957) identified these three affective dimensions along which words vary."
                    },
                    {
                        "text": "Synonymy, antonymy, and similarity",
                        "is_correct": false,
                        "explanation": "These are relations between senses, not affective dimensions."
                    },
                    {
                        "text": "Politeness, slang, and register",
                        "is_correct": false,
                        "explanation": "These are factors that may differentiate synonyms, not affective dimensions."
                    },
                    {
                        "text": "Semantic field, connotation, and sentiment",
                        "is_correct": false,
                        "explanation": "These are related concepts but not the specific affective dimensions identified by Osgood et al."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the SimLex-999 dataset used for?",
                "answers": [
                    {
                        "text": "Measuring the similarity of word meanings",
                        "is_correct": true,
                        "explanation": "The SimLex-999 dataset is used to measure how similar the meanings of words are, as indicated by the context provided."
                    },
                    {
                        "text": "Classifying text based on n-grams",
                        "is_correct": false,
                        "explanation": "The context discusses word meanings and similarity, not text classification based on n-grams."
                    },
                    {
                        "text": "Defining the exact synonyms of words",
                        "is_correct": false,
                        "explanation": "The context mentions that there are no perfect synonyms and discusses similarity rather than exact synonyms."
                    },
                    {
                        "text": "Identifying antonyms in a given language",
                        "is_correct": false,
                        "explanation": "The context discusses word similarity and relatedness, not specifically identifying antonyms."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    },
    "data/raw/Ngram-LMs.pdf": {
        "metadata": {
            "file_name": "Ngram-LMs.pdf",
            "file_type": "pdf",
            "content_length": 63263,
            "language": "en",
            "extraction_timestamp": "2025-11-29T21:52:53.844593+00:00",
            "timezone": "utc"
        },
        "content": "N-gram    Introduction to N-gram\nLanguage  Language Models\nModeling\n---\nPredicting words\n\nThe water of Walden Pond is beautifully ...\n\nblue            *refrigerator\ngreen           *that\nclear\n---\nLanguage Models\n\nA language model is a machine learning model\nthat predicts upcoming words.\n \u2022 More formally:\n     \u2022   An LM assigns a probability to each potential next word\n       \u2022 = gives a probability distribution over possible next words\n     \u2022   An LM assigns a probability to a whole sentence\n Two paradigms\n \u2022  N-gram language models (this lecture)\n \u2022  Large language models (LLMs, neural models, future lectures)\n---\nWhy word prediction?\n\nIt's a helpful part of language tasks\n\u2022 Grammar or spell checking\n Their are two midterms  Their There are two midterms\n Everything has improve  Everything has improve improved\n\n\u2022 Speech recognition\n I will be back soonish  I will be bassoon dish\n---\nWhy word prediction?\n\nIt's how large language models (LLMs) work!\nLLMs are trained to predict words\n\u2022 Left-to-right (autoregressive) LMs learn to predict next word\nLLMs generate text by predicting words\n\u2022 By predicting the next word over and over again\n---\n Language Modeling (LM) more formally\n\n Goal: compute the probability of a sentence or\n sequence of words W:\n P(W) = P(w\u2081,w\u2082,w\u2083,w\u2084,w\u2085\u2026w\u2099)\n Related task: probability of an upcoming word:\n P(w\u2085|w\u2081,w\u2082,w\u2083,w\u2084) or P(w\u2099|w\u2081,w\u2082\u2026w\u2099\u208b\u2081)\n An LM computes either of these:\n P(W)  or  P(w\u2099|w\u2081,w\u2082\u2026w\u2099\u208b\u2081)\n\n*Note: let's call these words for now, but for LLMs we instead use tokens\n---\n    P(blue|The water of Walden Pond is so beautifully)  (3.1)\n e way to estimate this probability is directly from relative frequency counts: take a\n    How to estimate these probabilities\n e way to estimate this probability is directly from relative frequency counts: take a\nry large corpus, count the number of times we see The water of Walden Pond\nry large corpus, count the number of times we see The water of Walden Pond\nso beautifully , and count the number of times this is followed by blue. This\nso beautifully , and count the number of times this is followed by blue. This\n uld be answering the question \u201cOut of the times we saw the history h, how many\n    Could we just count and divide?\n uld be answering the question \u201cOut of the times we saw the history h, how many\n es was it followed by the word w\u201d, as follows:\n es was it followed by the word w\u201d, as follows:\n    P(blue|The water of Walden Pond is so beautifully) ==\n    P(blue|The water of Walden Pond is so beautifully) =\n    C(The water of Walden Pond is so beautifully blue)\n    C(The water of Walden Pond is so beautifully blue)   (3.2\n    C(The water of Walden Pond is so beautifully)        (3.2)\n e had a C(The water of Walden Pond is so beautifully)\n e had a large enough corpus, we could compute these two counts and estimate\n    large enough corpus, we could compute these two counts and estimate\n probability from Eq. 3.2. But even the entire web isn\u2019t big enough to give us\n    No! Too many possible sentences!\n probability from Eq. 3.2. But even the entire web isn\u2019t big enough to give us\nod estimates for counts of entire sentences. This is because language is creative\n    We\u2019ll never see enough data for estimating these\nod estimates for counts of entire sentences. This is because language is creative;\nw sentences are invented all the time, and we can\u2019t expect to get accurate count\nw sentences are invented all the time, and we can\u2019t expect to get accurate counts\n such large objects as entire sentences. For this reason, we\u2019ll need more clever\n such large objects as entire sentences. For this reason, we\u2019ll need more clever\n---\nHow to compute P(W) or P(w\u2099|w\u2081, \u2026w\u2099\u208b\u2081)\nwith an N-gram Language Model\n\nFirst let's do the joint probability P(W):\n\nP(The, water, of, Walden, Pond, is, so, beautifully, blue)\n\nIntuition: let\u2019s rely on the Chain Rule of Probability\n---\nReminder: The Chain Rule\n\nRecall the definition of conditional probabilities\nP(B|A) = P(A,B)/P(A)  Rewriting: P(A,B) = P(A) P(B|A)\n\nMore variables:\nP(A,B,C,D) = P(A) P(B|A) P(C|A,B) P(D|A,B,C)\nThe Chain Rule in General\nP(x\u2081,x\u2082,x\u2083,\u2026,x\u2099) = P(x\u2081)P(x\u2082|x\u2081)P(x\u2083|x\u2081,x\u2082)\u2026P(x\u2099|x\u2081,\u2026,x\u2099\u208b\u2081)\n---\n       The Chain Rule applied to compute   3.1  \u2022  N-G RAM\n                                           joint\n       probability of words in sentence\nlying the chain rule to words, we get\n       P(w1:n ) = P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P(wn |w1:n\u22121 )\n                  n\n       = Y P(wk |w1:k\u22121 )\n                  k=1\n hain rule shows the link between computing the joint probability of a seq\n       P(\u201cThe water of Walden Pond\u201d) =\n omputing the conditional probability of a word given previous words.\n3.4    P(The) \u00d7 P(water|The) \u00d7 P(of|The water)\n     suggests that we could estimate the joint probability of an entire seque\n s     \u00d7 P(Walden|The water of) \u00d7    P(Pond|The water of Walden)\n       by multiplying together a number of conditional probabilities. But us\nn rule doesn\u2019t really seem to help us! We don\u2019t know any way to comp\n---\ntuition of the n-gram model is that instead of computing the proba\n   wn |wn\u22121 ). In other words, instead of computing the proba\n iven its entire history, we can approximate the history by just t\nm       Markov Assumption\n.  model, for example, approximates the probability of a word gi\ne water of Walden Pond is so beautifully)\n words  P(wn |w1:n\u22121 ) by using only the conditional probability of\n e bigram model, for example, approximates the probability of a\nd P(wSimplifying assumption:\n        |w  ). In other words, instead of computing the probabili\n ith the probability\n        n  n\u22121\n   revious words P(wn |w1:n\u22121 ) by using only the conditional probab\n ing word P(w |w  ). In other words, instead of computing the pr\ne|The water of Walden            A. A. Ma\u03c1Ron (1886).\n              n  n\u22121      Pond is so beautifully)    (\n   P(blue     P(blue|beautifully)  Andrei Markov\nte it      |The water of Walden Pond is so beautifully)\n        with the probability\n m model to predict the conditional probability of the next w\n roximate it with the probability\nhe          \u2248    P(blue|beautifully)                 (\n     following approximation:\n   igram model to     P(blue|beautifully)\n            P(w predict the conditional probability of the next word\n   ing the       n |w1:n\u22121 ) \u2248 P(wn |wn\u22121 )\nwe use following approximation:\n        a bigram model to predict the conditional probability of the\n                                           Wikimedia commons\n---\na complete word sequence by substituting Eq.  3.\n    Bigram Markov Assumption\n    n\n    P(w ) \u2248 Y P(w |w        )\n g the chain rule to words, we get\n        1:n    k      k \u22121\n    P(w ) =    k =1\n    1:n        P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P\n    instead of:    n\n                   = Y P(wk |w1:k\u22121 )\n                   k=1\nin rule shows the link between computing the join\n    We approximate each component in the product\n---\n next word in a sequence. We\u2019ll use      N here to\n    General N-gram model for each component\n eans bigrams and N = 3 means trigrams. Then w\n ord given its entire context as follows:\n    P(wn |w1:n\u22121 ) \u2248 P(wn |wn\u2212N +1:n\u22121 )\n assumption for the probability of an individual wo\nity of a complete word sequence by substituting Eq\n   n\n   Y\n---\n Bigram model\n     P ( w i | w\u2081w 2 \u2026 w i \u2212\u2081 ) \u2248 P ( w i | w i \u2212\u2081 )\n\nSome automatically generated sentences from Shakespeare bigram models\n Why dost stand forth thy canopy, forsooth; he                       is\n this palpable hit the King Henry. Live king.\n Follow.\n\n What means, sir. I confess she? then all sorts,                       he\n is  trim, captain.\n---\n  Even simpler Markov assumption: Unigram model\n\n              P ( w\u2081w 2 \u2026 w n ) \u2248 \u220f P ( w i )\n                                 i\n\nSome automatically generated sentences from Shakespeare unigram models\n  To  him swallowed confess   hear both . Which .\n  Of  save on trail for are   ay device and  rote\n  \u20ac\n  life have\n\n  Hill he late speaks   ; or  !  a more to leg less\n  first you   enter\n---\n More complex Markov assumption: Trigram model\n\n \ud835\udc43 \ud835\udc64! \ud835\udc64\" \ud835\udc64# \u2026 \ud835\udc64! $\"  \u2248 \ud835\udc43 \ud835\udc64! \ud835\udc64! $# \ud835\udc64! $\"\n\nSome automatically generated sentences from Shakespeare trigram models\n Fly, and will rid me these news of price.\n Therefore the sadness of parting,  as they say,\n \u2019tis done.\n\n This shall forbid it  should be branded, if    renown\n made it  empty\n---\n             of modeling the training corpus as we increase the value of N .\n             We can use the sampling method from the prior section to visualize both of\n4 different N-gram models\n             these facts! To give an intuition for the increasing power of higher-order n-grams,\n             Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-\n             gram models trained on Shakespeare\u2019s works.\n\n 1          \u2013To him swallowed confess hear both. Which. Of save on trail for are ay device and\n            rote life have\n gram       \u2013Hill he late speaks; or! a more to leg less first you enter\n 2          \u2013Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\n            king. Follow.\n gram       \u2013What means, sir. I confess she? then all sorts, he is trim, captain.\n 3          \u2013Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n            \u2019tis done.\n gram       \u2013This shall forbid it should be branded, if renown made it empty.\n 4          \u2013King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\n            great banquet serv\u2019d in;\n gram       \u2013It cannot be but so.\nFigure 3.4  Eight sentences randomly generated from four n-grams computed from Shakespeare\u2019s works. All\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\nfor capitalization to improve readability.\n---\nProblems with N-gram models\n1.         N-grams can't handle long-distance dependencies:\n\u201cThe soups that I made from that new cookbook I bought yesterday\n      were amazingly delicious.\"\n2.         N-grams don't do well at modeling new sequences (even\n           if they have similar meanings to sequences they've seen)\nThe solution: Large language models\n      \u2022     can handle much longer contexts\n      \u2022     because they use neural embedding spaces, can model\n            meaning better\n---\nWhy study N-gram models?\n\nThey are the \"fruit fly of NLP\", a simple model that\nintroduces important topics for large language models\n \u2022     training and test sets\n \u2022     the perplexity metric\n \u2022     sampling to generate sentences\n \u2022     ideas like interpolation and backoff\n N-grams are efficient and fast and useful in situations\n where LLMs are too heavyweight\n---\nN-gram    Introduction to N-gram\nLanguage  Language Models\nModeling\n---\nN-gram    Estimating N-gram\nLanguage  Probabilities\nModeling\n---\n etween 0 and 1.\n he bigrams that share the same first word  wn\u22121 :\nto compute a particular bigram probability of a word  w\n    Estimating bigram probabilities                   n\n    , we\u2019ll compute the   C(w  w )\n                          count of the bigram C(w  w ) and\nn\u22121  P(wn |wn\u22121 ) = P     n\u22121  n            n\u22121    n\nl the bigrams that share the same first word  w    :\n     The Maximum          C(wn\u22121 w)                n\u22121\n                   Likelihood Estimate\n                          w\ny this equation, since the sum of all bigram counts that star\n                          C(wn\u22121 wn )\n must be  P(w |w   ) = P\n          equal to the unigram count for that word w  (the\n nt to be   n   n\u22121        w C(wn\u22121 w)             n\u22121\nify this    convinced of this):\n          equation, since the sum of all bigram counts that st\n   must be equal to the   C(w  w )\n                     unigram count for that word w    (th\n 1          P(wn |wn\u22121 ) =     n\u22121 n                  n\u22121\n ent to be convinced of this):\n                            C(wn\u22121 )\n ugh an example using a mini-corpus of three sentences.\n                               C(wn\u22121 wn )\n---\n  An example\n\n   ~~ I am Sam ~~      P ( w i | w i\u2212\u2081 ) = c ( w i\u2212\u2081, w i )\n   ~~ Sam I am ~~                          c ( w i\u2212\u2081 )\n   ~~ I do not like green eggs and ham ~~  \n\nP(I| <s>=    \u20ac\n  2          P(Sam <s>)=                      P(am| I) =2\n             =.67    1= .33                               =.67\n             2                                P(do| I) = 1\nP(</s> | Sam) = = 0.5  P(Sam | am) =\u00b9 = .5                = .33\n                       2\n---\nMore examples:\nBerkeley Restaurant Project sentences\n\n can you tell me about any good cantonese restaurants close by\n tell me about chez panisse\n i\u2019m looking for a good place to eat breakfast\n when is caffe venezia open during the day\n---\n        i\u2019m looking for a good place to eat breakfast\n      Raw bigram counts\n        when is caffe venezia open during the day\n          Figure 3.1 shows the bigram counts from part of a bigram grammar from text-\nnormalized Berkeley Restaurant Project sentences.       Note that the majority of the\n      Out of 9222 sentences\nvalues are zero. In fact, we have chosen the sample words to cohere with each other;\na matrix selected from a random set of eight words would be even more sparse.\n\n             i      want     to      eat         chinese     food     lunch     spend\n i           5      827      0       9           0           0        0         2\n want        2      0        608     1           6           6        5         1\n to          2      0        4       686         2           0        6         211\n eat         0      0        2       0           16          2        42        0\n chinese     1      0        0       0           0           82       1         0\n food        15     0        15      0           1           4        0         0\n lunch       2      0        0       0           0           1        0         0\n spend       1      0        1       0           0           0        0         0\nFigure 3.1  Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-\n---\n means that want followed i 827 times in the corpus.\n         Raw bigram probabilities\n         Figure 3.2 shows the bigram probabilities after normalization (dividing each c\n in Fig. 3.1 by the appropriate unigram for its row, taken from the following set\n unigram counts):\n         Normalize by unigrams:\n                         i         want        to           eat  chinese        food  lunch    spend\nC          3       \u2022     2533      927         2417         746  158            1093  341      278\n   HAPTER               N-GRAM L ANGUAGE M ODELS\n         Result:\n Here are a few other useful probabilities:\n                              i           want        to          eat      chinese     food     lunch     spend\n   P(i|<s>) = 0.25                                   P(english|want) = 0.0011\n              i               0.002       0.33        0           0.0036   0           0        0         0.00079\n   P(         want            0.0022      0           0.66        0.0011   0.0065      0.0065   0.0054    0.0011\n         food|english) = 0.5                         P(</s>|food) = 0.68\n              to              0.00083     0           0.0017      0.28     0.00083     0        0.0025    0.087\n         Now we can compute the probability of sentences like I want English food\n I            eat             0           0           0.0027      0        0.021       0.0027   0.056     0\n       want Chinese food by simply multiplying the appropriate bigram probabilities t\n              chinese         0.0063      0           0           0        0           0.52     0.0063    0\n              food            0.014       0           0.014       0        0.00092     0.0037   0         0\n gether, as follows:\n              lunch           0.0059      0           0           0        0           0.0029   0         0\n                        P( ~~ i want english food ~~  )\n              spend           0.0036      0           0.0036      0        0           0        0         0\n---\nBigram estimates of sentence probabilities\n\nP( ~~ I want english food ~~  ) =\nP(I|<s>)\n\u00d7 P(want|I)\n\u00d7 P(english|want)\n\u00d7 P(food|english)\n\u00d7 P(</s>|food)\n= .000031\n---\nWhat kinds of knowledge do N-grams represent?\n\nP(to|want) = .66                  Knowledge of grammar\nP(want | spend) = 0               (\"want\" is followed by infinitive \"to\")\nP(of | to) = 0                    (\"of\" is not a verb)\n\nP (dinner|lunch or) = .83         Knowledge of meaning\nP(dinner|for) ~ P(lunch|for)     The words \"dinner\" or \"lunch\" are\n                                 semantically related.\n\nP(chinese|want) > P(english|want)     Knowledge about the world\n                                      Chinese food is very popular\n---\n.1.3  Dealing with scale in large n-gram models\nn     Dealing with scale in large n-grams\n    practice, language models can be very large, leading to practical issues.\nog probabilities  Language model probabilities are always stored and com\nn     LM probabilities are stored and computed in\n    log format, i.e., as log probabilities. This is because probabilities are (b\n      log format, i.e. log probabilities\nnition) less than or equal to 1, and so the more probabilities we multiply to\nhe smaller the product becomes. Multiplying enough n-grams together would\n      This avoids underflow from multiplying many\nn numerical underflow. Adding in log space is equivalent to multiplying in\npace, small numbers\n      so we combine log probabilities by adding them. By adding log proba\nnstead of multiplying probabilities, we get results that are not as small. We\n      log( p\u2081 \u00d7 p\u2082 \u00d7 p\u2083 \u00d7 p\u2084 ) = log p\u2081 + log p\u2082 + log p\u2083 + log p\u2084\nomputation and storage in log space, and just convert back into probabilitie\need to report probabilities at the end by taking the exp of the logprob:\n      If we need probabilities we can do one exp at the end\n      p1 \u21e5 p2 \u21e5 p3 \u21e5 p4 = exp(log p1 + log p2 + log p3 + log p4 )\n---\nLarger ngrams\n\n4-grams, 5-grams\nLarge datasets of large n-grams have been released\n \u2022  N-grams from Corpus of Contemporary American English (COCA)\n    1 billion words (Davies 2020)\n \u2022  Google Web 5-grams (Franz and Brants 2006) 1 trillion words)\n \u2022  Efficiency: quantize probabilities to 4-8 bits instead of 8-byte float\n Newest model: infini-grams (\u221e-grams) (Liu et al 2024)\n \u2022  No precomputing! Instead, store 5 trillion words of web text in\n    suffix arrays. Can compute n-gram probabilities with any n!\n---\nN-gram LM Toolkits\n\nSRILM\n\u25e6 http://www.speech.sri.com/projects/srilm/\nKenLM\n\u25e6 https://kheafield.com/code/kenlm/\n---\nN-gram    Estimating N-gram\nLanguage  Probabilities\nModeling\n---\n        Evaluation and Perplexity\n\nLanguage\nModeling\n---\nHow to evaluate N-gram models\n\n\"Extrinsic (in-vivo) Evaluation\"\nTo compare models A and B\n1.  Put each model in a real task\n \u2022  Machine Translation, speech recognition, etc.\n2.  Run the task, get a score for A and for B\n \u2022  How many words translated correctly\n \u2022  How many words transcribed correctly\n3.  Compare accuracy for A and B\n---\nIntrinsic (in-vitro) evaluation\n\nExtrinsic evaluation not always possible\n\u2022  Expensive, time-consuming\n\u2022  Doesn't always generalize to other applications\nIntrinsic evaluation: perplexity\n\u2022  Directly measures language model performance at\n   predicting words.\n\u2022  Doesn't necessarily correspond with real application\n   performance\n\u2022  But gives us a single general metric for language models\n\u2022  Useful for large language models (LLMs) as well as n-grams\n---\nTraining sets and test sets\n\nWe train parameters of our model on a training set.\nWe test the model\u2019s performance on data we\nhaven\u2019t seen.\n \u25e6   A test set is an unseen dataset; different from training set.\n   \u25e6 Intuition: we want to measure generalization to unseen data\n \u25e6   An evaluation metric (like perplexity) tells us how well\n     our model does on the test set.\n---\nChoosing training and test sets\n\n\u2022 If we're building an LM for a specific task\n  \u2022  The test set should reflect the task language we\n     want to use the model for\n\u2022 If we're building a general-purpose model\n  \u2022  We'll need lots of different kinds of training\n     data\n  \u2022  We don't want the training set or the test set to\n     be just from one domain or author or language.\n---\nTraining on the test set\n\nWe can\u2019t allow test sentences into the training set\n \u2022  Or else the LM will assign that sentence an artificially\n    high probability when we see it in the test set\n \u2022  And hence assign the whole test set a falsely high\n    probability.\n \u2022  Making the LM look better than it really is\nThis is called \u201cTraining on the test set\u201d\nBad science!\n\n                                                            38\n---\nDev sets\n\n\u2022 If we test on the test set many times we might\nimplicitly tune to its characteristics\n \u2022 Noticing which changes make the model better.\n\u2022 So we run on the test set only once, or a few times\n\u2022 That means we need a third dataset:\n \u2022 A development test set or, devset.\n \u2022 We test our LM on the devset until the very end\n \u2022 And then test our LM on the test set once\n---\n Intuition of perplexity as evaluation metric:\n How good is our language model?\nIntuition: A good LM prefers \"real\" sentences\n\u2022     Assign higher probability to \u201creal\u201d or \u201cfrequently\n      observed\u201d sentences\n\u2022     Assigns lower probability to \u201cword salad\u201d or\n      \u201crarely observed\u201d sentences?\n---\n Intuition of perplexity 2:\n Predicting upcoming words\n                    The Shannon Game: How well can we        time      0.9\n                    predict the next word?                   dream     0.03\n                    \u2022  Once upon a ____                      midnight 0.02\n                    \u2022  That is a picture of a ____           \u2026\n                    \u2022  For breakfast I ate my usual ____\n                                                             and       1e-100\nClaude   233355     Unigrams are terrible at this game (Why?)\n         Shannon\n\nA good LM is one that assigns a higher probability\nto the next word that actually occurs\n                                                         Picture credit: Historiska bildsamlingen\n                                                         https://creativecommons.org/licenses/by/2.0/\n---\nIntuition of perplexity 3: The best language model\nis one that best predicts the entire unseen test set\n \u2022  We said: a good LM is one that assigns a higher\n    probability to the next word that actually occurs.\n \u2022  Let's generalize to all the words!\n     \u2022     The best LM assigns high probability to the entire test\n           set.\n \u2022  When comparing two LMs, A and B\n     \u2022     We compute PA(test set) and PB(test set)\n     \u2022     The better LM will give a higher probability to (=be less\n           surprised by) the test set than the other LM.\n---\nIntuition of perplexity 4: Use perplexity instead of\nraw probability\n\u2022  Probability depends on size of test set\n    \u2022     Probability gets smaller the longer the text\n    \u2022     Better: a metric that is per-word, normalized by length\n\u2022  Perplexity is the inverse probability of the test set,\n   normalized by the number of words\n\n          PP (W )  = P (w w ...w )\u2212 1\n                     1 2         N  N\n\n                   = N P (w 1\n                       1w2 ...w N )\n---\nIntuition of perplexity 5: the inverse\n\nPerplexity is the inverse probability of the test set,\nnormalized by the number of words\n\nPP(W ) = P(w w ... w )\u2212 1\n         1 2         N  N\n\n       = N P(w1w1 ... w )\n              2         N\n(The inverse comes from the original definition of perplexity\nfrom cross-entropy rate in information theory)\nProbability range is [0,1], perplexity range is [1,\u221e]\nMinimizing perplexity is the same as maximizing probability\n---\nIntuition of perplexity 6: N-grams\n\nPP (W )  = P ( w w ...w )\u2212 1\n           1 2          N  N\n\n         = N P ( w1w1 ...w )\n                   2       N\n\n                   N        1\nChain rule:  PP(W)  N II\n             =      i=1P(wi| 1 . . . Wi\u22121)\n\n                    N       1\nBigrams:     PP(W)  N \u2161IP(i|i\u22121\n                    i=1\n---\n   Intuition of perplexity 7:\n   Weighted average branching factor\n\nPerplexity is also the weighted average branching factor of a language.\nBranching factor: number of possible next words that can follow any word\nExample: Deterministic language L = {red,blue, green}\n   Branching factor = 3 (any word can be followed by red, blue, green)\nNow assume LM A where each word follows any other word with equal probability \u2153\nGiven a test set T = \"red red red red blue\"\nPerplexityA(T) = PA(red red red red blue)-1/5 =  ((\u2153)\u2075)-1/5  = (\u2153)\u207b\u00b9  =3\nBut now suppose red was very likely in training set, such that for LM B:\n\u25e6  P(red) = .8 p(green) = .1 p(blue) = .1\nWe would expect the probability to be higher, and hence the perplexity to be smaller:\nPerplexityB(T) = PB(red red red red blue)-1/5\n   = (.8 * .8 * .8 * .8 * .1) -1/5  =.04096 -1/5     = .527\u207b\u00b9  = 1.89\n---\n Holding test set constant:\n Lower perplexity = better language model\n\nTraining 38 million words, test 1.5 million words, WSJ\n\nN-gram     Unigram  Bigram  Trigram\nOrder\nPerplexity 962      170     109\n---\n        Evaluation and Perplexity\n\nLanguage\nModeling\n---\n        Sampling and Generalization\n\nLanguage\nModeling\n---\nThe Shannon (1948) Visualization Method\nSample words from an LM\n\n Unigram:                                          233355\n                                             Claude Shannon\n\n REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME\n CAN DIFFERENT NATURAL HERE HE THE A IN CAME THE TO\n OF TO EXPERT GRAY COME TO FURNISHES THE LINE\n MESSAGE HAD BE THESE.\n\n Bigram:\nTHE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER\nTHAT THE CHARACTER OF THIS POINT IS THEREFORE\nANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHO\nEVER TOLD THE PROBLEM FOR AN  UNEXPECTED.\n---\nHow Shannon sampled those words in 1948\n\n\"Open a book at random and select a letter at random on the page.\nThis letter is recorded. The book is then opened to another page\nand one reads until this letter is encountered. The succeeding\nletter is then recorded. Turning to another page this second letter\nis searched for and the succeeding letter recorded, etc.\"\n---\nSampling a word from a distribution  8  2\n                                     %8 36 99 0\n                                     5230 12984\n                                     291082 64 402223\n                                     11 87 69 5341\n                                     2228899 88 8 g2\n                                        A2 L9G8\n\npolyphonic\n\nthe  of  a  to in    \u2026 however  p=.0000018\n                     (p=.0003)\n\n0.06  0.03  0.02 0.02 0.02\n                          \u2026     \u2026\n\n0     .06  .09 .11 .13 .15      .66       .99 1\n---\n  Visualizing Bigrams the Shannon Way\n\n Choose a random bigram (<s>, w)       <s> I\n  according to its probability p(w|<s>)  I        want\n Now choose a random bigram            (w, x)     want to\naccording to its probability p(x|w)                   to eat\n And so on until we choose </s>                          eat Chinese\n Then string the words together                             Chinese food\n                                                                           food </s>\n                                                 I want to eat Chinese food\n---\nNote: there are other sampling methods\n\nUsed for neural language models\nMany of them avoid generating words from the very\nunlikely tail of the distribution\nWe'll discuss when we get to neural LM decoding:\n \u25e6  Temperature sampling\n \u25e6  Top-k sampling\n \u25e6  Top-p sampling\n---\n             We can use the sampling method from the prior section to visualize both of\n            Approximating Shakespeare\n             these facts! To give an intuition for the increasing power of higher-order n-grams,\n             Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-\n             gram models trained on Shakespeare\u2019s works.\n\n1           \u2013To him swallowed confess hear both. Which. Of save on trail for are ay device and\n            rote life have\ngram        \u2013Hill he late speaks; or! a more to leg less first you enter\n2           \u2013Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\n            king. Follow.\ngram        \u2013What means, sir. I confess she? then all sorts, he is trim, captain.\n3           \u2013Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n            \u2019tis done.\ngram        \u2013This shall forbid it should be branded, if renown made it empty.\n4           \u2013King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\n            great banquet serv\u2019d in;\ngram        \u2013It cannot be but so.\nFigure 3.4  Eight sentences randomly generated from four n-grams computed from Shakespeare\u2019s works. All\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\n---\nShakespeare as corpus\n\nN=884,647 tokens, V=29,066\nShakespeare produced 300,000 bigram types out of\nV\u00b2= 844 million possible bigrams.\n \u25e6  So 99.96% of the possible bigrams were never seen (have\n    zero entries in the table)\n \u25e6  That sparsity is even worse for 4-grams, explaining why\n    our sampling generated actual Shakespeare.\n---\nThe Wall Street Journal is not Shakespeare\n            3.5                        \u2022  G ENERALIZATION AND Z EROS               13\n\n1           Months the my and issue of year foreign new exchange\u2019s september\ngram        were recession exchange new endorsed a acquire to six executives\n2           Last December through the way to preserve the Hudson corporation N.\n            B. E. C. Taylor would seem to complete the major central planners one\ngram        point five percent of U. S. E. has already old M. X. corporation of living\n            on information such as more frequently fishing to keep her\n3           They also point to ninety nine point six billion dollars from two hundred\n            four oh six three percent of the rates of interest stores as Mexico and\ngram        Brazil on market conditions\nFigure 3.5  Three sentences randomly generated from three n-gram models computed from\n40 million words of the Wall Street Journal, lower-casing all characters and treating punctua-\n---\nCan you guess the author? These 3-gram sentences\nare sampled from an LM trained on who?\n 1)  They also   point to ninety nine point\n six  billion dollars from two hundred four\n oh  six  three  percent of the rates of\n interest   stores  as Mexico and gram Brazil\n on  market conditions\n 2)  This shall  forbid it should be branded,\n if  renown made    it empty.\n 3)  \u201cYou are uniformly   charming!\u201d cried he,\n with  a  smile  of associating and  now and\n then  I  bowed  and they perceived  a chaise\n and  four  to wish for.\n                                        58\n---\nChoosing training data\n\nIf task-specific, use a training corpus that has a similar\ngenre to your task.\n\u2022   If legal or medical, need lots of special-purpose documents\nMake sure to cover different kinds of dialects and\nspeaker/authors.\n\u2022   Example: African-American Vernacular English (AAVE)\n  \u2022  One of many varieties that can be used by African Americans and others\n  \u2022  Can include the auxiliary verb finna that marks immediate future tense:\n  \u2022  \"My phone finna die\"\n---\nThe perils of overfitting\n\nN-grams only work well for word prediction if the\ntest corpus looks like the training corpus\n \u2022  But even when we try to pick a good training\n    corpus, the test set will surprise us!\n \u2022  We need to train robust models that generalize!\nOne kind of generalization: Zeros\n   \u2022 Things that don\u2019t ever occur in the training set\n    \u2022 But occur in the test set\n---\nZeros\nTraining set:           \u2022 Test set\n\u2026 ate lunch               \u2026 ate lunch\n\u2026 ate dinner              \u2026 ate breakfast\n\u2026 ate a\n\u2026 ate the\nP(\u201cbreakfast\u201d | ate) = 0\n---\nZero probability bigrams\n\nBigrams with zero probability\n \u25e6  Will hurt our performance for texts where those words\n    appear!\n \u25e6  And mean that we will assign 0 probability to the test set!\nAnd hence we cannot compute perplexity (can\u2019t\ndivide by 0)!\n---\n        Sampling and Generalization\n\nLanguage\nModeling\n---\nN-gram    Smoothing, Interpolation,\nLanguage  and Backoff\nModeling\n---\nThe problem of sparsity\n\nMaximum likelihood estimation has a problem\n\u2022 Sparsity! Our finite training corpus won't have\nsome perfectly fine sequences\n\u2022  Perhaps it has \"ruby\" and \"slippers\"\n\u2022  But happens not to have \"ruby slippers\"\n---\nThis sparsity can take many forms\n\nVerbs have direct objects; those can be sparse too!\n\nCount(w | denied the)\n\nCounts:              allegations\n 3 allegations       reports     outcome\n 2 reports                      requestattack    \u2026\n 1 claims            claims      man\n 1 request\n 0 attack\n 0 outcome\n 7 total\n---\nThe intuition of smoothing\n                               (Example modified from Dan Klein!)\nWhen we have sparse statistics:\nCount(w | denied the)          allegations\n3 allegations\n2 reports                      reports     outcome\n1 claims                                   attack  \u2026\n                                           request\n1 request                                  claims    man\n7 total\nSteal probability mass to generalize better\nCount(w | denied the)\n 2.5 allegations\n 1.5 reports                   allegations              outcome\n 0.5 claims                                          attack\n 0.5 request                   reports               man   \u2026\n 2 other                                   claims request\n 7 total\n---\n  lunch     3  1                 1               1                    1             2         1\n  spend     2  1            2          1              1                    1             1\n  spend     2  1            2          1              1                    1             1\n  Add-one estimation\n  spend        2  1              2               1                    1             1         1\nFigure 3.6  Add-one smoothed bigram counts for eight of the words (out of V\nFigure 3.6  Add-one smoothed bigram counts for eight of the words (out of V\n  Figure 3.6  Add-one smoothed bigram counts for eight of the words (out\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero count\n  the Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero co\n \u2022 Also called Laplace smoothing\n  Figure 3.7 shows the add-one smoothed probabilities for the bigrams\n  Figure 3.7 shows the add-one smoothed probabilities for the bigrams\n \u2022 Pretend we saw each word one more time than we did\nRecall that normal bigram probabilities are computed by normalizing e\nRecall that normal bigram probabilities are computed by normalizing e\n  Figure 3.7 shows the add-one smoothed probabilities for the bigra\ncounts by the unigram count:\ncounts by the unigram count:\n  Recall that normal bigram probabilities are computed by normalizin\n \u2022 Just add one to all the counts!\n  counts by the unigram count:                        C (w                 w )\n                                                      C (w                 w )\n                                                                           n\n                                                            n\u22121            n\n                     P(w |w            ) =                  n\u22121\n                     P(w |w             ) =\n                               n\n                               n       n\u22121\n                                       n\u22121            C (w                 )\n                                                      C (w                 )\n  MLE estimate:                                             C (w              w )\n                                                            n\u22121\n                     PMLE (wn |wn\u22121 ) =                               n\u22121 n\u22121 n\nFor add-one smoothed bigram counts,                                   C (w    )\nFor add-one smoothed bigram                 we need to augment the unigram co\n                               counts, we need to augment the unigram c\n                                                                            n\u22121\nnumber of total word types in the vocabulary V :\nnumber of total word types in the vocabulary V :\n  For add-one smoothed bigram counts, we need to augment the unigra\n  Add-1 estimate:\n  number of total word types in the vocabulary V :\n                                      C (w            w ) + 1                 C (w       w ) + 1\n                                       C (w           w ) + 1                 C (w       w ) + 1\n                                                      n\n                                                      n                                  n\n                                            n                                            n\n                                             n\n                                             \u2212\n                                                 \u2212\n                                                 1\n                                                      1                          n\n                                                                                    n\n                                                                                    \u2212\n                                                                                     \u2212\n                                                                                         1\n                                  P                                                      1\n         P        (w |w  ) =      P                                         =\n         P         (w |w  ) =                                               =\n            Laplace\n            Laplace  n\n                     n  n\n                          n\n                          \u2212\n                          \u2212\n                           1\n                           1           (C (w               w) + 1)             C (w       ) + V\n                                       (C (w               w) + 1)             C (w       ) + V\n                                                 n\n                                                      n\n                                                      \u2212\n                                                           \u2212\n                                                           1\n                                                            1                        n\n                                                                                     n\n                                                                                         \u2212\n                                                                                         \u2212\n                                                                                          1\n                                       w C (w               w ) + 1            C (w 1      w ) +\n                                       w         n\u22121             n                        n\u22121 n\n---\nMaximum Likelihood Estimates\n\nThe maximum likelihood estimate\n \u25e6     of some parameter of a model M from a training set T\n \u25e6     maximizes the likelihood of the training set T given the model M\nSuppose the word \u201cbagel\u201d occurs 400 times in a corpus of a million words\nWhat is the probability that a random word from some other text will be\n\u201cbagel\u201d?\nMLE estimate is 400/1,000,000 = .0004\nThis may be a bad estimate for some other corpus\n \u25e6     But it is the estimate that makes it most likely that \u201cbagel\u201d will occur 400 times\n       in a million word corpus.\n---\n                                                   c\u2217\n                                           d =     i\n ts,      Berkeley Restaurant Corpus: Laplace\n     we need to augment the unigram count by the\n          smoothed                           i     ci\n abulary             V : bigram counts\n   Now that we have the intuition for the unigram case, let\u2019s smooth our Berkeley\n   Restaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the\n  C(w          w ) + 1              C(w            w ) + 1\n   bigrams in Fig. 3.1.\n     (Cn\u22121     n                 =         n\u22121       n                        (3.24)\n  w         (wn\u22121iw) + 1)                 C(wn\u22121 ) + V\n                         want       to      eat           chinese     food     lunch     spend\n     i          6        828        1       10            1           1        1         3\n en in the previous section will need to be aug-\n     want       3        1          609     2             7           7        6         2\nthe smoothed bigram probabilities in Fig.                                   3.7.\n     to         3        1          5       687           3           1        7         212\n     eat        1        1          3       1             17          3        43        1\n uct the count matrix so we can see how much a\nhe chinese      2        1          1       1             1           83       2         1\n     original counts. These adjusted counts can be\n     food       16       1          16      1             2           5        1         1\n  shows the reconstructed counts.\n     lunch      3        1          1       1             1           2        1         1\n     spend      2        1          2       1             1           1        1         1\n  [C(w         w ) + 1] \u00d7 C(w                )\n   Figure 3.6  Add-one smoothed bigram counts for eight of the words (out of V = 1446) in\n            n\u22121 n                         n\u22121\n---\n                               P(w |w                   ) =\n                               P(w |w                    ) =                                                                 (3.2\n                                         n                                                                                   (3.2\n                                         n          n\u22121\n                                                    n\u22121              C (w    )\n                                                                     C (w    )\n                                                                           n\u22121\n         Laplace-                                                           n\u22121\n add-one                   smoothed bigrams\n             smoothed bigram counts, we need to augment the unigram count by th\nr add-one smoothed bigram counts, we need to augment the unigram count by t\n  ber of total word types in the vocabulary V :\nmber of total word types in the vocabulary V :\n                                                    C (w        w ) + 1               C (w               w ) + 1\n                                                    C (w            w ) + 1                C (w          w ) + 1\n                                                                     n                                   n\n                                                          n\u22121        n                              n\u22121  n\n                                         P                n\u22121                                        n\u22121\n    P                (w |w     ) =            P                                      =\n    P                (w |w            ) =                                             =                                      (3.2\n         Laplace                                                                                                             (3.2\n          Laplace       n\n                        n   n\u22121\n                            n\u22121                       (C (w               w) + 1)          C (w          ) + V\n                                                      (C (w               w) + 1)          C (w          ) + V\n                                                             n\u22121                                     n\u22121\n    16       C          3  \u2022   N-             L               n\u22121                                     n\u22121\n                                                    w        M\n                HAPTER             GRAM             w\n                                                   ANGUAGE          ODELS\n s, each of the unigram counts given in the previous section will need to be aug\nus, each of the unigram counts given in the previous section will need to be au\nnted by V =     i             want            to             eat           chinese         food          lunch       spend\n nted by V = 1446. The result is the smoothed bigram probabilities in Fig. 3.7.\n    i           1446. The result is the smoothed bigram probabilities in Fig. 3.7.\n                0.0015        0.21            0.00025        0.0025        0.00025         0.00025       0.00025     0.00075\n    want        0.0013        0.00042         0.26           0.00084       0.0029          0.0029        0.0025      0.00084\n It is often convenient to reconstruct the count matrix so we can see how much\n It is often convenient to reconstruct the count matrix so we can see how much\n    to          0.00078       0.00026         0.0013         0.18          0.00078         0.00026       0.0018      0.055\noothing algorithm has changed the original counts. These adjusted counts can b\noothing algorithm has changed the original counts. These adjusted counts can\n    eat         0.00046       0.00046         0.0014         0.00046       0.0078          0.0014        0.02        0.00046\n puted by Eq. 3.25. Figure 3.8 shows the reconstructed counts.\n  puted by Eq. 3.25. Figure 3.8 shows the reconstructed counts.\n    chinese     0.0012        0.00062         0.00062        0.00062       0.00062         0.052         0.0012      0.00062\n    food        0.0063        0.00039         0.0063         0.00039       0.00079         0.002         0.00039     0.00039\n    lunch       0.0017        0.00056         0.00056        0.00056       0.00056         0.0011        0.00056     0.00056\n                                                   [C (w        w ) + 1] \u00d7 C (w                      )\n                                                    [C (w           w ) + 1] \u00d7 C (w                   )\n                                                                     n\n                                                          n          n\n                                                           n\n                                                             \u2212\n                                                             \u2212\n                                                              1\n                                                              1                            n\n                                                                                           n\n                                                                                            \u2212\n                                                                                            \u2212\n                                                                                                    1\n                     \u2217                                                                              1\n                     c \u2217\n                     c (w      w ) =\n    spend                 (w       w ) =\n                0.0012        0.00058         0.0012         0.00058       0.00058         0.00058       0.00058     0.00058 (3.2\n                                      n                                                                                      (3.2\n                           n          n\n                              n\n                              \u2212\n                              \u2212\n                               1\n                               1                              C (w           ) + V\n    Figure 3.7       Add-one smoothed bigram                  C (w           ) + V\n                                                    probabilities for eight of the words (out of V = 1446) in the BeRP\n                                                                          n\n                                                                          n\n                                                                           \u2212\n                                                                           \u2212\n                                                                            1\n    corpus of 9332 sentences. Previously-zero probabilities are in           1\n Note that add-one smoothing has made a                                    gray.\n Note that add-one smoothing has made                                     very big change to the counts. Com\n                                                                          a very big change to the counts. Com\n---\noothed probability. This adjusted count is easier to compare direct\n056   0.00056       0.00056  0.00056  0.0011  0.00056  0\nLE counts. That is, the Laplace probability can equally be expresse\nE counts. That is, the Laplace probability can equally be expresse\n058   Visualization Technique: Reconstituted counts\nted   0.0012        0.00058  0.00058  0.00058  0.00058  0\n      count divided by the (non-smoothed) denominator from Eq. 3.25:\ned count divided by the (non-smoothed) denominator from Eq. 3.25:\ned bigram probabilities for eight of the words (out of V = 1446) in th\n                                      \u2217\n                             C(w  w ) + 1  C\u2217 (w       w )\nted by Eq.  3.26.            C(w  w ) + 1  C (w        w )\n               Previously-zero probabilities are in gray.\n                                  n\u22121 n                n\u22121 n\n      P       (w |w   ) =    n\u22121      n    =           n\u22121 n\n      P       (w |w   ) =                  =\n         Laplace      n  n\u22121\n      Laplace     n      n\u22121  C(w     ) + V   C(w          )\n                              C(w n\u22121) + V    C(w n\u22121)\n                                  n\u22121                  n\u22121\ng terms, we can solve for C \u2217 (wn\u22121 wn ) :\n\n      C \u2217 (wn\u22121 wn ) = [C (wn\u22121 wn ) + 1] \u00d7 C (wn\u22121 )\n                              C (wn\u22121 ) + V\nshows the reconstructed counts, computed by Eq. 3.27.\n i       want         to      eat        chinese  food      lunch\n---\nant          0.0013        0.00042       0.26         0.00084        0.0029           0.0029           0.0025      0.00084\n 0.00078                   0.00026       0.0013       0.18           0.00078          0.00026          0.0018      0.055\n             Reconstituted counts C*\n ting terms, we can solve for C \u2217 (w                                      w ) :\n             0.00046       0.00046       0.0014       0.00046        0.0078     n     0.0014           0.02        0.00046\n inese       0.0012        0.00062       0.00062      0.00062        n\u22121\n                                                                     0.00062          0.052            0.0012      0.00062\n od          0.0063        0.00039       0.0063     [ 0.00039        0.00079          0.002            0.00039     0.00039\n nch         0.0017        0.00056       0.00056    C (w             w ) + 1] \u00d7 C (w                             )\n                                                      0.00056        0.00056          0.0011           0.00056     0.00056\n end         0.0012C \u2217 (w               w ) =                 n\u22121        n                             n\u22121\n                           0.00058       0.0012       0.00058        0.00058          0.00058          0.00058     0.00058\ngure 3.7     Add-one         n\u22121         n                       C (wn\u22121 ) + V\n                         smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP\n pus of 9332 sentences. Previously-zero probabilities are in gray.\n.8 shows the reconstructed counts, computed by Eq. 3.27.\n                                 i         want      to          eat          chinese          food     lunch       spend\n                  i              3.8       527       0.64        6.4          0.64             0.64     0.64        1.9\n        i                want             to                  eat               chinese                  food           lunch\n                  want           1.2       0.39      238         0.78         2.7              2.7      2.3         0.78\n             C*:  to             1.9       0.63      3.1         430          1.9              0.63     4.4         133\n                  eat            0.34     0.64                6.4               0.64                     0.64           0.64\n                                           0.34      1           0.34         5.8              1        15          0.34\n                  chinese        0.2       0.098     0.098       0.098        0.098            8.2      0.2         0.098\n                         0.39             238                 0.78              2.7                      2.7            2.3\n                  food           6.9       0.43      6.9         0.43         0.86             2.2      0.43        0.43\n                  lunch          0.57      0.19      0.19        0.19         0.19             0.38     0.19        0.19\n                         0.63             3.1                 430               1.9                      0.63           4.4\n        0.34      spend          0.32      0.16      0.32        0.16         0.16             0.16     0.16        0.16\n                         0.34             1                   0.34              5.8                      1              15\n                 Figure 3.8      Add-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus\n---\n16       C          3     \u2022      N-                  L                   M\n            HAPTER normalized Berkeley Restaurant Project sentences.                                        Note that the majority of the\n                                     GRAM               ANGUAGE                ODELS\nCompare with raw bigram counts\n                    values are zero. In fact, we have chosen the sample words to cohere with each other;\n              i     a matrix selected from a random set of eight words would be even more sparse.\n                                 want                to               eat                chinese           food                 lunch      spend\n i            0.0015             0.21       i        0.00025          0.0025             0.00025           0.00025              0.00025      0.00075\n want         0.0013             0.00042                   want          to         eat       chinese              food          lunch      spend\n                          i                     5    0.26             0.00084            0.0029            0.0029               0.0025       0.00084\n to           0.00078            0.00026                   827           0          9         0                    0             0          2\n                          want                  2          0.0013     0.18               0.00078           0.00026              0.0018     0.055\n eat          0.00046            0.00046                     0           608        1         6                    6             5          1\n                          to                    2          0.0014     0.00046            0.0078            0.0014               0.02         0.00046\n chinese      0.0012             0.00062                     0           4          686       2                    0             6          211\n                          eat                   0    0.00062          0.00062            0.00062           0.052                0.0012       0.00062\n food         0.0063             0.00039                     0           2          0         16                   2             42         0\noriginal                  chinese               1          0.0063     0.00039            0.00079           0.002                0.00039      0.00039\n lunch        0.0017             0.00056                     0           0          0         0                    82            1          0\n                          food              15 0.00056                0.00056            0.00056           0.0011               0.00056      0.00056\n spend        0.0012             0.00058                     0           15         0         1                    4             0          0\n                          lunch                 2          0.0012     0.00058            0.00058           0.00058              0.00058      0.00058\nFigure 3.7                                                   0           0          0         0                    1             0          0\n                                                        Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP\ncorpus of 9332            spend                 1            0           1          0         0                    0             0          0\n                   sentences. Previously-zero probabilities are in gray.\n                    Figure 3.1                                      Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-\n                    rant               i                want        to               eat           chinese              food      lunch      spend\n                                Project corpus of 9332 sentences. Zero counts are in gray. Each cell shows the count of\n                    the column label word following the row label word. Thus the cell in row i and column want\n                     i                 3.8              527         0.64             6.4           0.64                 0.64      0.64       1.9\n                    means that want followed i 827 times in the corpus.\n                     want              1.2              0.39        238              0.78          2.7                  2.7       2.3        0.78\nadd-1                to Figure         1.9              0.63        3.1              430           1.9                  0.63      4.4        133\n                     eat             3.2 shows the bigram probabilities after normalization (dividing each cell\n                    in Fig. 3.1        0.34             0.34        1                0.34          5.8                  1         15         0.34\nsmoothed             chinese           by the appropriate unigram for its row, taken from the following set of\n                    unigram            0.2              0.098       0.098            0.098         0.098                8.2       0.2        0.098\n                     food        counts):\n                                       6.9              0.43        6.9              0.43          0.86                 2.2       0.43         0.43\n                     lunch                  i              want    to           eat      chinese    food      lunch            spend\n                                       0.57             0.19        0.19             0.19          0.19                 0.38      0.19         0.19\n                     spend             0.32             0.16        0.32             0.16          0.16                 0.16      0.16         0.16\n                                            2533           927     2417         746      158        1093      341              278\n---\nAdd-1 estimation is a blunt instrument\n\nSo add-1 isn\u2019t used for N-grams:\n \u25e6  Generally we use interpolation or backoff instead\nBut add-1 is used to smooth other NLP models\n \u25e6  Or we can use add-k where 0<k<1\n \u25e6  It's used in naive bayes text classification\n \u25e6  In domains where the number of zeros isn\u2019t so huge.\n---\nBackoff and Interpolation\nSometimes it helps to use a simpler model\n \u25e6  Condition on less context for contexts you know less about\nBackoff:\n \u2022  If enough evidence, use trigram P(w\u2099|w\u2099\u22122w\u2099\u22121)\n \u2022  If not, use bigram P(w\u2099|w\u2099\u22121)\n \u2022  Else unigram P(w\u2099)\n\nInterpolation:\n \u25e6  mix unigram, bigram, trigram\n\nInterpolation works better\n---\n                                             \u02c6\nunigram counts.                              P(wn |wn\u22122 wn\u22121 ) = l1 P(wn |wn\nLinear Interpolation\nIn simple linear interpolation, we combine different order N-grams by linearly+l2 P(wn |\ninterpolating all the models. Thus, we estimate the trigram probability P(wn |wn\u22122 wn\u22121 )\nby mixing together the unigram, bigram, and trigram probabilities, each weighted+l3 P(wn )\nby a l :\nSimple interpolation\n                   such that the l s sum to 1:                  X\n                   \u02c6\n                   P(wn |wn\u22122 wn\u22121 ) =  l1 P(wn |wn\u22122 wn\u22121 )     li = 1\n                                        +l2 P(wn |wn\u22121 )\n                                        +l3 P(wn )              i (4.24)\n\nsuch that the l s sum to 1:In a slightly more sophisticated version of linear i\nLambdas conditional on context:\n                    X\n                   computed in a more sophisticated way, by condition\n                     li = 1                                       (4.25)\n                    i\n                   if we have particularly accurate counts for a particul\nIn a                P(Wn|Wn\u22122Wn\u22121) = \u03bb1 (wn\u22122)P(wn|wn\u22122Wn\u22121)\n         slightly more sophisticated version of linear interpolation, each l weight is\n                   counts of the trigrams based on this bigram will be\ncomputed in a more sophisticated way, by     +\u03bb2(wn\u22122)P(wn|Wn\u22121)\n                                           conditioning on the context. This way,\nif we have particularly accurate counts for a particular bigram, we assume that the\n                   make the l s for those trigrams higher and thus give\ncounts of the trigrams based on this bigram  + \u03bb3(Wn\u22121 )P(Wn)\n                                             will be more trustworthy, so we can\nmake the l s for those trigrams higher and thus give that trigram more weight in\n---\nHow to set \u03bbs for interpolation?\n\nUse a held-out corpus\n\n   Training Data     Held-Out  Test\n                     Data      Data\n\nChoose \u03bbs to maximize probability of held-out data:\n\u25e6  Fix the N-gram probabilities (on the training data)\n\u25e6  Then search for \u03bbs that give largest probability to held-\n   out set\n---\n Backoff\n\nSuppose you want:\n        P(pancakes| delicious souffl\u00e9)\n If the trigram probability is 0, use the bigram\n        P(pancakes| souffl\u00e9)\n If the bigram probability is 0, use the unigram\n        P(pancakes)\nComplication: need to discount the higher-order ngram so\nprobabilities don't sum higher than 1 (e.g., Katz backoff)\n---\nStupid Backoff\n\nBackoff without discounting (not a true probability)\n\n                 \"    count(w\u2071           )\n                 $              i\u2212k+1         if  count(w\u2071  ) > 0\nS (w | wi\u2212\u00b9  ) = $    count(wi\u2212\u00b9         )        i\u2212k+1\ni    i\u2212k+1       #              i\u2212k+1\n                 $    0.4 S (w    | wi\u2212\u00b9          )  otherwise\n                 $\n                 %                i           i\u2212k+2\n\nS (wi ) = count(w\u2071 )\nN\n                    80\n---\n Summary: What to do if you never saw an n-gram\n in training\nSmoothing: Pretend you saw every n-gram one (or k) times\nmore than you did\n \u2022  A blunt instrument (replacing a lot of zeros) but sometimes useful\nBackoff: If you haven't seen the trigram, use the (weighted)\nbigram probability instead\n \u2022  Weighting is messy; \"stupid\" backoff works fine at web-scale\nInterpolation: (weighted) mix of trigram, bigram, unigram\n \u2022  Usually the best! We also use interpolation to combine multiple LLMs\n---\nN-gram    Smoothing, Interpolation,\nLanguage  and Backoff\nModeling\n\n",
        "summary": "Purpose of the Document:\nThe document provides an introduction to N-gram language models, focusing on their use in predicting upcoming words and evaluating their performance. It covers the basics of N-gram models, including their structure, estimation methods, and evaluation metrics like perplexity.\n\nMain Ideas:\n\u2022 N-gram models predict upcoming words based on previous words.\n\u2022 Language models assign probabilities to words or sentences.\n\u2022 N-gram models use the Markov assumption to simplify probability calculations.\n\u2022 Smoothing, interpolation, and backoff techniques handle sparsity in N-gram models.\n\u2022 Perplexity is used to evaluate the performance of language models.\n\nKey Concepts:\n\u2022 N-gram models\n\u2022 Language models\n\u2022 Markov assumption\n\u2022 Smoothing\n\u2022 Interpolation\n\u2022 Backoff\n\u2022 Perplexity\n\u2022 Training and test sets\n\u2022 Sampling\n\nKey Takeaways:\n\u2022 N-gram models predict words based on previous words in a sequence.\n\u2022 The Markov assumption simplifies the calculation of word probabilities.\n\u2022 Smoothing techniques help handle sparsity in N-gram models.\n\u2022 Interpolation and backoff are methods to improve the accuracy of N-gram models.\n\u2022 Perplexity is a metric used to evaluate the performance of language models.\n\u2022 Training and test sets are essential for evaluating model performance.\n\u2022 Sampling methods can be used to generate sentences from N-gram models.",
        "quiz": [
            {
                "question_text": "What is the primary goal of a language model?",
                "answers": [
                    {
                        "text": "To predict upcoming words",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that a language model is a machine learning model that predicts upcoming words."
                    },
                    {
                        "text": "To generate new sentences",
                        "is_correct": false,
                        "explanation": "While language models can generate text, their primary goal is to predict words, not to generate new sentences directly."
                    },
                    {
                        "text": "To correct grammar errors",
                        "is_correct": false,
                        "explanation": "Although word prediction can assist in grammar checking, it is not the primary goal of a language model."
                    },
                    {
                        "text": "To recognize speech patterns",
                        "is_correct": false,
                        "explanation": "Speech recognition is an application of word prediction, but it is not the primary goal of a language model."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the two main paradigms of language models mentioned in the content?",
                "answers": [
                    {
                        "text": "N-gram language models and large language models",
                        "is_correct": true,
                        "explanation": "The content explicitly mentions these as the two main paradigms of language models."
                    },
                    {
                        "text": "Rule-based models and statistical models",
                        "is_correct": false,
                        "explanation": "These terms are not mentioned in the provided content context."
                    },
                    {
                        "text": "Recurrent neural networks and transformers",
                        "is_correct": false,
                        "explanation": "These are specific types of models, not the main paradigms mentioned in the content."
                    },
                    {
                        "text": "Generative models and discriminative models",
                        "is_correct": false,
                        "explanation": "These are broader categories of models, not specifically mentioned as the two main paradigms in the content."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is one application of word prediction in language tasks?",
                "answers": [
                    {
                        "text": "Grammar or spell checking",
                        "is_correct": true,
                        "explanation": "The content explicitly mentions that word prediction is helpful in grammar or spell checking tasks."
                    },
                    {
                        "text": "Generating random sentences",
                        "is_correct": false,
                        "explanation": "The content does not mention generating random sentences as an application of word prediction."
                    },
                    {
                        "text": "Translating languages",
                        "is_correct": false,
                        "explanation": "The content does not mention translating languages as an application of word prediction."
                    },
                    {
                        "text": "Creating new words",
                        "is_correct": false,
                        "explanation": "The content does not mention creating new words as an application of word prediction."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the basic principle behind how large language models (LLMs) generate text?",
                "answers": [
                    {
                        "text": "By predicting the next word over and over again",
                        "is_correct": true,
                        "explanation": "The content explicitly states that LLMs generate text by predicting the next word repeatedly."
                    },
                    {
                        "text": "By using predefined templates for sentences",
                        "is_correct": false,
                        "explanation": "The content does not mention the use of predefined templates for text generation."
                    },
                    {
                        "text": "By randomly selecting words from a dictionary",
                        "is_correct": false,
                        "explanation": "The content does not suggest that LLMs use random word selection from a dictionary."
                    },
                    {
                        "text": "By analyzing the grammatical structure of sentences",
                        "is_correct": false,
                        "explanation": "While grammar is mentioned as a part of language tasks, it is not described as the principle behind text generation in LLMs."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What does the Chain Rule of Probability state?",
                "answers": [
                    {
                        "text": "The Chain Rule of Probability states that the joint probability of a sequence of words can be decomposed into the product of conditional probabilities of each word given all previous words.",
                        "is_correct": true,
                        "explanation": "This is the direct application of the Chain Rule of Probability as described in the content context."
                    },
                    {
                        "text": "The Chain Rule of Probability states that the probability of a word is independent of all previous words.",
                        "is_correct": false,
                        "explanation": "This contradicts the Chain Rule, which explicitly involves conditional probabilities given previous words."
                    },
                    {
                        "text": "The Chain Rule of Probability states that the probability of a sentence is the sum of the probabilities of each individual word.",
                        "is_correct": false,
                        "explanation": "The Chain Rule involves multiplication of conditional probabilities, not summation."
                    },
                    {
                        "text": "The Chain Rule of Probability states that the probability of a word is equal to the probability of the previous word.",
                        "is_correct": false,
                        "explanation": "The Chain Rule does not equate the probability of a word to the probability of the previous word but rather involves conditional probabilities."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the formula for the joint probability of a sequence of words according to the Chain Rule?",
                "answers": [
                    {
                        "text": "P(w\u2081)P(w\u2082|w\u2081)P(w\u2083|w\u2081,w\u2082)...P(w\u2099|w\u2081,...,w\u2099\u208b\u2081)",
                        "is_correct": true,
                        "explanation": "This is the correct formula for the joint probability of a sequence of words according to the Chain Rule as described in the content."
                    },
                    {
                        "text": "P(w\u2081) + P(w\u2082|w\u2081) + P(w\u2083|w\u2081,w\u2082) + ... + P(w\u2099|w\u2081,...,w\u2099\u208b\u2081)",
                        "is_correct": false,
                        "explanation": "This option incorrectly uses addition instead of multiplication, which is not the correct application of the Chain Rule."
                    },
                    {
                        "text": "P(w\u2081) \u00d7 P(w\u2082) \u00d7 P(w\u2083) \u00d7 ... \u00d7 P(w\u2099)",
                        "is_correct": false,
                        "explanation": "This option ignores the conditional probabilities and only considers the marginal probabilities of each word."
                    },
                    {
                        "text": "P(w\u2081|w\u2082) \u00d7 P(w\u2082|w\u2083) \u00d7 P(w\u2083|w\u2084) \u00d7 ... \u00d7 P(w\u2099\u208b\u2081|w\u2099)",
                        "is_correct": false,
                        "explanation": "This option reverses the order of the conditional probabilities, which is not consistent with the Chain Rule."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the main challenge in estimating probabilities of entire sentences directly from relative frequency counts?",
                "answers": [
                    {
                        "text": "There are too many possible sentences to count their frequencies accurately.",
                        "is_correct": true,
                        "explanation": "The content explicitly states that language is creative and new sentences are constantly invented, making it impossible to gather enough data to estimate probabilities of entire sentences directly from relative frequency counts."
                    },
                    {
                        "text": "The computational power required to process entire sentences is insufficient.",
                        "is_correct": false,
                        "explanation": "The content does not mention computational power as a limitation; it focuses on the vast number of possible sentences and the inability to gather sufficient data."
                    },
                    {
                        "text": "The probability distributions for entire sentences are too complex to model.",
                        "is_correct": false,
                        "explanation": "While complexity might be a factor, the content specifically highlights the issue of data insufficiency rather than the complexity of modeling probability distributions."
                    },
                    {
                        "text": "Existing language models are not designed to handle entire sentences.",
                        "is_correct": false,
                        "explanation": "The content does not state that language models are incapable of handling entire sentences; it discusses the challenge of estimating probabilities due to the vast number of possible sentences."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the primary purpose of an N-gram language model?",
                "answers": [
                    {
                        "text": "To predict the probability of a sequence of words or the next word in a sequence",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that a language model, including an N-gram model, predicts upcoming words and assigns probabilities to sequences or next words."
                    },
                    {
                        "text": "To generate random words from a predefined vocabulary",
                        "is_correct": false,
                        "explanation": "The concept description does not mention generating random words; it focuses on predicting words based on probabilities."
                    },
                    {
                        "text": "To correct grammar and spelling errors in text",
                        "is_correct": false,
                        "explanation": "While word prediction can assist in grammar and spell checking, the primary purpose of an N-gram model is to predict words, not directly correct errors."
                    },
                    {
                        "text": "To create new sentences from scratch without any input",
                        "is_correct": false,
                        "explanation": "The concept description explains that N-gram models predict words based on previous words, not create sentences from nothing."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    }
}