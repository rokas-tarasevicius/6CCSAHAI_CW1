{
    "data/raw/Week 17 - All slides in one.pdf": {
        "metadata": {
            "file_name": "Week 17 - All slides in one.pdf",
            "file_type": "pdf",
            "content_length": 8289,
            "language": "en",
            "extraction_timestamp": "2025-11-26T03:23:17.829327+00:00",
            "timezone": "utc"
        },
        "content": "1\n\nVariational autoencoders\n\nLuis C. Garcia Peraza Herrera\n---\n   2\nAgenda\n\n   Latent variable models\n   Nonlinear latent variable model\n   Training\n   ELBO properties\n   Variational approximation\n   The variational autoencoder\n   The reparametrisation trick\n   Applications\n   Conclusion\n---\n3\n\nLatent variable models\n---\n   4\nLatent variable models\n\n   Model a joint distribution P rpx, zq where z is an unobserved latent variable\n\n   Describe the probability of P rpxq as a marginalization of this joint\n   probability so that:\n                       \u017c    \u017c\n   P rpxq \u201c                 P rpx, zqdz \u201c  P rpx|zqP rpzqdz\n---\n  5\nExample: mixture of Gaussians\n\n  In a 1D mixture of Gaussians:\n\n  The likelihood P rpxq is given by the marginalization over the latent variable\n  z:\n---\n   6\nExample: mixture of Gaussians\n---\n  7\n\nNonlinear latent variable model\n---\n  8\nNonlinear latent variable model\n\n  The prior P rpzq is a standard multivariate normal:\n\n  The likelihood P rpx|z, \u03d5q is also normally distributed:\n\n  The data probability P rpx|\u03d5q is found by marginalizing over the latent\n  variable z:\n---\n   9\nNonlinear latent variable model\n---\n   10\n\nGeneration\n\n   Ancestral sampling process:\n   \u201a Draw latent variable \u00d1 sample a latent variable z\u02da from the prior\n   distribution P rpzq:\n                              z\u02da \u201e P rpzq\n   \u201a Compute likelihood mean \u00d1 pass z\u02da through the network f rz\u02da, \u03d5s to\n   compute the mean of the likelihood distribution P rpx | z\u02da, \u03d5q:\n\n                              Mean `P rpx | z\u02da, \u03d5q\u02d8\n   \u201a Draw data sample \u00d1 draw a new example x\u02da from the likelihood distribution\n   based on the computed mean:\n\n                              x\u02da \u201e P rpx | z\u02da, \u03d5q\n---\n   11\nGeneration\n---\n12\n\nTraining\n---\n   13\nTraining\n\n   To train the model, we maximise the log-likelihood over a training dataset\n   txiuI  with respect to the model parameters:\n   i\u201c1\n\n   where:\n---\n   14\nEvidence lower bound (ELBO)\n\n   We define a lower bound on the log-likelihood\n\n   This is a function that is always less than or equal to the log-likelihood for a\n   given value of \u03d5 and will also depend on some other parameters \u03b8\n\n   To define this lower bound, we need Jensen\u2019s inequality\n---\n   15\nJensen\u2019s inequality\n\n   Jensen\u2019s inequality says that a concave function g of the expectation of data\n   y is greater than or equal to the expectation of the function of the data:\n\n   If the concave function is the logarithm:\n---\n   16\nJensen\u2019s inequality: discrete case\n---\n   17\nJensen\u2019s inequality: continuous case\n---\n   18\nDeriving the bound\n\n   We use Jensen\u2019s inequality to derive the lower bound for the log-likelihood:\n---\n   19\nDeriving the bound\n\n   In practice the distribution qpzq has parameters \u03b8, so the ELBO can be\n   written as:\n\n   To learn the nonlinear latent variable model, we maximize this quantity as a\n   function of both \u03d5 and \u03b8\n\n   The neural architecture that computes this quantity is the VAE\n---\n20\n\nELBO properties\n---\n   21\nELBO properties\n\n   The original log-likelihood of the data is a function of the parameters \u03d5\n\n   We want to find its maximum\n\n   Depending on our choice of \u03b8 the lower bound may move closer or further\n   from the log-likelihood\n\n   When we change \u03d5, we move along the lower bound function\n---\n   22\nELBO properties\n---\n                                                                           23\nTightness of bound\n  To find the distribution qpz|\u03b8q that makes the bound tight, we factor the\n  numerator of the log term in the ELBO using the definition of conditional\n  probability:\n\n  The KL distance will be zero and the bound tight when:\n\n  qpz|\u03b8q \u201c P rpz|x, \u03d5q                                  (1)\n---\n   24\nTightness of bound\n---\n                                                  25\nELBO as a reconstruction loss minus KL distance to\nprior\n\n   We have seen two different ways to express the ELBO:\n   1.\n\n   2.\n---\n                                                  26\nELBO as a reconstruction loss minus KL distance to\nprior\n\n   A third way is to consider the bound as reconstruction error minus the\n   distance to the prior:\n---\n27\n\nVariational approximation\n---\n   28\nVariational approximation\n\n   The ELBO is tight when qpz|\u03b8q is the posterior P rpz|x, \u03d5q\n   Cannot use Bayes\u2019 rule because P rpx|\u03d5q \u00d1 intractable\n   One solution: we choose a simple parametric form for qpz|\u03b8q and use this to\n   approximate the true posterior\n   Since the optimal choice for qpz|\u03b8q was the posterior P rpz|xq, and this\n   depends on the data example x, the variational approximation should do\n   the same:\n\n   where grx, \u03b8s is a second neural network with parameters \u03b8 that predicts\n   the mean \u00b5 and variance \u03a3 of the normal variational approximation\n---\n   29\nVariational approximation\n---\n30\n\nThe variational autoencoder\n---\n   31\nThe variational autoencoder\n\n   We build a network that computes the ELBO:\n\n   where the distribution qpz|x, \u03b8q is the approximation:\n\n   The first term involves an intractable integral, we can approximate it by\n   sampliing, for any function a we have:\n\n   where z\u02da is the n-th sample from qpz|x, \u03b8q\n   n\n---\n   32\nThe variational autoencoder\n\n   For a very approximate estimate, we can just use a single sample z\u02da from\n   qpz|x, \u03b8q:\n\n   The second term is the KL divergence between the variational distribution\n   qpz|x, \u03b8q \u201c Normz r\u00b5, \u03a3s and the prior P rpzq \u201c Normz r0, Is:\n\n   where Dz is the dimensionality of the latent space\n---\n   33\n\nVAE algorithm\n\n   We aim to build a model that computes the evidence lower bound for a\n   point x\n   We use an optimization algorithm to maximize this lower bound over the\n   dataset and hence improve the log-likelihood\n   To compute the ELBO we:\n    \u201a Compute the mean \u00b5 and variance \u03a3 of the variational posterior distribution\n    qpz|\u03b8, xq for this data point x using the network gpx, \u03b8q\n    \u201a Draw a sample z\u02da from this distribution\n    \u201a Compute the ELBO using:\n---\n   34\nVAE algorithm\n---\n   35\nVAE algorithm\n---\n36\n\nThe reparametrisation trick\n---\n   37\nThe reparametrisation trick\n\n   The network involves a sampling step that is difficult to differentiate\n   We can move the stochastic part into a branch of the network that draws a\n   sample \u03f5\u02da from Norm\u03f5r0, Is and then use the following relation to draw\n   from the Gaussian:\n                     z\u02da \u201c \u00b5 ` \u03a31{2\u03f5\u02da                                      (2)\n---\n   38\nThe reparametrisation trick\n---\n39\n\nApplications\n---\n   40\nApproximating sample probability\n\n   The VAE describes the probability of a sample as:\n\n   In principle we could approximate this probability by drawing samples from\n   P rpzq \u201c Normz r0, Is and computing:\n\n   Bad news: huge number of samples to get a reliable estimate\n---\n   41\nApproximating sample probability\n\n   A better approach is to use importance sampling:\n\n   where now we draw samples from qpzq\n---\n   42\nGeneration\n\n   Samples from vanilla VAEs are generally low-quality:\n    \u201a Naive spherical Gaussian noise model\n    \u201a Gaussian models used for prior and variational posterior\n\n   To improve generation quality, sample from the aggregated posterior:\n\n    qpz|\u03b8q \u201c 1 \u00ff qpz|xi, \u03b8q                                            (3)\n    I  i\n---\n   43\nGeneration\n---\n   44\n\nResynthesis\n\n   VAEs can also be used to modify real data:\n\n   1. Taking the mean of the distribution predicted by the encoder\n   2. Solving an optimisation problem to find the latent variable z that maximises\n   the posterior probability\n---\n   45\nResynthesis\n---\n   46\nDisentanglement\n\n   When each dimension of z represents an independent real-world factor, the\n   latent space is described as disentangled\n\n   To encourage disentanglement:\n\n   The beta VAE upweights the second term in the ELBO:\n---\n   47\nDisentanglement\n---\n48\n\nConclusion\n---\n   49\n\nConclusion\n\n   VAEs learn a nonlinear latent variable model over x\n   Generation process:\n    \u201a Sample from the latent variable\n    \u201a Pass the result through a deep network\n    \u201a Add independent Gaussian noise\n\n   Challenges:\n    \u201a Not possible to compute likelihood of a data point in closed form\n    \u201a Computing the posterior probability of the latent variable given observed\n    data is intractable\n\n   Solution: variational approximation\n\n   Enhancement: sophisticated latent space modeling (e.g. hierarchical priors)\n---\n   50\nThanks!\n\n",
        "quiz": [
            {
                "question_text": "What is the primary focus of Week 17 - All Slides In One?",
                "answers": [
                    {
                        "text": "Latent variable models and variational autoencoders",
                        "is_correct": true,
                        "explanation": "The primary focus of Week 17 - All Slides In One is on latent variable models and variational autoencoders, as indicated by the agenda and the detailed content on these topics."
                    },
                    {
                        "text": "Deep learning fundamentals",
                        "is_correct": false,
                        "explanation": "The content does not focus on general deep learning fundamentals but specifically on latent variable models and variational autoencoders."
                    },
                    {
                        "text": "Convolutional neural networks",
                        "is_correct": false,
                        "explanation": "The slides do not cover convolutional neural networks; they focus on latent variable models and variational autoencoders."
                    },
                    {
                        "text": "Recurrent neural networks",
                        "is_correct": false,
                        "explanation": "The content does not address recurrent neural networks; it is centered around latent variable models and variational autoencoders."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "Who is the author associated with the content on variational autoencoders?",
                "answers": [
                    {
                        "text": "Luis C. Garcia Peraza Herrera",
                        "is_correct": true,
                        "explanation": "The content context explicitly mentions Luis C. Garcia Peraza Herrera as the author associated with the content on variational autoencoders."
                    },
                    {
                        "text": "Week 17 - All Slides In One",
                        "is_correct": false,
                        "explanation": "This is the title of the document, not the author."
                    },
                    {
                        "text": "Latent variable models",
                        "is_correct": false,
                        "explanation": "This is a topic discussed in the content, not the author."
                    },
                    {
                        "text": "Variational autoencoders",
                        "is_correct": false,
                        "explanation": "This is the subject of the content, not the author."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the main topic of the agenda in Week 17 - All Slides In One?",
                "answers": [
                    {
                        "text": "Latent variable models",
                        "is_correct": true,
                        "explanation": "The agenda explicitly lists 'Latent variable models' as the first topic."
                    },
                    {
                        "text": "Deep learning architectures",
                        "is_correct": false,
                        "explanation": "The agenda does not mention 'Deep learning architectures' as a topic."
                    },
                    {
                        "text": "Convolutional neural networks",
                        "is_correct": false,
                        "explanation": "The agenda does not mention 'Convolutional neural networks' as a topic."
                    },
                    {
                        "text": "Recurrent neural networks",
                        "is_correct": false,
                        "explanation": "The agenda does not mention 'Recurrent neural networks' as a topic."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What type of models are used to describe the probability of P(rpX) in Week 17 - All Slides In One?",
                "answers": [
                    {
                        "text": "Latent variable models",
                        "is_correct": true,
                        "explanation": "The concept description and content context explicitly mention that latent variable models are used to describe the probability of P(rpX)."
                    },
                    {
                        "text": "Variational autoencoders",
                        "is_correct": false,
                        "explanation": "While variational autoencoders are mentioned in the content context, they are not the primary models used to describe the probability of P(rpX)."
                    },
                    {
                        "text": "Mixture of Gaussians",
                        "is_correct": false,
                        "explanation": "Mixture of Gaussians is an example of a latent variable model, not the general type of model used to describe the probability of P(rpX)."
                    },
                    {
                        "text": "Nonlinear latent variable models",
                        "is_correct": false,
                        "explanation": "Nonlinear latent variable models are a specific type of latent variable model, but the general term 'latent variable models' is the correct answer."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the prior distribution P(rpZ) in the nonlinear latent variable model?",
                "answers": [
                    {
                        "text": "A standard multivariate normal distribution",
                        "is_correct": true,
                        "explanation": "The concept description specifies that the prior distribution P(rpZ) in the nonlinear latent variable model is a standard multivariate normal."
                    },
                    {
                        "text": "A uniform distribution",
                        "is_correct": false,
                        "explanation": "The concept description does not mention a uniform distribution for the prior P(rpZ)."
                    },
                    {
                        "text": "A Poisson distribution",
                        "is_correct": false,
                        "explanation": "The concept description does not mention a Poisson distribution for the prior P(rpZ)."
                    },
                    {
                        "text": "A Bernoulli distribution",
                        "is_correct": false,
                        "explanation": "The concept description does not mention a Bernoulli distribution for the prior P(rpZ)."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the likelihood distribution P(rpX|z, \u03d5) in the nonlinear latent variable model?",
                "answers": [
                    {
                        "text": "A standard multivariate normal distribution",
                        "is_correct": true,
                        "explanation": "The likelihood distribution P(rpX|z, \u03d5) in the nonlinear latent variable model is normally distributed, as stated in the concept description."
                    },
                    {
                        "text": "A uniform distribution",
                        "is_correct": false,
                        "explanation": "The concept description specifies that the likelihood distribution is normally distributed, not uniform."
                    },
                    {
                        "text": "A Poisson distribution",
                        "is_correct": false,
                        "explanation": "The concept description does not mention a Poisson distribution; it specifies a normal distribution."
                    },
                    {
                        "text": "A Bernoulli distribution",
                        "is_correct": false,
                        "explanation": "The concept description clearly states that the likelihood distribution is normally distributed, not Bernoulli."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the first step in the ancestral sampling process for generation?",
                "answers": [
                    {
                        "text": "Draw latent variable z\u02da from the prior distribution P rpzq",
                        "is_correct": true,
                        "explanation": "The first step in the ancestral sampling process for generation is to draw a latent variable z\u02da from the prior distribution P rpzq, as explicitly stated in the content context."
                    },
                    {
                        "text": "Compute the mean of the likelihood distribution P rpx | z\u02da, \u03d5q",
                        "is_correct": false,
                        "explanation": "This is the second step in the process, not the first."
                    },
                    {
                        "text": "Draw a new example x\u02da from the likelihood distribution",
                        "is_correct": false,
                        "explanation": "This is the third step in the process, not the first."
                    },
                    {
                        "text": "Maximize the log-likelihood over a training dataset",
                        "is_correct": false,
                        "explanation": "This is related to training the model, not the first step in the ancestral sampling process for generation."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the second step in the ancestral sampling process for generation?",
                "answers": [
                    {
                        "text": "Compute likelihood mean by passing z\u02da through the network f(z\u02da, \u03d5) to compute the mean of the likelihood distribution P(x | z\u02da, \u03d5)",
                        "is_correct": true,
                        "explanation": "This is the second step in the ancestral sampling process for generation as described in the content."
                    },
                    {
                        "text": "Draw latent variable by sampling a latent variable z\u02da from the prior distribution P(z)",
                        "is_correct": false,
                        "explanation": "This is the first step in the process, not the second."
                    },
                    {
                        "text": "Draw a new example x\u02da from the likelihood distribution based on the computed mean",
                        "is_correct": false,
                        "explanation": "This is the third step in the process, not the second."
                    },
                    {
                        "text": "Maximize the log-likelihood over a training dataset",
                        "is_correct": false,
                        "explanation": "This is related to training the model, not part of the ancestral sampling process for generation."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the third step in the ancestral sampling process for generation?",
                "answers": [
                    {
                        "text": "Draw a new example x\u02da from the likelihood distribution based on the computed mean",
                        "is_correct": true,
                        "explanation": "The third step in the ancestral sampling process for generation is to draw a new example x\u02da from the likelihood distribution based on the computed mean."
                    },
                    {
                        "text": "Compute the mean of the likelihood distribution P rpx | z\u02da, \u03d5q",
                        "is_correct": false,
                        "explanation": "This is the second step in the process, not the third."
                    },
                    {
                        "text": "Sample a latent variable z\u02da from the prior distribution P rpzq",
                        "is_correct": false,
                        "explanation": "This is the first step in the process, not the third."
                    },
                    {
                        "text": "Maximize the log-likelihood over a training dataset",
                        "is_correct": false,
                        "explanation": "This is part of the training process, not the generation process."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the objective function to maximize during the training of the model?",
                "answers": [
                    {
                        "text": "The Evidence Lower Bound (ELBO)",
                        "is_correct": true,
                        "explanation": "The content explicitly states that during training, the objective is to maximize the Evidence Lower Bound (ELBO)."
                    },
                    {
                        "text": "The log-likelihood of the data",
                        "is_correct": false,
                        "explanation": "While the log-likelihood is mentioned, the objective is to maximize the ELBO, which is a lower bound on the log-likelihood."
                    },
                    {
                        "text": "The KL distance to the prior",
                        "is_correct": false,
                        "explanation": "The KL distance is part of the ELBO calculation, but it is not the objective function to be maximized."
                    },
                    {
                        "text": "The reconstruction error",
                        "is_correct": false,
                        "explanation": "Reconstruction error is a component of the ELBO, but the objective is to maximize the entire ELBO, not just the reconstruction error."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the purpose of the evidence lower bound (ELBO) in the context of training?",
                "answers": [
                    {
                        "text": "To provide a lower bound on the log-likelihood during training",
                        "is_correct": true,
                        "explanation": "The ELBO is defined as a lower bound on the log-likelihood for a given value of the model parameters."
                    },
                    {
                        "text": "To maximize the log-likelihood directly",
                        "is_correct": false,
                        "explanation": "The ELBO is used to provide a lower bound, not to maximize the log-likelihood directly."
                    },
                    {
                        "text": "To compute the exact posterior distribution",
                        "is_correct": false,
                        "explanation": "The ELBO is a lower bound and does not compute the exact posterior distribution."
                    },
                    {
                        "text": "To sample new data points from the prior distribution",
                        "is_correct": false,
                        "explanation": "The ELBO is related to the training process, not the generation of new data points."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What inequality is used to derive the lower bound for the log-likelihood?",
                "answers": [
                    {
                        "text": "Jensen's inequality",
                        "is_correct": true,
                        "explanation": "The content explicitly states that Jensen's inequality is used to derive the lower bound for the log-likelihood."
                    },
                    {
                        "text": "Bayes' rule",
                        "is_correct": false,
                        "explanation": "Bayes' rule is mentioned in the context of the posterior distribution but not as the method to derive the lower bound for the log-likelihood."
                    },
                    {
                        "text": "Ancestral sampling",
                        "is_correct": false,
                        "explanation": "Ancestral sampling is a process for generating data samples, not for deriving the lower bound for the log-likelihood."
                    },
                    {
                        "text": "KL distance",
                        "is_correct": false,
                        "explanation": "The KL distance is a component of the ELBO but not the inequality used to derive it."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the condition for the ELBO to be tight?",
                "answers": [
                    {
                        "text": "qpz|\u03b8q \u201c P rpz|x, \u03d5q",
                        "is_correct": true,
                        "explanation": "This is the condition for the ELBO to be tight, as stated in the content."
                    },
                    {
                        "text": "qpz|\u03b8q \u201c P rpx|z, \u03d5q",
                        "is_correct": false,
                        "explanation": "This is incorrect because it describes the likelihood, not the condition for the ELBO to be tight."
                    },
                    {
                        "text": "qpz|\u03b8q \u201c P rpzq",
                        "is_correct": false,
                        "explanation": "This is incorrect because it describes the prior distribution, not the condition for the ELBO to be tight."
                    },
                    {
                        "text": "qpz|\u03b8q \u201c P rpxq",
                        "is_correct": false,
                        "explanation": "This is incorrect because it describes the marginal probability of the data, not the condition for the ELBO to be tight."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the variational approximation used for in the context of the posterior distribution?",
                "answers": [
                    {
                        "text": "To approximate the true posterior distribution P(rpz|x, \u03d5) when it is intractable",
                        "is_correct": true,
                        "explanation": "The variational approximation is used to approximate the true posterior distribution when it cannot be directly computed using Bayes' rule due to intractability."
                    },
                    {
                        "text": "To maximize the log-likelihood of the data directly",
                        "is_correct": false,
                        "explanation": "The variational approximation is not used to maximize the log-likelihood directly but to approximate the posterior distribution."
                    },
                    {
                        "text": "To define the prior distribution P(rpz)",
                        "is_correct": false,
                        "explanation": "The variational approximation is not used to define the prior distribution but to approximate the posterior distribution."
                    },
                    {
                        "text": "To compute the evidence lower bound (ELBO) without approximation",
                        "is_correct": false,
                        "explanation": "The variational approximation is used to approximate the posterior distribution, which is part of computing the ELBO, but it does involve approximation."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the neural architecture that computes the ELBO called?",
                "answers": [
                    {
                        "text": "Variational autoencoder",
                        "is_correct": true,
                        "explanation": "The neural architecture that computes the ELBO is explicitly referred to as the variational autoencoder in the provided content."
                    },
                    {
                        "text": "Latent variable model",
                        "is_correct": false,
                        "explanation": "While latent variable models are discussed, they are not specifically identified as the architecture that computes the ELBO."
                    },
                    {
                        "text": "Nonlinear latent variable model",
                        "is_correct": false,
                        "explanation": "Nonlinear latent variable models are mentioned, but they are not the architecture that computes the ELBO."
                    },
                    {
                        "text": "Reconstruction loss network",
                        "is_correct": false,
                        "explanation": "Reconstruction loss is part of the ELBO but not the name of the neural architecture that computes it."
                    }
                ],
                "topic": "Week 17 - All Slides In One",
                "subtopic": "Main Content",
                "concepts": [
                    "Week 17 - All Slides In One"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    },
    "data/raw/6CCSAHAI-HumanAI Interaction-Lect.pdf": {
        "metadata": {
            "file_name": "6CCSAHAI-HumanAI Interaction-Lect.pdf",
            "file_type": "pdf",
            "content_length": 34804,
            "language": "en",
            "extraction_timestamp": "2025-11-26T03:24:03.042075+00:00",
            "timezone": "utc"
        },
        "content": "6CCSAHAI\n\n  Human-centred AI\n  as a Design Process\n  Part 1: Understanding user needs\n  & Defining the right problem\n  Dr Georgia Panagiotidou\n\n  1\n---\nOverview\n\nWhat is Human-Centred Design?\n \u2022     Who to involve in the design process and why\n \u2022     Degrees and moments of user involvement\n \u2022     Four basic activities of the design process\n \u2022     A simple lifecycle model for the design process\nThe process supports understanding:\n \u2022     How to find out what people need\n \u2022     How to decide what to design\n \u2022     How to generate alternative designs\n \u2022     How to choose among alternative designs\n\n2\n---\nWhat is Human-Centred Design?\n\n\u2022 It is a process that is:\n  \u25aa  Focused on discovering requirements, designing to fulfil requirements,\n     producing prototypes and evaluating them\n  \u25aa  Traditionally focused on users and their goals although with HAI this is\n     somewhat evolving\n  \u25aa  Involves trade-offs to balance conflicting needs and requirements\n\n3\n---\n    ENGAGEMENT\n    Connecting the dots and building relationships\n    between different citizens, stakeholders and partners\n\n                 DESIGN\n               PRINCIPLES\n         1. Be People Centered\n2. Communicate (Visually & Inclusively)\n       3. Collaborate & Co-Create\n      4 Iterate, Iterate, Iterate\n\n    CHALLENGE    OUTCOME\n\n    I Discover  Derine           Develop  Deiver\n                          METHODS\n         BANK\n\nExplore, Shape, Build\n\nCreating the conditions that allow innovation,\n including culture change, skills and mindset\n    LEADERSHIP\n              4\n---\n    INSPIRATION                       IDEATION                                   IMPLEMENTATION\n\nI have a design challenge.        I have an opportunity for design.\nHow do I get started?             How do I interpret what I've learned?\nHow do I conduct an interview?    How do I turn my insights into\nHow do I stay human-centered?     tangible ideas?\n                                  How do I make a prototype?\nI have an innovative solution.\nHow do I make my concept real?\n How do I assess if it's working?\n How do I plan for sustainability?\n\n\n    DvEs 7                            CONE                                 7\n                                                         DVERCE                  CONVERGE\n\n    IDEO\n        5\n---\nStanford d.school Design Thinking Process\n\nInterviews                       Share ideas\nShadowing                        All ideas worthy\n\u2022Seek to understand              Diverge/Converge\nNon-judgmental                   \"Yes and\" thinking\n                                 Prioritize        Mockups\n                                                    Storyboards\nEMPATHIZE              IDEATE                      Keep it simple\n                                                   \u2022Fail fast\n                                                   Iterate quickly\n\n                  DEFINE                   PROTOYPE\nPersonas\n\u00b7Role objectives\n \u2022 Decisions\n Challenges                                             Test\n \u00b7 Pain Points                    Understand impediments\n                                   What works?\n https://dschool.stanford.edu     Role play\n                                  Iterate quickly\n\n6\n---\nOther lifecycle models:\nDesign Sprints\n\n  Phase 1  Phase 2    Phase 3  Phase 4    Phase 5       Phase 6\n\n  UNDERSTAND  DEFINE    SKETCH  DECIDE    PROTOTYPE     VALIDATE\n\n  designsprintkit.withgoogle.com/methodology/overview\n\n  7\n---\n                     ENGAGEMENT\n    Today\u2019s focus    Connecting the dots and building relationships\n                 between different citizens, stakeholders and partners\n\n                                         DESIGN\n                                       PRINCIPLES\n                                 1. Be People Centered\n                        2. Communicate (Visually & Inclusively)\n                               3. Collaborate & Co-Create\n                              4 Iterate, Iterate, Iterate\n\n    - Discover / Empathise / Understand    CHALLENGE  I Discover  Derine    Develop  Deiver    OUTCOME\n\n    - Define    METHODS\n                  BANK\n\n         Explore, Shape, Build\n\nCreating the conditions that allow innovation,\n including culture change, skills and mindset\n    LEADERSHIP\n\n    Interviews                      Share ideas\n    Shadowing                       All ideas worthy\n    Seek to understand              \u00b7Diverge/Converge\n    Non-judgmental                  \"Yes and\" thinking\n                                    Prioritize        \u00b7Mockups\n                                                       Storyboards\n    EMPATHIZE             IDEATE                      Keep it simple\n                                                      \u2022Fail fast\n                                                      Iterate quickly    Phase 1  Phase 2    Phase 3  Phase 4    Phase 5  Phase 6\n\n                   DEFINE                     PROTOYPE     UNDERSTAND  DEFINE    SKETCH  DECIDE    PROTOTYPE  VALIDATE\n    Personas\n    Role objectives\n    \u2022Decisions                                             TesT\n    Challenges\n    \u2022 Pain Points                    Understand impediments\n                                     What works?\n    https://dschool.stanford.edu     Role play\n                                     Iterate quickly\n                                                                                                              8\n---\nImportance of involving stakeholders\n\nExpectation management\n\u2022  Realistic expectations\n\u2022  No surprises, no disappointments\n\u2022  Timely training\n\u2022  Communication, but no hype\n\nOwnership\n\u2022  Make the users active stakeholders\n\u2022  More likely to forgive or accept problems\n\u2022  Can make a big difference in acceptance and success of product\n\n9\n---\nDiscover: understanding users\u2019 needs\n\n                                    THE UX DESIGNER PARADOX\n\n                      WHAT WE DREAM           WHAT WE SETTLE    WHAT THE\n                      UP AT KICKOFF           FOR AT LAUNCH     USER NEEDS\n\n                                    LONG RANGE\n                                    SUPERSONIC\n ... so you don\u2019t     TITANIUM-     ANTENNA\n                      PLATED\n      end up          NOSE\n                      CONE          ONE WAY                     ANTENNA\n    with this                       MIRRO\n               COMMEMORATIVE        VIEWPORT          NOSE      4\n               CUSTOM               PLUTONIUM-        CONE\n DISCOVER      ARTWORK              FUELED\n                                    TWIN SIDE\n User needs    TITANIUM             BOOSTERS                     FINS    BIKE\n               FINS(x4)                    NICKEL-    SINGLE                 RAMP\n                                    PLAPED            BEOSPER\n                                           RIVETS\n\n                                                                             BONUS 2015\n\n                                                                             10\n---\nDegrees of stakeholder involvement\n\n\u2022  Member of the design team\n\u2022  Small group or individual activities\n\u2022  Online contributions from thousands of users\n    \u25aa  Online Feedback Exchange (OFE) systems\n    \u25aa  Crowdsourcing design ideas\n    \u25aa  Citizen engagement\n\u2022  Participatory design\n\u2022  User involvement after product release\n    \u25aa  A/B testing\n    \u25aa  Customer reviews\n\n11\n---\nHow do needs and requirements interact?\n\n              1. To understand as much as possible about the users,\n              their activities, and the context of that activity, so the\nEMPATHISE     system under development can support them in achieving\n              their goals\n\nDEFINE     2. To produce a set of initial requirements that form a\n           sound basis to start designing\n\n12\n---\n              1. To understand as much as possible about the users,\n              their activities, and the context of that activity, so the\nEMPATHISE     system under development can support them in achieving\n              their goals\n\nwhat are requirements?\n\n           A requirement is a statement about an intended product that specifies what it should\n           do or how it should perform. One of the aims of the requirements activity is to make\nDEFINE     the requirements as specific, unambiguous, and clear as possible.\n\n                      (from Interaction Design: Beyond Human-\n                      Computer Interaction 4th ed, p353)\n\n                                                                            13\n---\na need\ntakes a user's perspective (of the problem to be solved)\n\nand describes constraints, goals, hopes,\nand activities performed by the user\n\nis often contextual, embedded,                          focusing and selection\nand social                                              translation to solutions\n                                                        and { partial } commitment to approach\n\n                                                         a requirement\n                                                         refers to properties of the system (to solve the problem)\n\n                                                         and describes what operations the system\n                                                         needs to afford, or what characteristics\n                                                         and properties it needs to have, and\n                                                         how it should operate\n\n                                                         usually embodies or at least partially commits to one\n                                                         particular solution, philosophy or\n                                                         approach to addressing needs\n\n                                                         there may be many strategies approaches to address a\n                                                         user's need\n\n                                                                                                                  14\n---\nHow do we { elicit, discover, understand } needs?\n\n \u00a9 MAZK ANDEZSON  Can you list all    WWW.ANDEIZTOONS.COM\n                  your needs please\n\n JDoSl\n\n 15\n---\nHow do we { elicit, discover, understand } users' needs?\n\nIf I had asked the people what they\nwanted, they would have said\nfaster horses. -Henry Ford\n\n16\n---\nWays of identifying stakeholder needs:\n                                                                    THIRD EDITION\n                                                                    Designing Interactive\nObservation (direct / indirect)                                    Systems\n                                                                   A comprehensive guide to HCI, UX and interaction design\nInterviews\n\nQuestionnaires\n\n(Digital) ethnography\n\nTechnology probes\n\nParticipant Observation (Field studies as a Participant Observer)   David Benyon\n\nCritical Design                                                    WTELE         PEARSON\n                                                                   Chapter 7\n---\nDEFINE\n\nMoving from individual observations to generalised insights\n\n... that help us focus on the \u2018right\u2019 design problem\n\n18\n---\nWhat does this child need?\n\n   19\n---\n    Sasha Costanza-Chock @schock \u00b7 5 Nis 2023\n\n    People working on AI\n\n People most likely\nto be harmed by the\nways companies and\ngovernments use Al\n\n    20\n---\n Stakeholder involvement in\n developing Human-AI systems\n\nBusiness    Data             AI    Al testing and    Deployment\n  case    preparation    modelling    evaluation\n\nWhy is Al needed?    Collecting data      Dataset    Dataset    Integration with\n                                                                complex system\n   What can be                          0\n    improved?                           0 o\n Cleaning and                           0 Model design    System verification    User uses the\nlabelling data                                            and validation         system\n\n   Beatrice Vincenzi, Simone Stumpf, Alex S. Taylor, and Yuri Nakao. 2024. Lay User Involvement in Developing Human -centric Responsible\n   AI Systems: When and How? ACM J. Responsib. Comput. 1, 2, Article 14 (June 2024), 25 pages. https://doi.org/10.1145/3652592          21\n---\n                                           Business        Data               AI            Al testing and         Deployment\n  Early in the process                     case        preparation        modelling         evaluation             Integration with\n     Why is Al needed?                                 Collecting data      Dataset         Dataset                complex system\n        What can be                                                       a\n         improved?                                                        0 o\n                                                       Cleaning and       0 Model design    System verification    User uses the\n                                                       labelling data                       and validation         system\n\n  \u2022  Consider if the problem you're solving\n     requires an AI solution\n  \u2022  Decide if AI adds unique value\n\n  \u2022  Balance a people-first approach and a\n     technology-first approach\n\nhttps://pair.withgoogle.com/guidebook-v3/chapters/user-needs-and-defining-success    22\n---\n                            Business        Data               AI            Al testing and         Deployment\n    Early in the process    case        preparation        modelling         evaluation             Integration with\nWhy is Al needed?                       Collecting data      Dataset         Dataset                complex system\n   What can be                                             a\n    improved?                                              0 o\n                                        Cleaning and       0 Model design    System verification    User uses the\n                                        labelling data                       and validation         system\n\n    When AI is probably better                                          When AI is probably not better\n    \u2022     Recommending different content to different users             \u2022  Maintaining predictability\n    \u2022     Prediction of future events                                   \u2022  Providing static or limited information\n    \u2022     Personalization improves the user experience                  \u2022  Minimizing costly errors.\n    \u2022     Natural language understanding.                               \u2022  Complete transparency\n    \u2022     Recognition of an entire class of entities.\n    \u2022     Detection of low occurrence events that change over time.     \u2022  Optimizing for high speed and low cost.\n    \u2022     An agent or bot experience for a particular domain.           \u2022  Automating high-value tasks.\n    \u2022     Showing dynamic content is more efficient than a\n          predictable interface.\n\n  https://pair.withgoogle.com/guidebook-v3/chapters/user-needs-and-defining-success    23\n---\n                                                  THIRD EDITION\nObservation (direct / indirect)                   Designing Interactive\n \u2022     How was this work done before AI?         Systems\n                                                 A comprehensive guide to HCI, UX and interaction design\nInterviews\n \u2022     What are people\u2019s mental models about the\n       models/processes under consideration?\n\nQuestionnaires\n\n(Digital) ethnography\n \u2022     What perceptions do people have of similar\n       products?\n \u2022     How was this work done before AI?          David Benyon\n                                                 WTELE         PEARSON\n                                                 Chapter 7\n---\n   2. Al suitability\n\n    domain    or input\n\n    25\n  https://pair.withgoogle.com/guidebook-v3/chapters/user-needs-and-defining-success\n\n  Google\nGoogle    People+AI\n---\n Data Preparation\n\n    \u2022  design new interfaces for lay users to\n       interact, understand, and manipulate\n       datasets and models\n\nBusiness    Data             AI    Al testing and    Deployment\n  case    preparation    modelling    evaluation\n\n    Why is Al needed?    Collecting data      Dataset    Dataset    Integration with\n                                                                    complex system\nWhat can be                                 II\n improved?                                  0 -                     Bo\n Cleaning and                               0 Model design    System verification    User uses the\nlabelling data                                                and validation         system\n\n  https://pair.withgoogle.com/guidebook-v3/chapters/user-needs-and-defining-success    26\n---\nData Preparation\n\n                                                                                                              56    IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 26, NO. 1, JANUARY 2020\n\n                                                                                                              The What-If Tool: Interactive Probing of Machine Learning Models\n   Use data that applies to different groups of users                                                               James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Vi\u00e9gas, and Jimbo Wilson\n\n   Your training data should reflect the diversity and cultural context of the people who will use it. Use          2\n   tools like Facets and WlT to explore your dataset and better understand its biases. In doing so, note that\n   to properly train your model, you might need to collect data from equal proportions of different user\n   groups that might not exist in equal proportions in the real world. For example, to have speech\n   recognition software work equally on all users in the United States, the training dataset might need to\n   contain 50% of data from non-native English speakers even if they are a minority of the population.\n\n        (a) Confusion matrix of a single binary classifi-    (b) Histogram of age, colored by classification    (c) Two-dimensional histogram of age and sex,\n        cation model, colored by prediction correctness                                                         colored by classification\n                                                             01- 51-65  10-13-- 1-22\n                                                             89         *8  * *\n        Female\n   2                                                                                38-59\n\n   Male\n\n                             80\n\n   ( Small multiples by sex. Each scatterplot          (e)Histograms of performance in a regression    (f) Using images as thumbnails for image\n   shows age vs positive classification score, col-    model that predicts age, faceted into 3 age buck-    datasets\n   ored by classification                              ets\n\n   https://pair-code.github.io/what-if-tool/    27\n---\n  Model building and evaluation\n\n    Allow users to shape the algorithms or e.g. develop\n    interactive machine learning approaches that can take lay\n    user feedback into account\n\nBusiness    Data             AI    Al testing and    Deployment\n  case    preparation    modelling    evaluation\n\n    Why is Al needed?    Collecting data    Dataset    Dataset    Integration with\n                                                                  complex system\n    What can be    II\n    improved?      0                        -                     Bo\n Cleaning and      0                        Model design    System verification    User uses the\nlabelling data                                              and validation         system\n\n    28\n---\n                                                                                                                                                                                                                                                            Why Hockey?\nModel building and evaluation                                                                                                                                                                                                                                         Part 1: Important words\n                                                                                                                                                                                                                                                      This message has more important words about Hockey than\n                                                                                                                                                                                                                                                             about Baseball\n\n                                                                                                                                                                                                                                                      baseball hockey stanley\nInclude Explanations to participants + receive their feedback                                                                                                                                                                                                                                                 , tiger\n\n                                                                                                                                                                                                                                                      The difference makes the computer think this message is 2.3 times more\n                                                                                                                                                                                                                                                      likely to be about Hockey than Baseball.\n\n I                                                                                                            Message Predictor 1.0.5.28868                                                                                                                 AND\n\n                                                                   Move message           Only show predictions                  OFF           SearchStanley         Clear\n                                                                   to folder...                         that just changed\n\n Folders                      Messages in the 'Unknown' folder\n       Unknown                Original                        Subject                    Predicted      Prediction     Re: Octopus in Detroit?                                                            Why Hockey?                                        Part 2: Folder size\n   (1,180 messages)           orderr    Re: Playoff Predictions                            topic       confidence      From: georgeh@gjhsun (George H)\n                              9287                                                         Hockey      99%                                            O                                                                    D\n                              9294      Re: Schedule...                                   Baseball     60%             Harold Zazula <DLMQC@CUNYVM.BITI                                          Part 1: Imp                    ords           The Baseball folder has more messages than the Hockey folder\n                                                                                                                                                                                                           This message has     rtant words\n      cori             hs     9306      Paul Kuryia and Canadian Wor                       Hockey      99%             >I was watching the Detroit-Minnesot:         ht and thought I saw an                     abo Hockey     Baseball\n       Baseball               9308      Re: My Predictions For 1993                       Baseball     64%             >(is there some custom to throw octopus                                   baseball nockey                               Hockey:      7\n              8/8             9312      Re: NHL Team Captains                             Baseball     64%             It is a long standing good luck Redwing's tradition to throw an octopus\n correct predictions          9316      Re: ugliest swing                                 Baseball     63%             onthe eduring a Stanley Cup game They say it dates back '                 stanley tiger\n                              9319      Re: Octopus in Detroit?                            Hockey      67%             at the Olympia when the Wings became the 1st team (I think) to sweep\n      Prediction totals                                                                                                the cup in 8 games. A lot hardet to throw one from Joe Louis seats                                                      Baseball:    8\n      Hockey  278             9339      Sparky Anderson Gets win #2000, Tigers beat A's   Baseball     99%             than from the old Olympia balcony, though.                                The difference makes the computer think this\n                              9347      Re: Goalie masks                                  Baseball     53%             Funniest I ever saw was when some Tiger fans threw one on the field       message is 2.3 times more likely to be about\n      Baseball 917            9362      Re: Young Catchers                                Baseball     82%             during a Detroit/Toronto baseball game .. I was living in California      Hockey than Baseball.\n                                                                                                                       and the folks I was watching with had never heard of hockey and were                                                           The difference makes the computer thinks each Unknown message is 1.1\n Messages containing          9371      Re: Winning Streaks                               Baseball     53%             incredulous when I recognized the octopus BEFORE the camera closeup !!                                                 AND     times more likely to be about Baseball than Hockey.\n       Stanley                9379      Royals                                            Baseball     64%\n Baseball                         9390  Phillies Mailing List?                            Baseball     65%                                                                                                                            Part 2: Folder size\n                                  9410  Reds snap 5-game losing streak: RedReport 4-18    Baseball     98%                                                                                                                 The Baseball folder has more messages than\n  Hockey                      9423      Re: Juggling Dodgers                              Baseball     57%                                                                                                                             the Hockey folder     YIELDS\n                              9424      RCandlestick ark experience on)                   Baseball     99%                                                                                       Hockey:\n      Unknown                 9433      Re: Notes on Jays vs. Indians Series              Baseball     53%                                                                                       Baseball:\n      I                       9434      Re: When did Dodgers move from NY to LA?          Baseball     53%\n                                  9439  Playoff pool                                       Hockey      96%                                                                                       The difference makes the computer thinks each\n                              9441      R: Hockey and the Hispanic community               Hockey      99%                                                                                       Unknown message is 1.1 times more likely to be\n              E 9449 Re: Yoai-isms                                                  Baseball 53%                                 F                                                               about Baseball than Hockey.                                67% probability this message is about Hockey\n                                                                   Important words\n                                                                   These are all of the words the computer used to make its p         more).                                                                                                          Combining 'Important words' and 'Folder size' makes\n                                                                                                                                                                                                                                                      the computer think this message is 2.0 times more likely\n                                                                         20                                                                                                                    Add a new word or phrase                               to be about Hockey than about Baseball.\n                                                                                                                                                                                                        Remove word\n\n                                                                                baseball bill canadian davedavid hockey player players prime stanley stats           tiger  time                 Undo importance adjustment\n\n Figure 1. The EluciDebug prototype. (A) List of folders. (B) List of messages in the selected folder. (C) The selected message.                                                                                                               Figure 2. The Why explanation tells users how features and\n (D) Explanation of the selected message's predicted folder. (E) Overview of which messages contain the selected word. (F) Complete                                                                                                            folder size were used to predict each message's topic. This figure\n list of words the learning system uses to make predictions.                                                                                                                                                                                   is a close-up of Figure 1 part D.\n\nT. Kulesza, M. Burnett, W.-K. Wong, and S. Stumpf, \u2018Principles of Explanatory Debugging to Personalize Interactive Machine Learn ing\u2019, in Proceedings of the 20th International\nConference on Intelligent User Interfaces, Atlanta Georgia USA: ACM, Mar. 2015, pp. 126\u2013\n137. doi: 10.1145/2678025.2701399.\n---\nModel building and evaluation\n\nCo-design/imagine effects with users before deploying\n\n  Leverage participatory or     Consider using participatory approaches from speculative and\n  speculative design            critical design practices to evaluate potential downstream effects\n  methods                       of your Al model. These approaches will provoke discussion\n                                 about the kinds of outcomes you want to enable with your Al\n                                 model, and help you identify risks and behaviors that you want to\n                                 avoid or mitigate. Use these to prioritize short-term UX\n                                 interventions while planning long-term strategies across your\n                                 product's lifecycle.\n\n Q&A: Participatory Machine Learning\n https://medium.com/people-ai-research/participatory-machine-learning-69b77f1e5e23    30\n---\nBusiness    Data    AI    Al testing and    Deployment\n  case    preparation    modelling    evaluation\n\n    Why is Al needed?    Collecting data      Dataset    Dataset    Integration with\nWhat can be                                                         complex system\n improved?                                  0 I\n                         Cleaning and       0 Model design    System verification    User uses the\n                         labelling data                       and validation         system\n\n    Fig. 8. Al system development process.\n\n   Beatrice Vincenzi, Simone Stumpf, Alex S. Taylor, and Yuri Nakao. 2024. Lay User Involvement in Developing Human -centric Responsible\n   AI Systems: When and How? ACM J. Responsib. Comput. 1, 2, Article 14 (June 2024), 25 pages. https://doi.org/10.1145/3652592          31\n---\nParticipation in the Age of Foundation Models\n\n  \u2022  Reinforcement learning with human feedback (RLHF).\n  \u2022  Rulesets and policies\n  \u2022  Red teaming\n  \u2022  Domain-oriented efforts\n\n  H. Suresh, E. Tseng, M. Young, M. Gray, E. Pierson, and K. Levy, \u2018Participation in the age of foundation models\u2019, in The 2024 ACM Conference    32\n  on Fairness Accountability and Transparency, Rio de Janeiro Brazil: ACM, June 2024, pp. 1609\u20131621. doi: 10.1145/3630106.3658992.\n---\n  Participation in the Age of Foundation Models\n\n    RLHF\n    (3.2.1)            e.g., crowd-worker annotations to\n                       improve output quality\n\n    GENERAL RULESETS/\n    GUIDELINES\n    (3.2.2)            e.g., one-time deliberation session with\n                       public stakeholders produce general\n                       guidelines for model behavior\n\n    RED-TEAMING               \u2022\n    (3.2.3)           e.g., adversarial testing to find technical\n\n                      model vulnerabilities to document\n\nDOMAIN-ORIENTED\n    EFFORTS                                                              e.g., stakeholders create\n    (3.2.4)          e.g., interviews with marginalized stakeholders     collaborative licensed dataset and\n                     produce implications for data curation              receive payment for usage\n\n                     CONSULT   INCLUDE                                  COLLABORATE  OWN\n\n    H. Suresh, E. Tseng, M. Young, M. Gray, E. Pierson, and K. Levy, \u2018Participation in the age of foundation models\u2019, in The 2024 ACM Conference    33\n    on Fairness Accountability and Transparency, Rio de Janeiro Brazil: ACM, June 2024, pp. 1609\u20131621. doi: 10.1145/3630106.3658992.\n---\n6CCSAHAI\n\n  Human-centred AI\n  as a Design Process\n  Part 1: Understanding user needs\n  & Defining the right problem\n  Dr Georgia Panagiotidou\n\n  34\n\n",
        "quiz": [
            {
                "question_text": "What is the primary focus of Human-Centred Design?",
                "answers": [
                    {
                        "text": "Discovering requirements, designing to fulfil requirements, producing prototypes, and evaluating them",
                        "is_correct": true,
                        "explanation": "The content explicitly states that Human-Centred Design is focused on discovering requirements, designing to fulfil requirements, producing prototypes, and evaluating them."
                    },
                    {
                        "text": "Creating visually appealing designs without user input",
                        "is_correct": false,
                        "explanation": "The content emphasizes user involvement and balancing conflicting needs, not just creating visually appealing designs without user input."
                    },
                    {
                        "text": "Focusing solely on technological advancements",
                        "is_correct": false,
                        "explanation": "The content highlights that Human-Centred Design is traditionally focused on users and their goals, not just technological advancements."
                    },
                    {
                        "text": "Ignoring user feedback after the initial design phase",
                        "is_correct": false,
                        "explanation": "The content mentions the importance of iterating and evaluating prototypes, which involves continuous user feedback."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "Which of the following is a basic activity of the design process?",
                "answers": [
                    {
                        "text": "Understanding user needs",
                        "is_correct": true,
                        "explanation": "The content explicitly mentions 'Understanding user needs' as one of the basic activities of the design process."
                    },
                    {
                        "text": "Generating financial reports",
                        "is_correct": false,
                        "explanation": "This is not mentioned in the content as a basic activity of the design process."
                    },
                    {
                        "text": "Conducting market research",
                        "is_correct": false,
                        "explanation": "While market research is important, it is not explicitly listed as a basic activity of the design process in the provided content."
                    },
                    {
                        "text": "Developing software code",
                        "is_correct": false,
                        "explanation": "The content focuses on design principles and user-centered activities, not technical implementation like software coding."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is one of the design principles mentioned in the content?",
                "answers": [
                    {
                        "text": "Be People Centered",
                        "is_correct": true,
                        "explanation": "This design principle is explicitly mentioned in the content context under the section 'DESIGN PRINCIPLES'."
                    },
                    {
                        "text": "Focus on Technology",
                        "is_correct": false,
                        "explanation": "The content emphasizes human-centered design, not technology-focused design."
                    },
                    {
                        "text": "Ignore User Feedback",
                        "is_correct": false,
                        "explanation": "The content stresses the importance of user involvement and feedback in the design process."
                    },
                    {
                        "text": "Avoid Prototyping",
                        "is_correct": false,
                        "explanation": "The content highlights the importance of prototyping and iteration in the design process."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What does the 'EMPATHIZE' phase in the Stanford d.school Design Thinking Process involve?",
                "answers": [
                    {
                        "text": "Understanding user needs and conducting interviews to seek to understand their perspectives",
                        "is_correct": true,
                        "explanation": "The 'EMPATHIZE' phase involves understanding user needs and conducting interviews to seek to understand their perspectives, as described in the Stanford d.school Design Thinking Process."
                    },
                    {
                        "text": "Generating multiple design solutions to a problem",
                        "is_correct": false,
                        "explanation": "Generating multiple design solutions is part of the 'IDEATE' phase, not the 'EMPATHIZE' phase."
                    },
                    {
                        "text": "Building prototypes to test design solutions",
                        "is_correct": false,
                        "explanation": "Building prototypes to test design solutions is part of the 'PROTOTYPE' phase, not the 'EMPATHIZE' phase."
                    },
                    {
                        "text": "Defining the problem and creating personas",
                        "is_correct": false,
                        "explanation": "Defining the problem and creating personas is part of the 'DEFINE' phase, not the 'EMPATHIZE' phase."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the purpose of the 'IDEATE' phase in the Stanford d.school Design Thinking Process?",
                "answers": [
                    {
                        "text": "To turn insights into tangible ideas and create prototypes",
                        "is_correct": true,
                        "explanation": "The IDEATE phase involves turning insights into tangible ideas and creating prototypes, as described in the Stanford d.school Design Thinking Process."
                    },
                    {
                        "text": "To conduct user interviews and gather requirements",
                        "is_correct": false,
                        "explanation": "Conducting user interviews and gathering requirements is part of the EMPATHIZE phase, not the IDEATE phase."
                    },
                    {
                        "text": "To define the problem and understand user needs",
                        "is_correct": false,
                        "explanation": "Defining the problem and understanding user needs is part of the DEFINE phase, not the IDEATE phase."
                    },
                    {
                        "text": "To test prototypes and validate solutions",
                        "is_correct": false,
                        "explanation": "Testing prototypes and validating solutions is part of the PROTOTYPE phase, not the IDEATE phase."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the four basic activities of the design process?",
                "answers": [
                    {
                        "text": "Understanding user needs, defining the right problem, generating alternative designs, and choosing among alternative designs",
                        "is_correct": true,
                        "explanation": "The concept description explicitly lists these as the four basic activities of the design process."
                    },
                    {
                        "text": "Planning, executing, monitoring, and evaluating",
                        "is_correct": false,
                        "explanation": "These are general project management activities, not specifically mentioned in the design process context."
                    },
                    {
                        "text": "Research, design, development, and testing",
                        "is_correct": false,
                        "explanation": "While these are common phases in product development, they are not the four basic activities of the design process as described."
                    },
                    {
                        "text": "Empathizing, defining, ideating, and prototyping",
                        "is_correct": false,
                        "explanation": "These are stages in the Stanford d.school Design Thinking Process, not the four basic activities of the design process as described."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the primary goal of the 'PROTOTYPE' phase in the Stanford d.school Design Thinking Process?",
                "answers": [
                    {
                        "text": "To create tangible representations of ideas to test and refine",
                        "is_correct": true,
                        "explanation": "The 'PROTOTYPE' phase in the Stanford d.school Design Thinking Process focuses on creating prototypes to test and refine ideas."
                    },
                    {
                        "text": "To conduct user interviews and gather requirements",
                        "is_correct": false,
                        "explanation": "This activity is typically part of the 'EMPATHIZE' phase, not the 'PROTOTYPE' phase."
                    },
                    {
                        "text": "To define the problem and user needs",
                        "is_correct": false,
                        "explanation": "This is the goal of the 'DEFINE' phase, not the 'PROTOTYPE' phase."
                    },
                    {
                        "text": "To generate a wide range of potential solutions",
                        "is_correct": false,
                        "explanation": "This is the goal of the 'IDEATE' phase, not the 'PROTOTYPE' phase."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the purpose of the 'DEFINE' phase in the Stanford d.school Design Thinking Process?",
                "answers": [
                    {
                        "text": "To define the problem and understand user needs",
                        "is_correct": true,
                        "explanation": "The 'DEFINE' phase in the Stanford d.school Design Thinking Process is explicitly described as focusing on understanding user needs and defining the right problem."
                    },
                    {
                        "text": "To create the final product design",
                        "is_correct": false,
                        "explanation": "The 'DEFINE' phase is about understanding needs and defining the problem, not creating the final product design."
                    },
                    {
                        "text": "To conduct user interviews and shadowing",
                        "is_correct": false,
                        "explanation": "Conducting user interviews and shadowing are part of the 'EMPATHIZE' phase, not the 'DEFINE' phase."
                    },
                    {
                        "text": "To develop prototypes and test them",
                        "is_correct": false,
                        "explanation": "Developing prototypes and testing them are part of the 'PROTOTYPE' phase, not the 'DEFINE' phase."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the four design principles mentioned in the content?",
                "answers": [
                    {
                        "text": "Be People Centered, Communicate (Visually & Inclusively), Collaborate & Co-Create, Iterate, Iterate, Iterate",
                        "is_correct": true,
                        "explanation": "These are the four design principles explicitly mentioned in the content."
                    },
                    {
                        "text": "Empathize, Define, Ideate, Prototype, Test",
                        "is_correct": false,
                        "explanation": "These are stages of the Stanford d.school Design Thinking Process, not the design principles mentioned."
                    },
                    {
                        "text": "Understand, Define, Sketch, Decide, Prototype, Validate",
                        "is_correct": false,
                        "explanation": "These are phases of the Design Sprints methodology, not the design principles mentioned."
                    },
                    {
                        "text": "Explore, Shape, Build, Lead",
                        "is_correct": false,
                        "explanation": "These are activities related to creating conditions for innovation, not the design principles mentioned."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the purpose of the 'UNDERSTAND' phase in the Design Sprint methodology?",
                "answers": [
                    {
                        "text": "To understand user needs and define the right problem",
                        "is_correct": true,
                        "explanation": "The 'UNDERSTAND' phase in the Design Sprint methodology is explicitly described as focusing on understanding user needs and defining the right problem."
                    },
                    {
                        "text": "To create detailed design prototypes",
                        "is_correct": false,
                        "explanation": "Creating detailed design prototypes is part of the 'PROTOTYPE' phase, not the 'UNDERSTAND' phase."
                    },
                    {
                        "text": "To decide on the final design solution",
                        "is_correct": false,
                        "explanation": "Deciding on the final design solution occurs in the 'DECIDE' phase, not the 'UNDERSTAND' phase."
                    },
                    {
                        "text": "To validate the design with users",
                        "is_correct": false,
                        "explanation": "Validating the design with users is part of the 'VALIDATE' phase, not the 'UNDERSTAND' phase."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    },
    "data/raw/Ngram-LMs.pdf": {
        "metadata": {
            "file_name": "Ngram-LMs.pdf",
            "file_type": "pdf",
            "content_length": 63263,
            "language": "en",
            "extraction_timestamp": "2025-11-26T03:24:28.082010+00:00",
            "timezone": "utc"
        },
        "content": "N-gram    Introduction to N-gram\nLanguage  Language Models\nModeling\n---\nPredicting words\n\nThe water of Walden Pond is beautifully ...\n\nblue            *refrigerator\ngreen           *that\nclear\n---\nLanguage Models\n\nA language model is a machine learning model\nthat predicts upcoming words.\n \u2022 More formally:\n     \u2022   An LM assigns a probability to each potential next word\n       \u2022 = gives a probability distribution over possible next words\n     \u2022   An LM assigns a probability to a whole sentence\n Two paradigms\n \u2022  N-gram language models (this lecture)\n \u2022  Large language models (LLMs, neural models, future lectures)\n---\nWhy word prediction?\n\nIt's a helpful part of language tasks\n\u2022 Grammar or spell checking\n Their are two midterms  Their There are two midterms\n Everything has improve  Everything has improve improved\n\n\u2022 Speech recognition\n I will be back soonish  I will be bassoon dish\n---\nWhy word prediction?\n\nIt's how large language models (LLMs) work!\nLLMs are trained to predict words\n\u2022 Left-to-right (autoregressive) LMs learn to predict next word\nLLMs generate text by predicting words\n\u2022 By predicting the next word over and over again\n---\n Language Modeling (LM) more formally\n\n Goal: compute the probability of a sentence or\n sequence of words W:\n P(W) = P(w\u2081,w\u2082,w\u2083,w\u2084,w\u2085\u2026w\u2099)\n Related task: probability of an upcoming word:\n P(w\u2085|w\u2081,w\u2082,w\u2083,w\u2084) or P(w\u2099|w\u2081,w\u2082\u2026w\u2099\u208b\u2081)\n An LM computes either of these:\n P(W)  or  P(w\u2099|w\u2081,w\u2082\u2026w\u2099\u208b\u2081)\n\n*Note: let's call these words for now, but for LLMs we instead use tokens\n---\n    P(blue|The water of Walden Pond is so beautifully)  (3.1)\n e way to estimate this probability is directly from relative frequency counts: take a\n    How to estimate these probabilities\n e way to estimate this probability is directly from relative frequency counts: take a\nry large corpus, count the number of times we see The water of Walden Pond\nry large corpus, count the number of times we see The water of Walden Pond\nso beautifully , and count the number of times this is followed by blue. This\nso beautifully , and count the number of times this is followed by blue. This\n uld be answering the question \u201cOut of the times we saw the history h, how many\n    Could we just count and divide?\n uld be answering the question \u201cOut of the times we saw the history h, how many\n es was it followed by the word w\u201d, as follows:\n es was it followed by the word w\u201d, as follows:\n    P(blue|The water of Walden Pond is so beautifully) ==\n    P(blue|The water of Walden Pond is so beautifully) =\n    C(The water of Walden Pond is so beautifully blue)\n    C(The water of Walden Pond is so beautifully blue)   (3.2\n    C(The water of Walden Pond is so beautifully)        (3.2)\n e had a C(The water of Walden Pond is so beautifully)\n e had a large enough corpus, we could compute these two counts and estimate\n    large enough corpus, we could compute these two counts and estimate\n probability from Eq. 3.2. But even the entire web isn\u2019t big enough to give us\n    No! Too many possible sentences!\n probability from Eq. 3.2. But even the entire web isn\u2019t big enough to give us\nod estimates for counts of entire sentences. This is because language is creative\n    We\u2019ll never see enough data for estimating these\nod estimates for counts of entire sentences. This is because language is creative;\nw sentences are invented all the time, and we can\u2019t expect to get accurate count\nw sentences are invented all the time, and we can\u2019t expect to get accurate counts\n such large objects as entire sentences. For this reason, we\u2019ll need more clever\n such large objects as entire sentences. For this reason, we\u2019ll need more clever\n---\nHow to compute P(W) or P(w\u2099|w\u2081, \u2026w\u2099\u208b\u2081)\nwith an N-gram Language Model\n\nFirst let's do the joint probability P(W):\n\nP(The, water, of, Walden, Pond, is, so, beautifully, blue)\n\nIntuition: let\u2019s rely on the Chain Rule of Probability\n---\nReminder: The Chain Rule\n\nRecall the definition of conditional probabilities\nP(B|A) = P(A,B)/P(A)  Rewriting: P(A,B) = P(A) P(B|A)\n\nMore variables:\nP(A,B,C,D) = P(A) P(B|A) P(C|A,B) P(D|A,B,C)\nThe Chain Rule in General\nP(x\u2081,x\u2082,x\u2083,\u2026,x\u2099) = P(x\u2081)P(x\u2082|x\u2081)P(x\u2083|x\u2081,x\u2082)\u2026P(x\u2099|x\u2081,\u2026,x\u2099\u208b\u2081)\n---\n       The Chain Rule applied to compute   3.1  \u2022  N-G RAM\n                                           joint\n       probability of words in sentence\nlying the chain rule to words, we get\n       P(w1:n ) = P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P(wn |w1:n\u22121 )\n                  n\n       = Y P(wk |w1:k\u22121 )\n                  k=1\n hain rule shows the link between computing the joint probability of a seq\n       P(\u201cThe water of Walden Pond\u201d) =\n omputing the conditional probability of a word given previous words.\n3.4    P(The) \u00d7 P(water|The) \u00d7 P(of|The water)\n     suggests that we could estimate the joint probability of an entire seque\n s     \u00d7 P(Walden|The water of) \u00d7    P(Pond|The water of Walden)\n       by multiplying together a number of conditional probabilities. But us\nn rule doesn\u2019t really seem to help us! We don\u2019t know any way to comp\n---\ntuition of the n-gram model is that instead of computing the proba\n   wn |wn\u22121 ). In other words, instead of computing the proba\n iven its entire history, we can approximate the history by just t\nm       Markov Assumption\n.  model, for example, approximates the probability of a word gi\ne water of Walden Pond is so beautifully)\n words  P(wn |w1:n\u22121 ) by using only the conditional probability of\n e bigram model, for example, approximates the probability of a\nd P(wSimplifying assumption:\n        |w  ). In other words, instead of computing the probabili\n ith the probability\n        n  n\u22121\n   revious words P(wn |w1:n\u22121 ) by using only the conditional probab\n ing word P(w |w  ). In other words, instead of computing the pr\ne|The water of Walden            A. A. Ma\u03c1Ron (1886).\n              n  n\u22121      Pond is so beautifully)    (\n   P(blue     P(blue|beautifully)  Andrei Markov\nte it      |The water of Walden Pond is so beautifully)\n        with the probability\n m model to predict the conditional probability of the next w\n roximate it with the probability\nhe          \u2248    P(blue|beautifully)                 (\n     following approximation:\n   igram model to     P(blue|beautifully)\n            P(w predict the conditional probability of the next word\n   ing the       n |w1:n\u22121 ) \u2248 P(wn |wn\u22121 )\nwe use following approximation:\n        a bigram model to predict the conditional probability of the\n                                           Wikimedia commons\n---\na complete word sequence by substituting Eq.  3.\n    Bigram Markov Assumption\n    n\n    P(w ) \u2248 Y P(w |w        )\n g the chain rule to words, we get\n        1:n    k      k \u22121\n    P(w ) =    k =1\n    1:n        P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P\n    instead of:    n\n                   = Y P(wk |w1:k\u22121 )\n                   k=1\nin rule shows the link between computing the join\n    We approximate each component in the product\n---\n next word in a sequence. We\u2019ll use      N here to\n    General N-gram model for each component\n eans bigrams and N = 3 means trigrams. Then w\n ord given its entire context as follows:\n    P(wn |w1:n\u22121 ) \u2248 P(wn |wn\u2212N +1:n\u22121 )\n assumption for the probability of an individual wo\nity of a complete word sequence by substituting Eq\n   n\n   Y\n---\n Bigram model\n     P ( w i | w\u2081w 2 \u2026 w i \u2212\u2081 ) \u2248 P ( w i | w i \u2212\u2081 )\n\nSome automatically generated sentences from Shakespeare bigram models\n Why dost stand forth thy canopy, forsooth; he                       is\n this palpable hit the King Henry. Live king.\n Follow.\n\n What means, sir. I confess she? then all sorts,                       he\n is  trim, captain.\n---\n  Even simpler Markov assumption: Unigram model\n\n              P ( w\u2081w 2 \u2026 w n ) \u2248 \u220f P ( w i )\n                                 i\n\nSome automatically generated sentences from Shakespeare unigram models\n  To  him swallowed confess   hear both . Which .\n  Of  save on trail for are   ay device and  rote\n  \u20ac\n  life have\n\n  Hill he late speaks   ; or  !  a more to leg less\n  first you   enter\n---\n More complex Markov assumption: Trigram model\n\n \ud835\udc43 \ud835\udc64! \ud835\udc64\" \ud835\udc64# \u2026 \ud835\udc64! $\"  \u2248 \ud835\udc43 \ud835\udc64! \ud835\udc64! $# \ud835\udc64! $\"\n\nSome automatically generated sentences from Shakespeare trigram models\n Fly, and will rid me these news of price.\n Therefore the sadness of parting,  as they say,\n \u2019tis done.\n\n This shall forbid it  should be branded, if    renown\n made it  empty\n---\n             of modeling the training corpus as we increase the value of N .\n             We can use the sampling method from the prior section to visualize both of\n4 different N-gram models\n             these facts! To give an intuition for the increasing power of higher-order n-grams,\n             Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-\n             gram models trained on Shakespeare\u2019s works.\n\n 1          \u2013To him swallowed confess hear both. Which. Of save on trail for are ay device and\n            rote life have\n gram       \u2013Hill he late speaks; or! a more to leg less first you enter\n 2          \u2013Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\n            king. Follow.\n gram       \u2013What means, sir. I confess she? then all sorts, he is trim, captain.\n 3          \u2013Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n            \u2019tis done.\n gram       \u2013This shall forbid it should be branded, if renown made it empty.\n 4          \u2013King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\n            great banquet serv\u2019d in;\n gram       \u2013It cannot be but so.\nFigure 3.4  Eight sentences randomly generated from four n-grams computed from Shakespeare\u2019s works. All\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\nfor capitalization to improve readability.\n---\nProblems with N-gram models\n1.         N-grams can't handle long-distance dependencies:\n\u201cThe soups that I made from that new cookbook I bought yesterday\n      were amazingly delicious.\"\n2.         N-grams don't do well at modeling new sequences (even\n           if they have similar meanings to sequences they've seen)\nThe solution: Large language models\n      \u2022     can handle much longer contexts\n      \u2022     because they use neural embedding spaces, can model\n            meaning better\n---\nWhy study N-gram models?\n\nThey are the \"fruit fly of NLP\", a simple model that\nintroduces important topics for large language models\n \u2022     training and test sets\n \u2022     the perplexity metric\n \u2022     sampling to generate sentences\n \u2022     ideas like interpolation and backoff\n N-grams are efficient and fast and useful in situations\n where LLMs are too heavyweight\n---\nN-gram    Introduction to N-gram\nLanguage  Language Models\nModeling\n---\nN-gram    Estimating N-gram\nLanguage  Probabilities\nModeling\n---\n etween 0 and 1.\n he bigrams that share the same first word  wn\u22121 :\nto compute a particular bigram probability of a word  w\n    Estimating bigram probabilities                   n\n    , we\u2019ll compute the   C(w  w )\n                          count of the bigram C(w  w ) and\nn\u22121  P(wn |wn\u22121 ) = P     n\u22121  n            n\u22121    n\nl the bigrams that share the same first word  w    :\n     The Maximum          C(wn\u22121 w)                n\u22121\n                   Likelihood Estimate\n                          w\ny this equation, since the sum of all bigram counts that star\n                          C(wn\u22121 wn )\n must be  P(w |w   ) = P\n          equal to the unigram count for that word w  (the\n nt to be   n   n\u22121        w C(wn\u22121 w)             n\u22121\nify this    convinced of this):\n          equation, since the sum of all bigram counts that st\n   must be equal to the   C(w  w )\n                     unigram count for that word w    (th\n 1          P(wn |wn\u22121 ) =     n\u22121 n                  n\u22121\n ent to be convinced of this):\n                            C(wn\u22121 )\n ugh an example using a mini-corpus of three sentences.\n                               C(wn\u22121 wn )\n---\n  An example\n\n   ~~ I am Sam ~~      P ( w i | w i\u2212\u2081 ) = c ( w i\u2212\u2081, w i )\n   ~~ Sam I am ~~                          c ( w i\u2212\u2081 )\n   ~~ I do not like green eggs and ham ~~  \n\nP(I| <s>=    \u20ac\n  2          P(Sam <s>)=                      P(am| I) =2\n             =.67    1= .33                               =.67\n             2                                P(do| I) = 1\nP(</s> | Sam) = = 0.5  P(Sam | am) =\u00b9 = .5                = .33\n                       2\n---\nMore examples:\nBerkeley Restaurant Project sentences\n\n can you tell me about any good cantonese restaurants close by\n tell me about chez panisse\n i\u2019m looking for a good place to eat breakfast\n when is caffe venezia open during the day\n---\n        i\u2019m looking for a good place to eat breakfast\n      Raw bigram counts\n        when is caffe venezia open during the day\n          Figure 3.1 shows the bigram counts from part of a bigram grammar from text-\nnormalized Berkeley Restaurant Project sentences.       Note that the majority of the\n      Out of 9222 sentences\nvalues are zero. In fact, we have chosen the sample words to cohere with each other;\na matrix selected from a random set of eight words would be even more sparse.\n\n             i      want     to      eat         chinese     food     lunch     spend\n i           5      827      0       9           0           0        0         2\n want        2      0        608     1           6           6        5         1\n to          2      0        4       686         2           0        6         211\n eat         0      0        2       0           16          2        42        0\n chinese     1      0        0       0           0           82       1         0\n food        15     0        15      0           1           4        0         0\n lunch       2      0        0       0           0           1        0         0\n spend       1      0        1       0           0           0        0         0\nFigure 3.1  Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-\n---\n means that want followed i 827 times in the corpus.\n         Raw bigram probabilities\n         Figure 3.2 shows the bigram probabilities after normalization (dividing each c\n in Fig. 3.1 by the appropriate unigram for its row, taken from the following set\n unigram counts):\n         Normalize by unigrams:\n                         i         want        to           eat  chinese        food  lunch    spend\nC          3       \u2022     2533      927         2417         746  158            1093  341      278\n   HAPTER               N-GRAM L ANGUAGE M ODELS\n         Result:\n Here are a few other useful probabilities:\n                              i           want        to          eat      chinese     food     lunch     spend\n   P(i|<s>) = 0.25                                   P(english|want) = 0.0011\n              i               0.002       0.33        0           0.0036   0           0        0         0.00079\n   P(         want            0.0022      0           0.66        0.0011   0.0065      0.0065   0.0054    0.0011\n         food|english) = 0.5                         P(</s>|food) = 0.68\n              to              0.00083     0           0.0017      0.28     0.00083     0        0.0025    0.087\n         Now we can compute the probability of sentences like I want English food\n I            eat             0           0           0.0027      0        0.021       0.0027   0.056     0\n       want Chinese food by simply multiplying the appropriate bigram probabilities t\n              chinese         0.0063      0           0           0        0           0.52     0.0063    0\n              food            0.014       0           0.014       0        0.00092     0.0037   0         0\n gether, as follows:\n              lunch           0.0059      0           0           0        0           0.0029   0         0\n                        P( ~~ i want english food ~~  )\n              spend           0.0036      0           0.0036      0        0           0        0         0\n---\nBigram estimates of sentence probabilities\n\nP( ~~ I want english food ~~  ) =\nP(I|<s>)\n\u00d7 P(want|I)\n\u00d7 P(english|want)\n\u00d7 P(food|english)\n\u00d7 P(</s>|food)\n= .000031\n---\nWhat kinds of knowledge do N-grams represent?\n\nP(to|want) = .66                  Knowledge of grammar\nP(want | spend) = 0               (\"want\" is followed by infinitive \"to\")\nP(of | to) = 0                    (\"of\" is not a verb)\n\nP (dinner|lunch or) = .83         Knowledge of meaning\nP(dinner|for) ~ P(lunch|for)     The words \"dinner\" or \"lunch\" are\n                                 semantically related.\n\nP(chinese|want) > P(english|want)     Knowledge about the world\n                                      Chinese food is very popular\n---\n.1.3  Dealing with scale in large n-gram models\nn     Dealing with scale in large n-grams\n    practice, language models can be very large, leading to practical issues.\nog probabilities  Language model probabilities are always stored and com\nn     LM probabilities are stored and computed in\n    log format, i.e., as log probabilities. This is because probabilities are (b\n      log format, i.e. log probabilities\nnition) less than or equal to 1, and so the more probabilities we multiply to\nhe smaller the product becomes. Multiplying enough n-grams together would\n      This avoids underflow from multiplying many\nn numerical underflow. Adding in log space is equivalent to multiplying in\npace, small numbers\n      so we combine log probabilities by adding them. By adding log proba\nnstead of multiplying probabilities, we get results that are not as small. We\n      log( p\u2081 \u00d7 p\u2082 \u00d7 p\u2083 \u00d7 p\u2084 ) = log p\u2081 + log p\u2082 + log p\u2083 + log p\u2084\nomputation and storage in log space, and just convert back into probabilitie\need to report probabilities at the end by taking the exp of the logprob:\n      If we need probabilities we can do one exp at the end\n      p1 \u21e5 p2 \u21e5 p3 \u21e5 p4 = exp(log p1 + log p2 + log p3 + log p4 )\n---\nLarger ngrams\n\n4-grams, 5-grams\nLarge datasets of large n-grams have been released\n \u2022  N-grams from Corpus of Contemporary American English (COCA)\n    1 billion words (Davies 2020)\n \u2022  Google Web 5-grams (Franz and Brants 2006) 1 trillion words)\n \u2022  Efficiency: quantize probabilities to 4-8 bits instead of 8-byte float\n Newest model: infini-grams (\u221e-grams) (Liu et al 2024)\n \u2022  No precomputing! Instead, store 5 trillion words of web text in\n    suffix arrays. Can compute n-gram probabilities with any n!\n---\nN-gram LM Toolkits\n\nSRILM\n\u25e6 http://www.speech.sri.com/projects/srilm/\nKenLM\n\u25e6 https://kheafield.com/code/kenlm/\n---\nN-gram    Estimating N-gram\nLanguage  Probabilities\nModeling\n---\n        Evaluation and Perplexity\n\nLanguage\nModeling\n---\nHow to evaluate N-gram models\n\n\"Extrinsic (in-vivo) Evaluation\"\nTo compare models A and B\n1.  Put each model in a real task\n \u2022  Machine Translation, speech recognition, etc.\n2.  Run the task, get a score for A and for B\n \u2022  How many words translated correctly\n \u2022  How many words transcribed correctly\n3.  Compare accuracy for A and B\n---\nIntrinsic (in-vitro) evaluation\n\nExtrinsic evaluation not always possible\n\u2022  Expensive, time-consuming\n\u2022  Doesn't always generalize to other applications\nIntrinsic evaluation: perplexity\n\u2022  Directly measures language model performance at\n   predicting words.\n\u2022  Doesn't necessarily correspond with real application\n   performance\n\u2022  But gives us a single general metric for language models\n\u2022  Useful for large language models (LLMs) as well as n-grams\n---\nTraining sets and test sets\n\nWe train parameters of our model on a training set.\nWe test the model\u2019s performance on data we\nhaven\u2019t seen.\n \u25e6   A test set is an unseen dataset; different from training set.\n   \u25e6 Intuition: we want to measure generalization to unseen data\n \u25e6   An evaluation metric (like perplexity) tells us how well\n     our model does on the test set.\n---\nChoosing training and test sets\n\n\u2022 If we're building an LM for a specific task\n  \u2022  The test set should reflect the task language we\n     want to use the model for\n\u2022 If we're building a general-purpose model\n  \u2022  We'll need lots of different kinds of training\n     data\n  \u2022  We don't want the training set or the test set to\n     be just from one domain or author or language.\n---\nTraining on the test set\n\nWe can\u2019t allow test sentences into the training set\n \u2022  Or else the LM will assign that sentence an artificially\n    high probability when we see it in the test set\n \u2022  And hence assign the whole test set a falsely high\n    probability.\n \u2022  Making the LM look better than it really is\nThis is called \u201cTraining on the test set\u201d\nBad science!\n\n                                                            38\n---\nDev sets\n\n\u2022 If we test on the test set many times we might\nimplicitly tune to its characteristics\n \u2022 Noticing which changes make the model better.\n\u2022 So we run on the test set only once, or a few times\n\u2022 That means we need a third dataset:\n \u2022 A development test set or, devset.\n \u2022 We test our LM on the devset until the very end\n \u2022 And then test our LM on the test set once\n---\n Intuition of perplexity as evaluation metric:\n How good is our language model?\nIntuition: A good LM prefers \"real\" sentences\n\u2022     Assign higher probability to \u201creal\u201d or \u201cfrequently\n      observed\u201d sentences\n\u2022     Assigns lower probability to \u201cword salad\u201d or\n      \u201crarely observed\u201d sentences?\n---\n Intuition of perplexity 2:\n Predicting upcoming words\n                    The Shannon Game: How well can we        time      0.9\n                    predict the next word?                   dream     0.03\n                    \u2022  Once upon a ____                      midnight 0.02\n                    \u2022  That is a picture of a ____           \u2026\n                    \u2022  For breakfast I ate my usual ____\n                                                             and       1e-100\nClaude   233355     Unigrams are terrible at this game (Why?)\n         Shannon\n\nA good LM is one that assigns a higher probability\nto the next word that actually occurs\n                                                         Picture credit: Historiska bildsamlingen\n                                                         https://creativecommons.org/licenses/by/2.0/\n---\nIntuition of perplexity 3: The best language model\nis one that best predicts the entire unseen test set\n \u2022  We said: a good LM is one that assigns a higher\n    probability to the next word that actually occurs.\n \u2022  Let's generalize to all the words!\n     \u2022     The best LM assigns high probability to the entire test\n           set.\n \u2022  When comparing two LMs, A and B\n     \u2022     We compute PA(test set) and PB(test set)\n     \u2022     The better LM will give a higher probability to (=be less\n           surprised by) the test set than the other LM.\n---\nIntuition of perplexity 4: Use perplexity instead of\nraw probability\n\u2022  Probability depends on size of test set\n    \u2022     Probability gets smaller the longer the text\n    \u2022     Better: a metric that is per-word, normalized by length\n\u2022  Perplexity is the inverse probability of the test set,\n   normalized by the number of words\n\n          PP (W )  = P (w w ...w )\u2212 1\n                     1 2         N  N\n\n                   = N P (w 1\n                       1w2 ...w N )\n---\nIntuition of perplexity 5: the inverse\n\nPerplexity is the inverse probability of the test set,\nnormalized by the number of words\n\nPP(W ) = P(w w ... w )\u2212 1\n         1 2         N  N\n\n       = N P(w1w1 ... w )\n              2         N\n(The inverse comes from the original definition of perplexity\nfrom cross-entropy rate in information theory)\nProbability range is [0,1], perplexity range is [1,\u221e]\nMinimizing perplexity is the same as maximizing probability\n---\nIntuition of perplexity 6: N-grams\n\nPP (W )  = P ( w w ...w )\u2212 1\n           1 2          N  N\n\n         = N P ( w1w1 ...w )\n                   2       N\n\n                   N        1\nChain rule:  PP(W)  N II\n             =      i=1P(wi| 1 . . . Wi\u22121)\n\n                    N       1\nBigrams:     PP(W)  N \u2161IP(i|i\u22121\n                    i=1\n---\n   Intuition of perplexity 7:\n   Weighted average branching factor\n\nPerplexity is also the weighted average branching factor of a language.\nBranching factor: number of possible next words that can follow any word\nExample: Deterministic language L = {red,blue, green}\n   Branching factor = 3 (any word can be followed by red, blue, green)\nNow assume LM A where each word follows any other word with equal probability \u2153\nGiven a test set T = \"red red red red blue\"\nPerplexityA(T) = PA(red red red red blue)-1/5 =  ((\u2153)\u2075)-1/5  = (\u2153)\u207b\u00b9  =3\nBut now suppose red was very likely in training set, such that for LM B:\n\u25e6  P(red) = .8 p(green) = .1 p(blue) = .1\nWe would expect the probability to be higher, and hence the perplexity to be smaller:\nPerplexityB(T) = PB(red red red red blue)-1/5\n   = (.8 * .8 * .8 * .8 * .1) -1/5  =.04096 -1/5     = .527\u207b\u00b9  = 1.89\n---\n Holding test set constant:\n Lower perplexity = better language model\n\nTraining 38 million words, test 1.5 million words, WSJ\n\nN-gram     Unigram  Bigram  Trigram\nOrder\nPerplexity 962      170     109\n---\n        Evaluation and Perplexity\n\nLanguage\nModeling\n---\n        Sampling and Generalization\n\nLanguage\nModeling\n---\nThe Shannon (1948) Visualization Method\nSample words from an LM\n\n Unigram:                                          233355\n                                             Claude Shannon\n\n REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME\n CAN DIFFERENT NATURAL HERE HE THE A IN CAME THE TO\n OF TO EXPERT GRAY COME TO FURNISHES THE LINE\n MESSAGE HAD BE THESE.\n\n Bigram:\nTHE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER\nTHAT THE CHARACTER OF THIS POINT IS THEREFORE\nANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHO\nEVER TOLD THE PROBLEM FOR AN  UNEXPECTED.\n---\nHow Shannon sampled those words in 1948\n\n\"Open a book at random and select a letter at random on the page.\nThis letter is recorded. The book is then opened to another page\nand one reads until this letter is encountered. The succeeding\nletter is then recorded. Turning to another page this second letter\nis searched for and the succeeding letter recorded, etc.\"\n---\nSampling a word from a distribution  8  2\n                                     %8 36 99 0\n                                     5230 12984\n                                     291082 64 402223\n                                     11 87 69 5341\n                                     2228899 88 8 g2\n                                        A2 L9G8\n\npolyphonic\n\nthe  of  a  to in    \u2026 however  p=.0000018\n                     (p=.0003)\n\n0.06  0.03  0.02 0.02 0.02\n                          \u2026     \u2026\n\n0     .06  .09 .11 .13 .15      .66       .99 1\n---\n  Visualizing Bigrams the Shannon Way\n\n Choose a random bigram (<s>, w)       <s> I\n  according to its probability p(w|<s>)  I        want\n Now choose a random bigram            (w, x)     want to\naccording to its probability p(x|w)                   to eat\n And so on until we choose </s>                          eat Chinese\n Then string the words together                             Chinese food\n                                                                           food </s>\n                                                 I want to eat Chinese food\n---\nNote: there are other sampling methods\n\nUsed for neural language models\nMany of them avoid generating words from the very\nunlikely tail of the distribution\nWe'll discuss when we get to neural LM decoding:\n \u25e6  Temperature sampling\n \u25e6  Top-k sampling\n \u25e6  Top-p sampling\n---\n             We can use the sampling method from the prior section to visualize both of\n            Approximating Shakespeare\n             these facts! To give an intuition for the increasing power of higher-order n-grams,\n             Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-\n             gram models trained on Shakespeare\u2019s works.\n\n1           \u2013To him swallowed confess hear both. Which. Of save on trail for are ay device and\n            rote life have\ngram        \u2013Hill he late speaks; or! a more to leg less first you enter\n2           \u2013Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\n            king. Follow.\ngram        \u2013What means, sir. I confess she? then all sorts, he is trim, captain.\n3           \u2013Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n            \u2019tis done.\ngram        \u2013This shall forbid it should be branded, if renown made it empty.\n4           \u2013King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\n            great banquet serv\u2019d in;\ngram        \u2013It cannot be but so.\nFigure 3.4  Eight sentences randomly generated from four n-grams computed from Shakespeare\u2019s works. All\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\n---\nShakespeare as corpus\n\nN=884,647 tokens, V=29,066\nShakespeare produced 300,000 bigram types out of\nV\u00b2= 844 million possible bigrams.\n \u25e6  So 99.96% of the possible bigrams were never seen (have\n    zero entries in the table)\n \u25e6  That sparsity is even worse for 4-grams, explaining why\n    our sampling generated actual Shakespeare.\n---\nThe Wall Street Journal is not Shakespeare\n            3.5                        \u2022  G ENERALIZATION AND Z EROS               13\n\n1           Months the my and issue of year foreign new exchange\u2019s september\ngram        were recession exchange new endorsed a acquire to six executives\n2           Last December through the way to preserve the Hudson corporation N.\n            B. E. C. Taylor would seem to complete the major central planners one\ngram        point five percent of U. S. E. has already old M. X. corporation of living\n            on information such as more frequently fishing to keep her\n3           They also point to ninety nine point six billion dollars from two hundred\n            four oh six three percent of the rates of interest stores as Mexico and\ngram        Brazil on market conditions\nFigure 3.5  Three sentences randomly generated from three n-gram models computed from\n40 million words of the Wall Street Journal, lower-casing all characters and treating punctua-\n---\nCan you guess the author? These 3-gram sentences\nare sampled from an LM trained on who?\n 1)  They also   point to ninety nine point\n six  billion dollars from two hundred four\n oh  six  three  percent of the rates of\n interest   stores  as Mexico and gram Brazil\n on  market conditions\n 2)  This shall  forbid it should be branded,\n if  renown made    it empty.\n 3)  \u201cYou are uniformly   charming!\u201d cried he,\n with  a  smile  of associating and  now and\n then  I  bowed  and they perceived  a chaise\n and  four  to wish for.\n                                        58\n---\nChoosing training data\n\nIf task-specific, use a training corpus that has a similar\ngenre to your task.\n\u2022   If legal or medical, need lots of special-purpose documents\nMake sure to cover different kinds of dialects and\nspeaker/authors.\n\u2022   Example: African-American Vernacular English (AAVE)\n  \u2022  One of many varieties that can be used by African Americans and others\n  \u2022  Can include the auxiliary verb finna that marks immediate future tense:\n  \u2022  \"My phone finna die\"\n---\nThe perils of overfitting\n\nN-grams only work well for word prediction if the\ntest corpus looks like the training corpus\n \u2022  But even when we try to pick a good training\n    corpus, the test set will surprise us!\n \u2022  We need to train robust models that generalize!\nOne kind of generalization: Zeros\n   \u2022 Things that don\u2019t ever occur in the training set\n    \u2022 But occur in the test set\n---\nZeros\nTraining set:           \u2022 Test set\n\u2026 ate lunch               \u2026 ate lunch\n\u2026 ate dinner              \u2026 ate breakfast\n\u2026 ate a\n\u2026 ate the\nP(\u201cbreakfast\u201d | ate) = 0\n---\nZero probability bigrams\n\nBigrams with zero probability\n \u25e6  Will hurt our performance for texts where those words\n    appear!\n \u25e6  And mean that we will assign 0 probability to the test set!\nAnd hence we cannot compute perplexity (can\u2019t\ndivide by 0)!\n---\n        Sampling and Generalization\n\nLanguage\nModeling\n---\nN-gram    Smoothing, Interpolation,\nLanguage  and Backoff\nModeling\n---\nThe problem of sparsity\n\nMaximum likelihood estimation has a problem\n\u2022 Sparsity! Our finite training corpus won't have\nsome perfectly fine sequences\n\u2022  Perhaps it has \"ruby\" and \"slippers\"\n\u2022  But happens not to have \"ruby slippers\"\n---\nThis sparsity can take many forms\n\nVerbs have direct objects; those can be sparse too!\n\nCount(w | denied the)\n\nCounts:              allegations\n 3 allegations       reports     outcome\n 2 reports                      requestattack    \u2026\n 1 claims            claims      man\n 1 request\n 0 attack\n 0 outcome\n 7 total\n---\nThe intuition of smoothing\n                               (Example modified from Dan Klein!)\nWhen we have sparse statistics:\nCount(w | denied the)          allegations\n3 allegations\n2 reports                      reports     outcome\n1 claims                                   attack  \u2026\n                                           request\n1 request                                  claims    man\n7 total\nSteal probability mass to generalize better\nCount(w | denied the)\n 2.5 allegations\n 1.5 reports                   allegations              outcome\n 0.5 claims                                          attack\n 0.5 request                   reports               man   \u2026\n 2 other                                   claims request\n 7 total\n---\n  lunch     3  1                 1               1                    1             2         1\n  spend     2  1            2          1              1                    1             1\n  spend     2  1            2          1              1                    1             1\n  Add-one estimation\n  spend        2  1              2               1                    1             1         1\nFigure 3.6  Add-one smoothed bigram counts for eight of the words (out of V\nFigure 3.6  Add-one smoothed bigram counts for eight of the words (out of V\n  Figure 3.6  Add-one smoothed bigram counts for eight of the words (out\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero count\n  the Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero co\n \u2022 Also called Laplace smoothing\n  Figure 3.7 shows the add-one smoothed probabilities for the bigrams\n  Figure 3.7 shows the add-one smoothed probabilities for the bigrams\n \u2022 Pretend we saw each word one more time than we did\nRecall that normal bigram probabilities are computed by normalizing e\nRecall that normal bigram probabilities are computed by normalizing e\n  Figure 3.7 shows the add-one smoothed probabilities for the bigra\ncounts by the unigram count:\ncounts by the unigram count:\n  Recall that normal bigram probabilities are computed by normalizin\n \u2022 Just add one to all the counts!\n  counts by the unigram count:                        C (w                 w )\n                                                      C (w                 w )\n                                                                           n\n                                                            n\u22121            n\n                     P(w |w            ) =                  n\u22121\n                     P(w |w             ) =\n                               n\n                               n       n\u22121\n                                       n\u22121            C (w                 )\n                                                      C (w                 )\n  MLE estimate:                                             C (w              w )\n                                                            n\u22121\n                     PMLE (wn |wn\u22121 ) =                               n\u22121 n\u22121 n\nFor add-one smoothed bigram counts,                                   C (w    )\nFor add-one smoothed bigram                 we need to augment the unigram co\n                               counts, we need to augment the unigram c\n                                                                            n\u22121\nnumber of total word types in the vocabulary V :\nnumber of total word types in the vocabulary V :\n  For add-one smoothed bigram counts, we need to augment the unigra\n  Add-1 estimate:\n  number of total word types in the vocabulary V :\n                                      C (w            w ) + 1                 C (w       w ) + 1\n                                       C (w           w ) + 1                 C (w       w ) + 1\n                                                      n\n                                                      n                                  n\n                                            n                                            n\n                                             n\n                                             \u2212\n                                                 \u2212\n                                                 1\n                                                      1                          n\n                                                                                    n\n                                                                                    \u2212\n                                                                                     \u2212\n                                                                                         1\n                                  P                                                      1\n         P        (w |w  ) =      P                                         =\n         P         (w |w  ) =                                               =\n            Laplace\n            Laplace  n\n                     n  n\n                          n\n                          \u2212\n                          \u2212\n                           1\n                           1           (C (w               w) + 1)             C (w       ) + V\n                                       (C (w               w) + 1)             C (w       ) + V\n                                                 n\n                                                      n\n                                                      \u2212\n                                                           \u2212\n                                                           1\n                                                            1                        n\n                                                                                     n\n                                                                                         \u2212\n                                                                                         \u2212\n                                                                                          1\n                                       w C (w               w ) + 1            C (w 1      w ) +\n                                       w         n\u22121             n                        n\u22121 n\n---\nMaximum Likelihood Estimates\n\nThe maximum likelihood estimate\n \u25e6     of some parameter of a model M from a training set T\n \u25e6     maximizes the likelihood of the training set T given the model M\nSuppose the word \u201cbagel\u201d occurs 400 times in a corpus of a million words\nWhat is the probability that a random word from some other text will be\n\u201cbagel\u201d?\nMLE estimate is 400/1,000,000 = .0004\nThis may be a bad estimate for some other corpus\n \u25e6     But it is the estimate that makes it most likely that \u201cbagel\u201d will occur 400 times\n       in a million word corpus.\n---\n                                                   c\u2217\n                                           d =     i\n ts,      Berkeley Restaurant Corpus: Laplace\n     we need to augment the unigram count by the\n          smoothed                           i     ci\n abulary             V : bigram counts\n   Now that we have the intuition for the unigram case, let\u2019s smooth our Berkeley\n   Restaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the\n  C(w          w ) + 1              C(w            w ) + 1\n   bigrams in Fig. 3.1.\n     (Cn\u22121     n                 =         n\u22121       n                        (3.24)\n  w         (wn\u22121iw) + 1)                 C(wn\u22121 ) + V\n                         want       to      eat           chinese     food     lunch     spend\n     i          6        828        1       10            1           1        1         3\n en in the previous section will need to be aug-\n     want       3        1          609     2             7           7        6         2\nthe smoothed bigram probabilities in Fig.                                   3.7.\n     to         3        1          5       687           3           1        7         212\n     eat        1        1          3       1             17          3        43        1\n uct the count matrix so we can see how much a\nhe chinese      2        1          1       1             1           83       2         1\n     original counts. These adjusted counts can be\n     food       16       1          16      1             2           5        1         1\n  shows the reconstructed counts.\n     lunch      3        1          1       1             1           2        1         1\n     spend      2        1          2       1             1           1        1         1\n  [C(w         w ) + 1] \u00d7 C(w                )\n   Figure 3.6  Add-one smoothed bigram counts for eight of the words (out of V = 1446) in\n            n\u22121 n                         n\u22121\n---\n                               P(w |w                   ) =\n                               P(w |w                    ) =                                                                 (3.2\n                                         n                                                                                   (3.2\n                                         n          n\u22121\n                                                    n\u22121              C (w    )\n                                                                     C (w    )\n                                                                           n\u22121\n         Laplace-                                                           n\u22121\n add-one                   smoothed bigrams\n             smoothed bigram counts, we need to augment the unigram count by th\nr add-one smoothed bigram counts, we need to augment the unigram count by t\n  ber of total word types in the vocabulary V :\nmber of total word types in the vocabulary V :\n                                                    C (w        w ) + 1               C (w               w ) + 1\n                                                    C (w            w ) + 1                C (w          w ) + 1\n                                                                     n                                   n\n                                                          n\u22121        n                              n\u22121  n\n                                         P                n\u22121                                        n\u22121\n    P                (w |w     ) =            P                                      =\n    P                (w |w            ) =                                             =                                      (3.2\n         Laplace                                                                                                             (3.2\n          Laplace       n\n                        n   n\u22121\n                            n\u22121                       (C (w               w) + 1)          C (w          ) + V\n                                                      (C (w               w) + 1)          C (w          ) + V\n                                                             n\u22121                                     n\u22121\n    16       C          3  \u2022   N-             L               n\u22121                                     n\u22121\n                                                    w        M\n                HAPTER             GRAM             w\n                                                   ANGUAGE          ODELS\n s, each of the unigram counts given in the previous section will need to be aug\nus, each of the unigram counts given in the previous section will need to be au\nnted by V =     i             want            to             eat           chinese         food          lunch       spend\n nted by V = 1446. The result is the smoothed bigram probabilities in Fig. 3.7.\n    i           1446. The result is the smoothed bigram probabilities in Fig. 3.7.\n                0.0015        0.21            0.00025        0.0025        0.00025         0.00025       0.00025     0.00075\n    want        0.0013        0.00042         0.26           0.00084       0.0029          0.0029        0.0025      0.00084\n It is often convenient to reconstruct the count matrix so we can see how much\n It is often convenient to reconstruct the count matrix so we can see how much\n    to          0.00078       0.00026         0.0013         0.18          0.00078         0.00026       0.0018      0.055\noothing algorithm has changed the original counts. These adjusted counts can b\noothing algorithm has changed the original counts. These adjusted counts can\n    eat         0.00046       0.00046         0.0014         0.00046       0.0078          0.0014        0.02        0.00046\n puted by Eq. 3.25. Figure 3.8 shows the reconstructed counts.\n  puted by Eq. 3.25. Figure 3.8 shows the reconstructed counts.\n    chinese     0.0012        0.00062         0.00062        0.00062       0.00062         0.052         0.0012      0.00062\n    food        0.0063        0.00039         0.0063         0.00039       0.00079         0.002         0.00039     0.00039\n    lunch       0.0017        0.00056         0.00056        0.00056       0.00056         0.0011        0.00056     0.00056\n                                                   [C (w        w ) + 1] \u00d7 C (w                      )\n                                                    [C (w           w ) + 1] \u00d7 C (w                   )\n                                                                     n\n                                                          n          n\n                                                           n\n                                                             \u2212\n                                                             \u2212\n                                                              1\n                                                              1                            n\n                                                                                           n\n                                                                                            \u2212\n                                                                                            \u2212\n                                                                                                    1\n                     \u2217                                                                              1\n                     c \u2217\n                     c (w      w ) =\n    spend                 (w       w ) =\n                0.0012        0.00058         0.0012         0.00058       0.00058         0.00058       0.00058     0.00058 (3.2\n                                      n                                                                                      (3.2\n                           n          n\n                              n\n                              \u2212\n                              \u2212\n                               1\n                               1                              C (w           ) + V\n    Figure 3.7       Add-one smoothed bigram                  C (w           ) + V\n                                                    probabilities for eight of the words (out of V = 1446) in the BeRP\n                                                                          n\n                                                                          n\n                                                                           \u2212\n                                                                           \u2212\n                                                                            1\n    corpus of 9332 sentences. Previously-zero probabilities are in           1\n Note that add-one smoothing has made a                                    gray.\n Note that add-one smoothing has made                                     very big change to the counts. Com\n                                                                          a very big change to the counts. Com\n---\noothed probability. This adjusted count is easier to compare direct\n056   0.00056       0.00056  0.00056  0.0011  0.00056  0\nLE counts. That is, the Laplace probability can equally be expresse\nE counts. That is, the Laplace probability can equally be expresse\n058   Visualization Technique: Reconstituted counts\nted   0.0012        0.00058  0.00058  0.00058  0.00058  0\n      count divided by the (non-smoothed) denominator from Eq. 3.25:\ned count divided by the (non-smoothed) denominator from Eq. 3.25:\ned bigram probabilities for eight of the words (out of V = 1446) in th\n                                      \u2217\n                             C(w  w ) + 1  C\u2217 (w       w )\nted by Eq.  3.26.            C(w  w ) + 1  C (w        w )\n               Previously-zero probabilities are in gray.\n                                  n\u22121 n                n\u22121 n\n      P       (w |w   ) =    n\u22121      n    =           n\u22121 n\n      P       (w |w   ) =                  =\n         Laplace      n  n\u22121\n      Laplace     n      n\u22121  C(w     ) + V   C(w          )\n                              C(w n\u22121) + V    C(w n\u22121)\n                                  n\u22121                  n\u22121\ng terms, we can solve for C \u2217 (wn\u22121 wn ) :\n\n      C \u2217 (wn\u22121 wn ) = [C (wn\u22121 wn ) + 1] \u00d7 C (wn\u22121 )\n                              C (wn\u22121 ) + V\nshows the reconstructed counts, computed by Eq. 3.27.\n i       want         to      eat        chinese  food      lunch\n---\nant          0.0013        0.00042       0.26         0.00084        0.0029           0.0029           0.0025      0.00084\n 0.00078                   0.00026       0.0013       0.18           0.00078          0.00026          0.0018      0.055\n             Reconstituted counts C*\n ting terms, we can solve for C \u2217 (w                                      w ) :\n             0.00046       0.00046       0.0014       0.00046        0.0078     n     0.0014           0.02        0.00046\n inese       0.0012        0.00062       0.00062      0.00062        n\u22121\n                                                                     0.00062          0.052            0.0012      0.00062\n od          0.0063        0.00039       0.0063     [ 0.00039        0.00079          0.002            0.00039     0.00039\n nch         0.0017        0.00056       0.00056    C (w             w ) + 1] \u00d7 C (w                             )\n                                                      0.00056        0.00056          0.0011           0.00056     0.00056\n end         0.0012C \u2217 (w               w ) =                 n\u22121        n                             n\u22121\n                           0.00058       0.0012       0.00058        0.00058          0.00058          0.00058     0.00058\ngure 3.7     Add-one         n\u22121         n                       C (wn\u22121 ) + V\n                         smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP\n pus of 9332 sentences. Previously-zero probabilities are in gray.\n.8 shows the reconstructed counts, computed by Eq. 3.27.\n                                 i         want      to          eat          chinese          food     lunch       spend\n                  i              3.8       527       0.64        6.4          0.64             0.64     0.64        1.9\n        i                want             to                  eat               chinese                  food           lunch\n                  want           1.2       0.39      238         0.78         2.7              2.7      2.3         0.78\n             C*:  to             1.9       0.63      3.1         430          1.9              0.63     4.4         133\n                  eat            0.34     0.64                6.4               0.64                     0.64           0.64\n                                           0.34      1           0.34         5.8              1        15          0.34\n                  chinese        0.2       0.098     0.098       0.098        0.098            8.2      0.2         0.098\n                         0.39             238                 0.78              2.7                      2.7            2.3\n                  food           6.9       0.43      6.9         0.43         0.86             2.2      0.43        0.43\n                  lunch          0.57      0.19      0.19        0.19         0.19             0.38     0.19        0.19\n                         0.63             3.1                 430               1.9                      0.63           4.4\n        0.34      spend          0.32      0.16      0.32        0.16         0.16             0.16     0.16        0.16\n                         0.34             1                   0.34              5.8                      1              15\n                 Figure 3.8      Add-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus\n---\n16       C          3     \u2022      N-                  L                   M\n            HAPTER normalized Berkeley Restaurant Project sentences.                                        Note that the majority of the\n                                     GRAM               ANGUAGE                ODELS\nCompare with raw bigram counts\n                    values are zero. In fact, we have chosen the sample words to cohere with each other;\n              i     a matrix selected from a random set of eight words would be even more sparse.\n                                 want                to               eat                chinese           food                 lunch      spend\n i            0.0015             0.21       i        0.00025          0.0025             0.00025           0.00025              0.00025      0.00075\n want         0.0013             0.00042                   want          to         eat       chinese              food          lunch      spend\n                          i                     5    0.26             0.00084            0.0029            0.0029               0.0025       0.00084\n to           0.00078            0.00026                   827           0          9         0                    0             0          2\n                          want                  2          0.0013     0.18               0.00078           0.00026              0.0018     0.055\n eat          0.00046            0.00046                     0           608        1         6                    6             5          1\n                          to                    2          0.0014     0.00046            0.0078            0.0014               0.02         0.00046\n chinese      0.0012             0.00062                     0           4          686       2                    0             6          211\n                          eat                   0    0.00062          0.00062            0.00062           0.052                0.0012       0.00062\n food         0.0063             0.00039                     0           2          0         16                   2             42         0\noriginal                  chinese               1          0.0063     0.00039            0.00079           0.002                0.00039      0.00039\n lunch        0.0017             0.00056                     0           0          0         0                    82            1          0\n                          food              15 0.00056                0.00056            0.00056           0.0011               0.00056      0.00056\n spend        0.0012             0.00058                     0           15         0         1                    4             0          0\n                          lunch                 2          0.0012     0.00058            0.00058           0.00058              0.00058      0.00058\nFigure 3.7                                                   0           0          0         0                    1             0          0\n                                                        Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP\ncorpus of 9332            spend                 1            0           1          0         0                    0             0          0\n                   sentences. Previously-zero probabilities are in gray.\n                    Figure 3.1                                      Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-\n                    rant               i                want        to               eat           chinese              food      lunch      spend\n                                Project corpus of 9332 sentences. Zero counts are in gray. Each cell shows the count of\n                    the column label word following the row label word. Thus the cell in row i and column want\n                     i                 3.8              527         0.64             6.4           0.64                 0.64      0.64       1.9\n                    means that want followed i 827 times in the corpus.\n                     want              1.2              0.39        238              0.78          2.7                  2.7       2.3        0.78\nadd-1                to Figure         1.9              0.63        3.1              430           1.9                  0.63      4.4        133\n                     eat             3.2 shows the bigram probabilities after normalization (dividing each cell\n                    in Fig. 3.1        0.34             0.34        1                0.34          5.8                  1         15         0.34\nsmoothed             chinese           by the appropriate unigram for its row, taken from the following set of\n                    unigram            0.2              0.098       0.098            0.098         0.098                8.2       0.2        0.098\n                     food        counts):\n                                       6.9              0.43        6.9              0.43          0.86                 2.2       0.43         0.43\n                     lunch                  i              want    to           eat      chinese    food      lunch            spend\n                                       0.57             0.19        0.19             0.19          0.19                 0.38      0.19         0.19\n                     spend             0.32             0.16        0.32             0.16          0.16                 0.16      0.16         0.16\n                                            2533           927     2417         746      158        1093      341              278\n---\nAdd-1 estimation is a blunt instrument\n\nSo add-1 isn\u2019t used for N-grams:\n \u25e6  Generally we use interpolation or backoff instead\nBut add-1 is used to smooth other NLP models\n \u25e6  Or we can use add-k where 0<k<1\n \u25e6  It's used in naive bayes text classification\n \u25e6  In domains where the number of zeros isn\u2019t so huge.\n---\nBackoff and Interpolation\nSometimes it helps to use a simpler model\n \u25e6  Condition on less context for contexts you know less about\nBackoff:\n \u2022  If enough evidence, use trigram P(w\u2099|w\u2099\u22122w\u2099\u22121)\n \u2022  If not, use bigram P(w\u2099|w\u2099\u22121)\n \u2022  Else unigram P(w\u2099)\n\nInterpolation:\n \u25e6  mix unigram, bigram, trigram\n\nInterpolation works better\n---\n                                             \u02c6\nunigram counts.                              P(wn |wn\u22122 wn\u22121 ) = l1 P(wn |wn\nLinear Interpolation\nIn simple linear interpolation, we combine different order N-grams by linearly+l2 P(wn |\ninterpolating all the models. Thus, we estimate the trigram probability P(wn |wn\u22122 wn\u22121 )\nby mixing together the unigram, bigram, and trigram probabilities, each weighted+l3 P(wn )\nby a l :\nSimple interpolation\n                   such that the l s sum to 1:                  X\n                   \u02c6\n                   P(wn |wn\u22122 wn\u22121 ) =  l1 P(wn |wn\u22122 wn\u22121 )     li = 1\n                                        +l2 P(wn |wn\u22121 )\n                                        +l3 P(wn )              i (4.24)\n\nsuch that the l s sum to 1:In a slightly more sophisticated version of linear i\nLambdas conditional on context:\n                    X\n                   computed in a more sophisticated way, by condition\n                     li = 1                                       (4.25)\n                    i\n                   if we have particularly accurate counts for a particul\nIn a                P(Wn|Wn\u22122Wn\u22121) = \u03bb1 (wn\u22122)P(wn|wn\u22122Wn\u22121)\n         slightly more sophisticated version of linear interpolation, each l weight is\n                   counts of the trigrams based on this bigram will be\ncomputed in a more sophisticated way, by     +\u03bb2(wn\u22122)P(wn|Wn\u22121)\n                                           conditioning on the context. This way,\nif we have particularly accurate counts for a particular bigram, we assume that the\n                   make the l s for those trigrams higher and thus give\ncounts of the trigrams based on this bigram  + \u03bb3(Wn\u22121 )P(Wn)\n                                             will be more trustworthy, so we can\nmake the l s for those trigrams higher and thus give that trigram more weight in\n---\nHow to set \u03bbs for interpolation?\n\nUse a held-out corpus\n\n   Training Data     Held-Out  Test\n                     Data      Data\n\nChoose \u03bbs to maximize probability of held-out data:\n\u25e6  Fix the N-gram probabilities (on the training data)\n\u25e6  Then search for \u03bbs that give largest probability to held-\n   out set\n---\n Backoff\n\nSuppose you want:\n        P(pancakes| delicious souffl\u00e9)\n If the trigram probability is 0, use the bigram\n        P(pancakes| souffl\u00e9)\n If the bigram probability is 0, use the unigram\n        P(pancakes)\nComplication: need to discount the higher-order ngram so\nprobabilities don't sum higher than 1 (e.g., Katz backoff)\n---\nStupid Backoff\n\nBackoff without discounting (not a true probability)\n\n                 \"    count(w\u2071           )\n                 $              i\u2212k+1         if  count(w\u2071  ) > 0\nS (w | wi\u2212\u00b9  ) = $    count(wi\u2212\u00b9         )        i\u2212k+1\ni    i\u2212k+1       #              i\u2212k+1\n                 $    0.4 S (w    | wi\u2212\u00b9          )  otherwise\n                 $\n                 %                i           i\u2212k+2\n\nS (wi ) = count(w\u2071 )\nN\n                    80\n---\n Summary: What to do if you never saw an n-gram\n in training\nSmoothing: Pretend you saw every n-gram one (or k) times\nmore than you did\n \u2022  A blunt instrument (replacing a lot of zeros) but sometimes useful\nBackoff: If you haven't seen the trigram, use the (weighted)\nbigram probability instead\n \u2022  Weighting is messy; \"stupid\" backoff works fine at web-scale\nInterpolation: (weighted) mix of trigram, bigram, unigram\n \u2022  Usually the best! We also use interpolation to combine multiple LLMs\n---\nN-gram    Smoothing, Interpolation,\nLanguage  and Backoff\nModeling\n\n",
        "quiz": [
            {
                "question_text": "What is the primary goal of a language model?",
                "answers": [
                    {
                        "text": "To predict upcoming words",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that a language model is a machine learning model that predicts upcoming words."
                    },
                    {
                        "text": "To generate random sentences",
                        "is_correct": false,
                        "explanation": "The concept description does not mention generating random sentences as the goal of a language model."
                    },
                    {
                        "text": "To correct grammar errors",
                        "is_correct": false,
                        "explanation": "While grammar checking is a use case for language models, it is not their primary goal."
                    },
                    {
                        "text": "To recognize speech patterns",
                        "is_correct": false,
                        "explanation": "Speech recognition is an application of language models, but it is not their primary goal."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the two main paradigms of language models?",
                "answers": [
                    {
                        "text": "N-gram language models and large language models",
                        "is_correct": true,
                        "explanation": "The concept description explicitly lists these as the two main paradigms of language models."
                    },
                    {
                        "text": "Rule-based models and statistical models",
                        "is_correct": false,
                        "explanation": "These are not mentioned in the provided content context."
                    },
                    {
                        "text": "Transformers and recurrent neural networks",
                        "is_correct": false,
                        "explanation": "These are types of models within the large language models paradigm, not the main paradigms themselves."
                    },
                    {
                        "text": "Markov models and Bayesian networks",
                        "is_correct": false,
                        "explanation": "These are not mentioned as the main paradigms of language models in the provided content context."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is one application of word prediction in language tasks?",
                "answers": [
                    {
                        "text": "Grammar or spell checking",
                        "is_correct": true,
                        "explanation": "The content explicitly mentions grammar or spell checking as an application of word prediction in language tasks."
                    },
                    {
                        "text": "Generating new words",
                        "is_correct": false,
                        "explanation": "The content does not mention generating new words as an application of word prediction."
                    },
                    {
                        "text": "Translating languages",
                        "is_correct": false,
                        "explanation": "The content does not discuss language translation as an application of word prediction."
                    },
                    {
                        "text": "Creating dictionaries",
                        "is_correct": false,
                        "explanation": "The content does not mention dictionary creation as an application of word prediction."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the primary method used by large language models (LLMs) to generate text?",
                "answers": [
                    {
                        "text": "Predicting the next word in a sequence",
                        "is_correct": true,
                        "explanation": "The content explicitly states that LLMs generate text by predicting the next word over and over again."
                    },
                    {
                        "text": "Using predefined templates for text generation",
                        "is_correct": false,
                        "explanation": "The content does not mention predefined templates; it focuses on word prediction."
                    },
                    {
                        "text": "Analyzing sentence structure for grammar",
                        "is_correct": false,
                        "explanation": "While grammar checking is mentioned as a use case, it is not the primary method of text generation."
                    },
                    {
                        "text": "Generating text based on user input prompts",
                        "is_correct": false,
                        "explanation": "The content does not mention user input prompts as the primary method; it focuses on predicting the next word."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What does an N-gram language model compute?",
                "answers": [
                    {
                        "text": "It computes the probability of a sequence of words or the probability of an upcoming word given previous words.",
                        "is_correct": true,
                        "explanation": "The concept description states that an N-gram language model computes the probability of a sentence or the probability of an upcoming word given previous words."
                    },
                    {
                        "text": "It predicts the next word in a sentence by using neural networks.",
                        "is_correct": false,
                        "explanation": "The concept description specifies that N-gram language models, not neural networks, are discussed here."
                    },
                    {
                        "text": "It generates new sentences by combining random words.",
                        "is_correct": false,
                        "explanation": "The concept description does not mention generating new sentences by combining random words."
                    },
                    {
                        "text": "It assigns a fixed probability to each word in the vocabulary.",
                        "is_correct": false,
                        "explanation": "The concept description states that an N-gram language model assigns a probability distribution over possible next words, not a fixed probability."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the Chain Rule of Probability?",
                "answers": [
                    {
                        "text": "The Chain Rule of Probability is a way to compute the joint probability of a sequence of words by multiplying together conditional probabilities of each word given the previous words.",
                        "is_correct": true,
                        "explanation": "This is directly supported by the concept description and content context, which explains the Chain Rule as a method for breaking down the joint probability of a sentence into a product of conditional probabilities."
                    },
                    {
                        "text": "The Chain Rule of Probability is a method for predicting the next word in a sentence based on the previous words.",
                        "is_correct": false,
                        "explanation": "While this is related to language models, it is not the definition of the Chain Rule of Probability, which is about computing joint probabilities."
                    },
                    {
                        "text": "The Chain Rule of Probability is a technique used to count the frequency of words in a large corpus.",
                        "is_correct": false,
                        "explanation": "The Chain Rule is not about counting word frequencies but about computing probabilities using conditional probabilities."
                    },
                    {
                        "text": "The Chain Rule of Probability is a rule for determining the grammatical correctness of a sentence.",
                        "is_correct": false,
                        "explanation": "The Chain Rule is a probabilistic concept and not related to grammar checking."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the formula for the joint probability of words in a sentence according to the Chain Rule?",
                "answers": [
                    {
                        "text": "P(w\u2081,w\u2082,w\u2083,w\u2084,w\u2085\u2026w\u2099) = P(w\u2081)P(w\u2082|w\u2081)P(w\u2083|w\u2081,w\u2082)\u2026P(w\u2099|w\u2081,\u2026,w\u2099\u208b\u2081)",
                        "is_correct": true,
                        "explanation": "This is the direct application of the Chain Rule to compute the joint probability of words in a sentence."
                    },
                    {
                        "text": "P(w\u2081,w\u2082,w\u2083,w\u2084,w\u2085\u2026w\u2099) = P(w\u2081) + P(w\u2082|w\u2081) + P(w\u2083|w\u2081,w\u2082) + \u2026 + P(w\u2099|w\u2081,\u2026,w\u2099\u208b\u2081)",
                        "is_correct": false,
                        "explanation": "The Chain Rule involves multiplication of probabilities, not addition."
                    },
                    {
                        "text": "P(w\u2081,w\u2082,w\u2083,w\u2084,w\u2085\u2026w\u2099) = P(w\u2081) \u00d7 P(w\u2082) \u00d7 P(w\u2083) \u00d7 \u2026 \u00d7 P(w\u2099)",
                        "is_correct": false,
                        "explanation": "This formula does not account for the conditional probabilities of subsequent words given previous words."
                    },
                    {
                        "text": "P(w\u2081,w\u2082,w\u2083,w\u2084,w\u2085\u2026w\u2099) = P(w\u2099|w\u2081,w\u2082,w\u2083,w\u2084,w\u2085\u2026w\u2099\u208b\u2081)",
                        "is_correct": false,
                        "explanation": "This formula only represents the conditional probability of the last word given all previous words, not the joint probability of the entire sentence."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    },
    "data/raw/Week9 - LGT.pdf": {
        "metadata": {
            "file_name": "Week9 - LGT.pdf",
            "file_type": "pdf",
            "content_length": 54823,
            "language": "en",
            "extraction_timestamp": "2025-11-26T03:24:46.256728+00:00",
            "timezone": "utc"
        },
        "content": "                    KING'S\n   Applications of  .\n                    College\n   LLM \u2013 Part I:    LONDON\n   Retrieval\n   Augmented\n   Generation\n   (RAG)\n\n   Week 9 - LGT     BVSH HOVSE\n\nG  Dr Lin Gui\n   Lin.1.gui@kcl.ac.uk\n---\nLearning outcomes\n\n\u26ab By the end of this topic, you will be able to:\n\n  \u26ab  Understand the core concepts of Retrieval-Augmented Generation and how it\n     differs from standard LLM approaches.\n\n  \u26ab  Build and configure a basic RAG pipeline using embeddings, retrievers, and\n     generators.\n\n  \u26ab  Evaluate and optimize RAG performance through effective data preparation,\n     chunking, and retrieval strategies.\n\n2\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n3\n---\n                                                        RAG overview\n\n                                                                               \u26ab                                  When answering questions\n                                                                                                                  or generating text, it first\n                                                                                                                  retrieves relevant\n                                                                                                                  information from a large\n                                                                                                                  number of documents, and\n                                                                                                                  then LLMs generates\n                                                                                                                  answers based on this\n                                                                                                                  information.\n\n                                                                               \u26ab                                  By attaching an external\n                                                                                                                  knowledge base, there is\n                                                                                                                  no need to retrain the\n                                                                                                                  entire large model for each\n                                                                                                                  specific task.\n\n                                                                               \u26ab                                  The RAG model is\n                                                                                                                  especially suitable for\n\n                                           Input        Query                       Indexing                      knowledge-intensive tasks.\nUser                                        How do you evaluate the fact       Documents\n                                            that OpenAI's CEO, Sam Altman,              Chunks Vectors\n\n        Output                              by the board in just three days,\n                                            and then was rehired by the                 embeddings\n                                            company, resembling a real-life\n                                            version of \"Game of Thrones\" in             Retrieval\n\nwithout RAG                                                                    Relevant Documents\n\nfuture events. Currently, I do not have     LLM         Generation\n\nand rehiring of OpenAI's CEO ..            Question:                           Chunk 1: \"Sam Altman Returns to                                4\nwith RAG\n\nthe company's future direction and        based on the following information:     Chunk 2: \"The Drama Concludes? Sam\n                                                                                  Altman to Return as CEO of OpenAl,\nand turns reflect power struggles and     Chunk 2:                                Board to Undergo Restructuring\"\n                                          Chunk 3 :\nOpenAl...                                 Combine Context                         OpenAl Comes to an End: Who Won\n             Answer                       and Prompts                             and Who Lost?\"\n---\n   Symbolic Knowledge or Parametric Knowledge\n\n    \u26ab Ways to optimize LLMs.\n\n    \u26ab Prompt Engineering    This week\n\n    \u26ab Instruct / Fine-tuning\n\n    \u26ab  Retrieval-Augmented\n       Generation\n\n    Week 7    Week 8\n\nExternal Knowledge\n     Required\n    High     Modular RAG\n\n             multiple modules    Retriever Fine-tuning\nAdvanced RAG                     Collaborative Fine-tuning\n\noptimization                     All of the above\n Naive RAG                       RAG             Generator Fine-tuning\n\n   XoT Prompt     Prompt Engineering  Fine-tuning          5\n e.g. CoT, ToT\nFew-shot Prompt\n    Low           Standard Prompt                      Model Adaptation\n           Low                                       High  Required\n---\nRAG vs Fine-tuning\n\n Data Processing\n                ddling.    datasets, and limited datasets may not result\n\n 6\n\n higher latency.    retrieval, resulting in lower latency.\n---\nRAG Application\n\n\u26ab Scenarios where RAG is applicable:\n\n  \u26ab  Long-tail distribution of data\n\n  \u26ab  Frequent knowledge updates\n\n  \u26ab  Answers requiring verification and traceability\n\n  \u26ab  Specialized domain knowledge\n\n  \u26ab  Data privacy preservation\n\nQuestion Answering     Fact checking    Dialog systems    Summarisation\n\nMachine translation    Code generation    Sentiment Analysis    Commonsense\n                                                                reasoning\n\n                                                                           7\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  Foundation of information Retrieval\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 8\n---\n   Foundation of information Retrieval\n\n   \u26ab What is information Retrieval?\n\n     \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n        returns those items to the user, typically in list form sorted per computed\n        relevance#\n\n   \u26ab Three main questions in information retrieval:\n\n     \u26ab  How to map the text into features (Embedding method)\n\n     \u26ab  How to measure the similarity between features (IR Modelling)\n\n     \u26ab  How to do it efficiently (Indexing)\n\n[#] Qiaozhu Mei and Dragomir Radev, \u201cInformation Retrieval,\u201d The Oxford Handbook of Computational Linguistics,\n2\u207f\u1d48 edition, Oxford University Press, 2016.    9\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n10\n---\nDiscrete representation\n\n\u26ab  In discrete representation, for both query and document, we assign each word a\n   specific dimension. If a word appears query/document, then value of the\n   corresponding dimension is:\n\n    \u26ab  In Binary representation: 1\n\n    \u26ab  In TF (term frequency) based representation: t (how many times this word\n       appears within the query/documents)\n\n    \u26ab  In TF-IDF (inverse document frequency) based representation: tlog(n/x)\n\n       \u26ab  Here, t is term frequency, n is number of documents, x is the number of\n          documents which contains this term.\n\n11\n---\nDiscrete representation (example)\n\n\u26ab We have the following documents:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n\u26ab After pre-processing:\n\n  \u26ab  D1 = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d.\n\n  \u26ab  D2 = \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n  \u26ab  D3 = \u201cshipment\u201d, \u201cgold\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n                                                  12\n---\nDiscrete representation (example)\n\n \u26ab Building vocabulary:\n\n   \u26ab V = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d, \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n \u26ab  Detect the feature for each document. If the feature occurs, the corresponding\n    value is \u20181\u2019, otherwise \u20180\u2019 (binary feature):\n\n           shipment  gold  damage  fire          delivery  silver  arrive  truck\n     D1     1        1     1       1             0         0       0       0\n     D2     0        0     0       0             1         1       1       1\n     D3     1        1     0       0             0         0       1       1\n\n 13\n---\nDiscrete representation (example)\n\n\u26ab  Definition \u2013 term frequency (TF):\n\n    \u26ab  \ud835\udc61 - how many times the term appears in the document\n\n\u26ab  Example:\n\n    \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n    \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n    \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n              shipment  gold  damage  fire  delivery  silver    arrive  truck\n        D1     1        1     1       1     0              0     0      0\n        D2     0        0     0       0     1              2     1      1\n        D3     1        1     0       0     0              0     1      1\n\n                                                                             14\n---\nDiscrete representation (example)\n\n\u26ab Definition \u2013 inverse document frequency (IDF):\n\n  \u26ab  \ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc5b/\ud835\udc65) \u2013 n is number of documents, x is the number of documents which\n     contains this term\n\n\u26ab Example:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n          shipment     gold  damage  fire  delivery  silver     arrive  truck\n          0.176     0.176    0.477   0.477  0.477    0.477      0.176   0.176\n                       Inverse document frequency vector\n\n                                                                             15\n---\nDiscrete representation (example)\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1      1         1        1      1         0        0           0         0\n D2      0         0        0      0         1        2           1         1\n D3      1         1        0      0         0        0           1         1\n                           Term frequency matrix\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n        0.176     0.176    0.477   0.477    0.477     0.477      0.176     0.176\n                           Inverse document frequency vector\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1     0.176     0.176    0.477   0.477     0        0           0         0\n D2      0        0         0      0        0.477     0.954      0.176     0.176\n D3     0.176     0.176     0      0         0        0          0.176     0.176\n\n                            TF-IDF Matrix\n                                                                                16\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n17\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n18\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n19\n---\n   Dense Passage Retrieval\n\n   \u26ab  Encode questions and text passages into continuous vectors (embeddings) and\n      retrieve passages using vector similarity instead of keyword overlapping.\n\n   \u26ab  Train directly on question\u2013passage pairs, using in-batch negatives to improve\n      efficiency.\n\n                 Question q    Passage p                                In each batch, there are multiple\n\n                 BERTQ         BERTp                Passage encoder     question\u2013answer pairs, both\n      Question encoder                                                  matched and unmatched. Matched\n                                                                        pairs should have similar\n\n                          OOOOOOOO  0OOOOOOO                            representations, while unmatched\n                                                                        pairs should have representations\n                          $h_q$                                         that are far apart.\n                                                Training phase\n\n      Similarity score: dot product  (q, =  Fine-tune two encoders\nhttps://aclanthology.org/2020.emnlp-main.550.pdf                                                 20\n---\n   ReContriever\n\n   \u26ab  What if we don\u2019t have annotated data (Matched and unmatched QA-pair).\n\n   \u26ab  Using pseudo-examples: For each passage/document p, create an augmented\n      version p\u2032. Then treat (p, p\u2032) as a positive pair:\n\n       \u26ab  Masking words (random word masking)\n\n       \u26ab  Span deletion\n\n       \u26ab  Back-translation Sentence\n\n       \u26ab  Reordering Adding noise\n\n       \u26ab  Perturbations Cropping (taking a subset of sentences)\n\nhttps://aclanthology.org/2023.findings-acl.695.pdf    21\n---\n  Using API\n\n   \u26ab  There are many APIs could do this job, for example, Mistral AI:\n\n                       YMISTRAL EMBED API\n\n                       8OPEN IN COLAB\n\n   \u26ab  Example: link    How to Generate Embeddings\n                       To generate text embeddings using Mistral Al's embeddings APl, we can make a request to the APl endpoint and specify the\n                       embedding model mistra1-embed , along with providing a list of input texts. The APl will then return the corresponding\n                       embeddings as numerical vectors, which can be used for further analysis or processing in NLP applications.\n\n   \u26ab Some other options:    PYTHON TYPESCRIPT      CURL                                                           OUTPUT\n                            import os\n                            from mistralai import Mistral\n          Sentence Bert     api_key = os.environ[\"MISTRAL_API_KEY\"]\n     \u26ab                      model = \"mistral-embed\"\n                            client = Mistral(api_key=api_key)\n\n     \u26ab    SimCSE            embeddings_batch_response = client.embeddings.create(\n                            model=model,\n                            inputs=[\"Embed this sentence.\", \"As well as this one.\"],\n\n     \u26ab    \u2026\u2026                )\n                            The output is an embedding object with the embeddings and the token usage information.\n\n                            Let's take a look at the length of the first embedding:\n\n                            PYTHON TYPESCRIPT CURL\n                            len(embeddings batch response.data[0].embedding)\n\nhttps://docs.mistral.ai/capabilities/embeddings    22\n---\nIR Modelling\n\n\u26ab What is information Retrieval?\n\n  \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n     returns those items to the user, typically in list form sorted per computed\n     relevance\n\n\u26ab Three main questions in information retrieval:\n\n  \u26ab  How to map the text into features (Embedding method)\n\n  \u26ab  How to measure the similarity between features (IR Modelling)\n\n  \u26ab  How to do it efficiently (Indexing)\n\n23\n---\nIR Modelling\n\n\u26ab  In IR modelling, we use different metric to measure the similarity/distance\n   between the query and given documents. The target is to find the top-k relevant\n   documents based on the given query.\n\n    \u26ab  Cosine similarity (for both Discrete & Continuous representation)\n\n    \u26ab  Jaccard distance (for Discrete representation only)\n\n    \u26ab  BM25 (for Discrete representation only)\n\n24\n---\nCosine similarity\n\n\u26ab Cosine similarity\n                           \u03c3\ud835\udc5b \ud835\udc65\ud835\udc56 \ud835\udc66\ud835\udc56\n                   \ud835\udc36\ud835\udc5c\ud835\udc60 \ud835\udc65, \ud835\udc66 = \u03c3\ud835\udc5b  \ud835\udc56=1 \u03c3\ud835\udc5b\n                                   \ud835\udc56=1(\ud835\udc65\ud835\udc56 )2  \ud835\udc56=1(\ud835\udc66\ud835\udc56 )2\n\n\u26ab Considering\n  \u2212  D1 = [1,1,1,1,0,0,0,0]\n  \u2212  D3 = [1,1,0,0,0,0,1,1]\n                                 \ud835\udc36\ud835\udc5c\ud835\udc60(D1,D3)=1/2\n\n25\n---\nJaccard similarity\n\n\u26ab Only considering if there is over lapping or not. We don\u2019t care about the value.\n\n\u26ab For example:            C1    sim(cl,c2)    C1  C2\n\n  \u26ab  \ud835\udc45\ud835\udc65 = [2,0,3,3]     C2\n\n  \u26ab  \ud835\udc45\ud835\udc66 = [1,1,0,5]     C3                    JACCARD SIMILARITY\n                                                  2    0.5\n                                              4  2+1+1\n\n\u26ab Jaccard similarity: \ud835\udc60\ud835\udc56\ud835\udc5a \ud835\udc65, \ud835\udc66 = \ud835\udc79\ud835\udc99\u2229\ud835\udc79\ud835\udc9a\n                                    \ud835\udc79\ud835\udc99\u222a\ud835\udc79\ud835\udc9a\n\n                                               26\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              hyperparameters         Average length of all docs\n                                                                                  27\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b      \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1      \ud835\udc56               1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n     Maybe\u2026a bit confusing                            Average length of all docs\n     Can you speak in English?  hyperparameters\n                                                                                  28\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b             \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1             \ud835\udc56        1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n Relax\u2026it is pretty simple actually    hyperparameters    Average length of all docs\n\n                                                                                  29\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:  IDF term in Q (is it an important word?)\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| )\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              The \u2018percentage\u2019 of querying words in D\n\n                                                                                  30\n---\nIndexing\n\n\u26ab Next question, how to do it efficiently (Indexing)\n\n\u26ab  Suppose we have 1k queries, and there are 1 billion documents in knowledge\n   based, how many times of comparison we need?\n\n\u26ab  1k x 1b\n    It is a really huge number.\n    In real world scenario, it could be even larger\n    If there is only one important task in information retrieval, it must be\n    \u201cindexing\u201d\n\n31\n---\nIndexing - Discrete representation\n\n\u26ab  Inverted index\n\n\u26ab  Since the discrete representation is sparse (most dims are zero), we can build\n   inverted index. For each word, we build a link list to store all the documents\n   contain this word.\n\n\u26ab  For the given query, the complexity is now only related to the #unique words in\n   the query. (In most queries, the size is just few words)\n   doclD                          geo-scopelD              geo-scopelD   docID\n     1                            Europe                   Europe        1 2 7\n     2                                Europe               France        3\n     3                                France               Portugal      5\n     4                                England              England       4\n     5                                Portugal             Quebec        6\n     6                                Quebec               Spain         8\n     7                                Europe\n     8                                Spain\n                     Forward Index                         Inverted Index\n                                                                                 32\n---\n   Indexing - Continuous representation\n\n   \u26ab  In continuous representation, it might be a bit complex. There is no sparse\n      representation anymore.\n\n   \u26ab  We can use the following method to speed up the searching.\n\n       \u26ab  Vector compression \u2013 reduce the size of vectors\n\n       \u26ab  Hierarchical clustering \u2013 in each layer only search the nearest cluster\n                                          Clustering the documents first, and then,\n                                          Only consider the nearest centroid during the searching\n          voronoi cells  xq  Centroids                        voronoi cells  xq    Centroids\n                         o\n                             o                                                     o\n                  e\n                         o\n                  o          \u00a9    o                           9                  o\n                                  -\n                       o                                           o\n         Pause (k)\n\nhttps://www.pinecone.io/learn/series/faiss/faiss-tutorial/                                       33\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n34\n---\nNaive RAG\n\n\u26ab  Step 1 \u2013 indexing\n\n    \u26ab  Divide the document into even chunks, each chunk being a piece of the\n       original text.\n\n    \u26ab  Using the encoding model to generate an embedding for each chunk.\n\n    \u26ab  Store the Embedding of each block in the vector database.\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  Retrieve the k most relevant documents using vector similarity search.\n\n\u26ab  Step 3 \u2013 Generation\n\n    \u26ab  The original query and the retrieved text are combined and input into a LLM\n       to get the final answer\n                                                                             35\n---\n  Naive RAG\n\n    \u26ab Step 1 \u2013 indexing\n\n    \u26ab Step 2 \u2013 Retrieval\n\n    \u26ab Step 3 \u2013 Generation\n\n                                    Offline\n\nDocuments  Document Chunks  Vector Database\n    8                                      36\n   User  Query    Related DocumentChunks\n                            Frozen\n    Augmented Prompt        LLM\n---\nAdvanced RAG\n\n  \u26ab Step 1 \u2013 indexing\n\n  \u26ab + index optimization\n\n  \u26ab + pre-retrieval process\n\n  \u26ab Step 2 \u2013 Retrieval\n\n  \u26ab +post-retrieval process\n\n  \u26ab Step 3 \u2013 Generation\n\n  URLS  PDFs  Database\n     Documents               Document Chunks       Vector Database\n                             Fine-grained Data Cleaning\n                             Sliding Window /Small2Big\n                             Add File Structure\n                             Query Rewrite/Clarifcation\n  User    Query              Retriever Router                          37\n                              Pre-retrieval    Related Document Chunks\n\n  Prompt              LLM                        Rerank  Filter  Prompt Compression\n                                                         Post-retrieval\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Sliding windows\n\n    \u26ab  + index optimization       Fine-grained segmentation\n\n    \u26ab  + pre-retrieval process    Adding metadata\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n38\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Sliding windows\n\n      \u26ab  + index optimization       Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process    Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n                                    Document\n\n         0                200 100   300 200  400 300  500\n\n  Split the doc into chunks, and ensure there is over lapping between chunks (WHY?)\n                                                                                   39\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                                      Sliding windows\n\n      \u26ab  + index optimization          Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                            Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n\n                          Document     Section 1             Paragraph 1.1\n\n                                       Section 2             Paragraph 1.2\n\n                          Searching on fine-grained text     Paragraph 1.3\n                                                                           40\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                               Sliding windows\n\n      \u26ab  + index optimization                        Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                     Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation                             Web page    Publishing date\n\n                                                                    Title\n     The metadata is the aspects of each chunk.\n     It will help both retriever and generator to                Parents node\n     improve the performance.\n\n  41\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process    Summarization\n\n\u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n    \u26ab  +post-retrieval process    Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n\n42\n---\nAdvanced RAG\n\n   \u26ab  Step 1 \u2013 indexing                    Retrieve routes\n\n       \u26ab  + index optimization\n\n       \u26ab  + pre-retrieval process          Summarization\n\n   \u26ab  Step 2 \u2013 Retrieval                   Rewriting\n\n       \u26ab  +post-retrieval process          Confidence judgment\n\n   \u26ab  Step 3 \u2013 Generation                  Instead of one flat \u201cretrieve chunks by embeddings\u201d step, you can:\n\n                                           Searching doc first\n\n  Retrieve routes = multiple retrieval     Query    Document    Chunk\n  paths that a RAG system can choose                            Searching chunks\n  from, depending on query intent, data                         within the doc\n  type, or document structure.\n                                                                                                             43\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing                       Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process             Summarization\n\n\u26ab  Step 2 \u2013 Retrieval                      Rewriting\n\n    \u26ab  +post-retrieval process             Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n                                           Document\n                                                           Summarise first\n                                  Query    Summarisation\n\n                                  Searching in smmarisation\n                                  instead of full documents\n                                                                          44\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing               Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process     Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval              Rewriting\n\n      \u26ab  +post-retrieval process     Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n        Benefits:                    Query\n        a more explicit query             Rewrite the query first\n        a more keyword-rich query    Rewrite the\n        a more structured query      query      Searching\n        multiple diverse sub-queries\n                                     Searching by re-written query\n                                                                  45\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process    Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n      \u26ab  +post-retrieval process    Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n  Query     Document             LLM            Confidence checking    Output\n\n            Confirm the Confidence before output\n             By similarity scores\n             By LLM Confidence scores                                        46\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing           Re-order\n\n    \u26ab  + index optimization    Filter content retrieval\n\n    \u26ab  + pre-retrieval process\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n47\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing           Re-order\n\n      \u26ab  + index optimization    Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation         Evidence#1  Evidence#2  Evidence#3  Question\n\n  LLMs is sensitive with the input order\n  The early input chunks has higher weights\n  How to organize the searched evidence for final output is\n  important\n\n  48\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Re-order\n\n      \u26ab  + index optimization       Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process    Evidence#1  Evidence#2  Evidence#3  Question\n\n  \u26ab  Step 3 \u2013 Generation\n\n                                                           Evidence#1  Evidence#3  Question\n\n  To avoid possible hallucination, filtering the irrelevant\n  evidences.\n\n                                                                                           49\n---\nModular RAG\n\n   Na\u00ef\n\u26ab  ve RAG\n\n      Read           Retrieve    Generate\n\n\u26ab  DSP\n\n   Demonstrate       Search      Predict    Generate\n\n\u26ab  Rewrite-Retrieve-Read\n\n      Rewrite        Retrieve    Read\n\n\u26ab  Retrieve-then-read\n\n   Retrieve          Read        Generate\n\n50\n---\n   Different RAG Paradigms\n\n    Modules\n    8 C 8 E2    Search\nUser Query  Documents    User Query  Documents    Routing    Predict\n\n    Indexing    Query Routing    Indexing    Rewrite  RAG  Rerank\n\n  Read\n                               Fusion\n Memory\n\n    \\Post-Retrieval    Patterns\n \u2192|l|+                               51\nSummary                        Retrieve\n\n    Output    Output\n\n    Naive RAG    Advanced RAG    Modular RAG\n---\nKey problems in RAG\n\n \u26ab  How to retrieve\n\n \u26ab  When to retrieve\n\n \u26ab  How to use the retrieved information\n\n 52\n---\nHow to retrieve\n\n \u26ab By using the information on different structuration levels\n\n \u26ab  Token level         It excels in handling long-tail and cross-domain issues with high\n                        computational efficiency, but it requires significant storage.\n\n \u26ab  Phrase level\n\n \u26ab  Chunk level         The search is broad, recalling a large amount of information, but with\n                        low accuracy, high coverage but includes much redundant information.\n\n \u26ab  Entity level\n\n \u26ab  Knowledge level     Richer semantic and structured information, but the retrieval efficiency\n                        is lower and is limited by the quality of KG.\n\n 53\n---\n   When to retrieve\n\n    \u26ab Two questions:\n\n    \u26ab When we need to retrieve information to support the QA\n\n    \u26ab How many times we need to retrieve the information\n\n    \u26ab Solution#1: Conducting once search during the reasoning process.\n\n    High efficiency, but low relevance of the\n    retrieved documents\n\n    Retrieved document d.\n              Jobs cofounded     Jobs was raisedd;    Jobs is thex    apple\n  Retriever    Apple in his      by adopted...        CEO of        pearnot\n             parents' garage\n Document    Input                 Steve Jobs         Jobs is the     apple    apple\nRetrieval    Reformulation       passed away...       CEO of        pearnot    pearnot    54\nTest Context X    Black-box      Jobs cofoundedJobs is the            apple\n Jobs is the    LM                  Apple...          CEO of           pear\n   CEO of _                                           Ensemble          not\n    Apple\n---\n  When to retrieve\n\n   \u26ab  Two questions:\n\n   \u26ab  When we need to retrieve information to support the QA\n\n   \u26ab  How many times we need to retrieve the information\n\n   \u26ab Solution#2: Adaptively conduct the search.\n\n   Balancing efficiency and information\n   might not yield the optimal solution\n\n Search results:Dx               Retriever\n [1]:Search results:Dq2\n [2]:[1]:Search results:Dq3\n [2]:[1]: ...\nP     [2]: ..                             x\n     x Generate a summary about Joe Biden.\nAy1 Joe Biden attended           q2                55\n Q2[Search(Joe Biden University)]\n y2tthe University of Pennsylvania, where he earned\n q3[Search(Joe Biden degree)]    q3\n y3 a law degree.\n---\n When to retrieve\n\n  \u26ab  Two questions:\n\n  \u26ab  When we need to retrieve information to support the QA\n\n  \u26ab  How many times we need to retrieve the information\n\n  \u26ab Solution#3: Retrieve once for every N tokens generated.\n\n  A large amount of information with low\n  efficiency and redundant information.\n\n    Masked Language Modelling:\n    Bermuda Triangle is in the    western part\n  <MASK> of the Atlantic Ocean.\nPretraining    Atlas\nFew-shot\n          Fact checking:\nBermuda Triangle is in the western                                False    56\n      part of the Himalayas.            The Bermuda\n                                    Triangle is an urban\n                                    legend focused on a\n                                      loosely-defined\n       Question answering:             region in the       Western part of the\n  Where is the Bermuda Triangle?    western part of the    North Atlantic Ocean\n                                       North Atlantic\n                                           Ocean.\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 57\n---\nKey Technologies\n\n\u26ab  Data indexing optimization\n\n\u26ab  Structured Corpus\n\n\u26ab  Retrieval Source Optimization\n\n\u26ab  KG as a Retrieval Data Source\n\n\u26ab  Query Optimization\n\n\u26ab  Embedding Optimization\n\n\u26ab  Fine-tuning on RAG\n\n58\n---\n Data indexing optimization\n\n  \u26ab Chunk optimization\n\n  \u26ab Small-2-big: Embedding at sentence level expand the window during generation\n\n  process.\n\n                   Embed Sentence \u2192 Link to Expanded Window\n\n                              Continuous observation of the Atlantic meridional\n                              overturning circulation (AMOC) has improved the\n                              understanding of its variability (Frajka-Williams et al.,    What the LLM Sees\n                              2019), but there is low confidence in the quantification\n                              of AMOC changes in the 20th century because of low\n                   Embeddingagreement in quantitative reconstructed and simulated\n                              trends. Direct observational records since the\n                   Lookup     mid-2000s remain too short to determine the relative\nQuestion:                     contributions of internal variability, natural                                59\n                              forcing and anthropogenic forcing to AMOC change\nWhat are the                  (high confidence). Over the 21st century, AMOC wil\nconcerns                      very likely decline for all SSP scenarios but will not\n                              involve an abrupt collapse before 2100. 3.2.2.4 Sea Ice\nsurrounding the               Changes\nAMOC?                         Sea ice is a key driver of polar marine life, hosting        What the LLM Sees\n                              unique ecosystems and affecting diverse marine\n                              organisms and food webs through its impact on light\n                              penetration and supplies of nutrients and organic\n                              matter (Arrigo, 2014)\n---\nData indexing optimization\n\n\u26ab  Chunk optimization\n\n\u26ab  Sliding window: sliding chunk covers the entire text, avoiding semantic ambiguity.\n\n                          Maintain overlap for\n                          contextual continuity\n\nLoaded large\ndocument  Dividing into  Merging units into\n          compact units  larger chunks\n\n60\n---\nData indexing optimization\n\n \u26ab  Chunk optimization\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 61\n---\nStructured Corpus\n\n \u26ab  Adding meta-data: adding meta-data in the query searching to improve retrieval\n    accuracy, provide context during chunking, and enables filtering\n\n                             Indexed documents\n                                                                        Filtered subset of\n                                                                        documents    Most relevant\n    Did we implement any new                                                           documents\n       policies in 2021?     year: 2020, content: ...\n\n                             year: 2020, content:                       year: 2021, content.:..\n    year = 2021              year: 2021, content: .    Select relevant  year: 2021, content...    Vector similarity    year: 2021, content:.\n                                                       documents                                  search\n                             year: 2021, content:..                     year: 2021, content...                       year: 2021, content:...\n    Metadata filter          year: 2021, content: .\n\n Filter the irrelevant docs\n\n                           Ensure each chunk contains the metadata\n\n                                                                  62\n---\nRetrieval Source Optimization\n\n \u26ab  Adding meta-data\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 63\n---\nKG as a Retrieval Data Source\n\n\u26ab  Extract entities from the user's input query, then construct a subgraph to form\n   context, and finally feed it into the large model for generation.\n\n    \u26ab  Use LLM (or other models) to extract key entities from the question.\n\n    \u26ab  Retrieve subgraphs based on entities, delving to a certain depth, such as 2\n       hops or even more.\n\n    \u26ab  Utilize the obtained context to generate answers through LLM.Two-stage\n       method: Retrieve documents through summaries, then retrieve text blocks\n       from the documents.\n\n64\n---\n  KG as a Retrieval Data Source\n\n  \u26ab  Extract entities from the user's input query, then construct a subgraph to form\n     context, and finally feed it into the large model for generation.\n     P Meta Summary Entities        x k    Layer[i+1]\n                                    Summary Entities    Summarization by LLM  Layer[i]\n                                    Normal Entities     GMM Clustering        Layer[i-1]\n     Documents                      Hilndex: Indexing with Hierarchical Knowledge\n\n                                    6Communities        Global                                            Community Report\n                  Query             Key Entity          Bridge\n                                    Reasoning Paths                              Reasoning Paths                          Generation by LLM\n                                                        xk\n                                    Hatten KG             Local    Key Entity\n                                                                  Descriptions\n                                                        HiRetrieval: Retrieval with Hierarchical Knowledge\n\nhttps://arxiv.org/pdf/2503.10150                                                                                                           65\n---\n   Query Optimization\n\n    \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n       the Query can yield better retrieval results.\n\n\u26ab  Rewrite query:\n                 Input     Input\n                           Black-box LLM\n\n                 Retriever    Rewriter\n\nInput    Example\nSmall PrLM  Input:\n            What profession does Nicholas Ray and\nRewriter            Elia Kazan have in common?\n\n                                  Query             Quuery              Query: Nicholas Ray profession\n                     Documents    Web Search        Web Search            Query: Elia Kazan profession\n                                  Retriever         Retriever           Elia Kazan was an American film and\n                                                                         theatre director, producer,\n       Black-box LLM                                Documents            screenwriter and actor, described\n           Reader                 Documents                               Nicholas Ray American author and\n                                                                          director, original name Raymond\n                                                                          Nicholas Kienzle, born August 7,\n                     Output       Black-box LLM     Black-box LLM         1911, Galesville, Wisconsin, U.S.\n                                  Reader            Reader               Correct (reader                   director\n                                  Output            Reward Output        Hit (retriever\n                     (a) Retrieve-then-read (b)Rewrite-retrieve-read    (c) Trainable rewrite-retrieve-read\n\n https://arxiv.org/pdf/2305.14283    66\n---\n Query Optimization\n\n  \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n     the Query can yield better retrieval results.\n\n  \u26ab Clarify the query:                                        Ambiguous Question (AQ)\n                                                    \"What country has the most medals         o  M\n                                                             in Olympic history?\"             88\n\n                                                      Tree of Clarifications\n                                                      Question                       Pruned   Information\n                                                   Clarification                              Retrieval\n                                                                       *\n                                                   DQ1            DQ2   DQ3\n                                                                       \"What country has\n                                                                       the most total medals\n                                                                       in Olympic history?\"\n                                                   Question       Question\n                                                   Clarification  Clarification\n                                                   *                   *                      Passages\n                                                   DQ11 DQ12 DQ13 DQ21 DQ22 DQ23 DQ24\n                                                   \"What country has the  \"What country has\n                                                   most medals in winter  the most gold medals\n                                                     Olympic history?\"  in Olympic history?\"\n\n                                                                                     Long Form Answer\n\n                                                     Answer                          \"The United States has the\n                                                   Generation                        most total medals. .\n                                                                                        Norway has won most\n\nhttps://aclanthology.org/2023.emnlp-main.63.pdf                                      medals in winter Olympic.\"    67\n---\n Embedding Optimization\n\n  \u26ab Better embedding always indicate a better retrieval results:\n\n             \u26ab    Selecting a more suitable embedding method\n                                                                0Retriever &  HotpotQA      Dataset\n                  Fine-tuning the embedding model               Framework      EM F1        2Wiki      NQ   WebQ\n             \u26ab                                                  [BM25                       EM F1 EM F1 EM F1\n                                                                               |25.4, 37.2[16.6 21.1[26.0 32.8 [22.2 31.2\n                                                                2+SuRe      38.8 53.523.8.31.036.6 47.934 4 48.5\n                                                                 +EmbQA (ours) 42.0 55.8|27.4 36.642.2 54.438.2 52.1\n                                                                DPR            20.6 21.7[10.8 13.5[25.0 34.2[23.8 34.4\n                                                                 +SuRe         25.0 31.9 14.2 16.038.8 52.336.0 49.6\n                                                                 +EmbQA (ours) |29.8 36.3 16.8 21.0    38.0 52.0\n                                                                                                   43.0 54.4\n                                                                 Contriever   [22.6 35.4\n                                                                                     [16.6 20.7[25.8 32.8\n                                                                                                             25.2 34.2\n                                                                 +SuRe          33.8 50.6 21.0 29.3 39.0 52.834.4 48.5\n                                                                 +EmbQA (ours) 36.6 52.7 26.4 34.2 42.2 53.6\n                  Try different embedding methods in the RAG    [BM25         [21.2 29.2               36.0 49.6\n                                                                20+SuRe        32.2 46.1 [13.8 21.7 18.8 25.319.0 26.1\n                                                                                            17.8 30.1\n                                                                                                    35.2 45.131.6 45.7\n                                                                  +EmbQA (ours) 34.8 44.3 18.6 30.5 35.8 46.035.8 48.1\n                                                                [DPR               7.8 11.0 3.8 4.5[22.2,26.718.8 27.7\n                                                                +Sure          15.0 21.8 6.4 8.540.0 51.8\n                                                                 +EmbQA (ours) 16.2 23.3 7.6 9.6             32.6 47.7\n                                                                                                   |40.2 49.433.4 46.0\n                                                                Contriever     19.4 28.6[13.6 20.7[21.8 27.4117.8,244\n                                                                 +SuRe         28.0 41.6 17.2 25.4 39.8 51.630.2 45.0\n                                                                 +EmbQA (ours)29.8 42.3 17.4 26.2 40.6 51.8 31.6 43.0\n                                                                [BM25          [28.6 37.1[20.2 24.1 [24.0 29.4 [22.6 31.4\n                                                                20+Sure        43.6 54.7 28.4 34.1 41.6 49.0 36.6 47.3\n                                                                 +EmbQA (ours) 44.6 55.628.8 33.8 42.4 49.2 38.2 48.7\n                                                                [DPR           8.8 9.8 5.6 7.1\n                                                                                                  [29.2 32.6[25.6 31.1\n                                                                 +Sure         21.8 27.3 12.2 16.1\n                                                                                                   45.4 54.6\n                                                                                                             38.4 49.6\n                                                                 +EmbQA (ours) 22.6 29.1 13.8 17.345.8 54.7\n  AD                                                                                                      38.6 50.1\n  Facts                                                         Contriever     27.0 34.0[17.6 20.0 26.6 31.9 21.0 29.1\n\n0                                                                +Sure         38.8 50.323.8 30.4 44.0 52.9 36.4 48.1\n                                                                +EmbQA (ours)39.0 50.2\n            General-Purpose                                                              24.4 30.9 45.2 50.5 37.0 48.6\n     C-Pack  Text Embedding                                                                                              68\nhttps://arxiv.org/pdf/2503.01606\n\n  C-MTEB  C-MTP  C-TEM  Recipe\n---\nEmbedding Optimization\n\n \u26ab Better embedding always indicate a better retrieval results:\n\n   \u26ab  Selecting a more suitable embedding method\n\n   \u26ab  Fine-tuning the embedding model\n\n      An in-context learning based method to generate prompt\n\n                                                                                       generate Query-Doc pair    Fine-tuning with pseudo data\n\n 1           A few query and\n             relevant document\n             examples\n             for each doc     You are an award\n             in documents     winning relevance                                 GPT-x\n                              expert. Suggest          Large Language Model     BARD\n                              relevant queries for                              Flan-T5\nDocuments                     this article $article                                                               69\n                              queries:                 Synthetic queries for documents\n             LLM Query Generation Prompt\n                                                       \"Labeled data\"                  9\n---\nFine-tuning on RAG\n\n\u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n  \u26ab  Retriever fine-tuning\n\n  \u26ab  Generator fine-tuning\n\n70\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n      \u26ab  Retriever fine-tuning\n\n      \u26ab  Generator fine-tuning\n\n                              A small LM\n\n    Using the attention scores\n    annotate which documents\n    the LM \u201cprefers\u201d.\n\n                                                DecSource LM\n                                                    Fusion-in-Decoder\n                                                Enc Enc. Enc\n    Source Task                                     Q+D1 Q+D2Q+DN\n                                                    Retrieve\n                                                      N Docs\n  Positives         Negatives                   Pre-Trained Retriever    71\nGround Truth U    ANCE Sampling\n    -Top-K FiDAtt                               Target LMs Target Tasks\n    https://aclanthology.org/2023.acl-long.136.pdf\n                                     Generic                GCMETRY\n                                     Plug-In                  WAKT\n   Augmentation-Adapted Retriever                           RISTORY\n                                                            LITERATRE\n                                                            SCIENCE\n                                                              MATH\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n    \u26ab Retriever fine-tuning\n\n    \u26ab Generator fine-tuning\n\n    Product Search                 Premium hiking bag.            Unstructured Data                                             Structured Data (Negative)\n    Black large capacity hiking    Size: Max Color: Black                                     Structured Data (Positive)\n    bag, made of canvas.           Material: canvas               These erasable mood pencils     [MASK1] changing [MASK2]       Tire Specifications: Material:\n\nCode Search\nGiven two numbers, the\n\nlargest number is returned\nQuery\n]\nPretrained Language Model\n                               are made of quality wood       [MASK3] with [MASK4]            Rubber Tire Size: 16X6.50-8\n                               and color temperature          Function: removing wrong        Tire Type: Tubeless Rim\ndef compare(a, b):             coating, have non-fading        writing Material: [MASK2]      Width: 5.375\" Tread Depth:\nreturn max(a, b)          colors.                              Changing Size: 18 x 0.5 cm     7.1mm\" Pattern: P332\nStructured Data (Positive)\n Structured Data (Negative)                                               T5\n                               Structured Data Alignment (Loss: C sDA)      Masked Entity Prediction (Loss: LP)\n\n    Training    Push Away                           Prediction                                          erasers\n                                                    [MASK1] Color\n                Ground Truth                        [MASK2] mood\n                                Align               [MASK3] pencils,\n    .                                               [MASK4]\n\n    Embedding Space    Optimized Embedding Space    Add an entity prediction loss in the fine-tuning           72\n---\nK\nING'S\nCollege\nLONDON\n\nThank you\n\nDr Lin Gui\nLin.1.Gui@kcl.ac.uk\nwww.kcl.ac.uk/people/lin-gui\n\n",
        "quiz": [
            {
                "question_text": "What is the primary focus of Week9 - LGT?",
                "answers": [
                    {
                        "text": "Retrieval-Augmented Generation (RAG)",
                        "is_correct": true,
                        "explanation": "The primary focus of Week9 - LGT is Retrieval-Augmented Generation (RAG), as described in the learning outcomes and content context."
                    },
                    {
                        "text": "Standard LLM approaches",
                        "is_correct": false,
                        "explanation": "The primary focus is not standard LLM approaches but rather how RAG differs from them."
                    },
                    {
                        "text": "Fine-tuning LLMs",
                        "is_correct": false,
                        "explanation": "While fine-tuning is mentioned as a way to optimize LLMs, it is not the primary focus of Week9 - LGT."
                    },
                    {
                        "text": "Prompt Engineering",
                        "is_correct": false,
                        "explanation": "Prompt Engineering is mentioned as a way to optimize LLMs but is not the primary focus of the week."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the email address of Dr. Lin Gui?",
                "answers": [
                    {
                        "text": "Lin.1.gui@kcl.ac.uk",
                        "is_correct": true,
                        "explanation": "The email address of Dr. Lin Gui is explicitly mentioned in the content context."
                    },
                    {
                        "text": "lin.gui@kcl.ac.uk",
                        "is_correct": false,
                        "explanation": "The email address provided does not match the exact format given in the content context."
                    },
                    {
                        "text": "gui.lin@kcl.ac.uk",
                        "is_correct": false,
                        "explanation": "The email address provided does not match the exact format given in the content context."
                    },
                    {
                        "text": "lin.gui@kings.ac.uk",
                        "is_correct": false,
                        "explanation": "The email address provided does not match the exact format given in the content context."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What does RAG stand for in the context of Week9 - LGT?",
                "answers": [
                    {
                        "text": "Retrieval-Augmented Generation",
                        "is_correct": true,
                        "explanation": "The content context explicitly defines RAG as 'Retrieval Augmented Generation'."
                    },
                    {
                        "text": "Random Access Generation",
                        "is_correct": false,
                        "explanation": "This option is incorrect as it does not match the definition provided in the content context."
                    },
                    {
                        "text": "Real-time Augmented Generation",
                        "is_correct": false,
                        "explanation": "This option is incorrect as it does not align with the definition of RAG provided in the content context."
                    },
                    {
                        "text": "Retrieval-Assisted Generation",
                        "is_correct": false,
                        "explanation": "This option is incorrect as it slightly alters the correct term 'Retrieval-Augmented Generation'."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the two main steps involved in the RAG process?",
                "answers": [
                    {
                        "text": "Retrieving relevant information from documents and generating answers based on this information",
                        "is_correct": true,
                        "explanation": "The RAG process involves first retrieving relevant information from documents and then generating answers based on this retrieved information."
                    },
                    {
                        "text": "Training a new model from scratch for each specific task",
                        "is_correct": false,
                        "explanation": "The RAG process does not require training a new model from scratch for each specific task; it uses an external knowledge base."
                    },
                    {
                        "text": "Using only pre-trained models without any external knowledge",
                        "is_correct": false,
                        "explanation": "The RAG process involves attaching an external knowledge base, which is not the case when using only pre-trained models without any external knowledge."
                    },
                    {
                        "text": "Manually curating all the answers without any retrieval step",
                        "is_correct": false,
                        "explanation": "The RAG process involves an automated retrieval step to gather relevant information before generating answers."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is one of the key advantages of the RAG model mentioned in the content?",
                "answers": [
                    {
                        "text": "It can retrieve relevant information from a large number of documents before generating answers.",
                        "is_correct": true,
                        "explanation": "The content explicitly states that the RAG model retrieves relevant information from documents before generating answers."
                    },
                    {
                        "text": "It requires retraining the entire large model for each specific task.",
                        "is_correct": false,
                        "explanation": "The content states that there is no need to retrain the entire large model for each specific task when using the RAG model."
                    },
                    {
                        "text": "It is only suitable for non-knowledge-intensive tasks.",
                        "is_correct": false,
                        "explanation": "The content states that the RAG model is especially suitable for knowledge-intensive tasks."
                    },
                    {
                        "text": "It does not use any external knowledge base.",
                        "is_correct": false,
                        "explanation": "The content explicitly mentions that the RAG model attaches an external knowledge base."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the name of the college associated with the Week9 - LGT content?",
                "answers": [
                    {
                        "text": "King's College London",
                        "is_correct": true,
                        "explanation": "The content context explicitly mentions 'KING'S College LONDON' as the associated college."
                    },
                    {
                        "text": "Oxford University",
                        "is_correct": false,
                        "explanation": "Oxford University is not mentioned in the provided content context."
                    },
                    {
                        "text": "Cambridge University",
                        "is_correct": false,
                        "explanation": "Cambridge University is not mentioned in the provided content context."
                    },
                    {
                        "text": "University of Edinburgh",
                        "is_correct": false,
                        "explanation": "The University of Edinburgh is not mentioned in the provided content context."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    },
    "data/raw/Words and tokens - EXTENDED.pdf": {
        "metadata": {
            "file_name": "Words and tokens - EXTENDED.pdf",
            "file_type": "pdf",
            "content_length": 46645,
            "language": "en",
            "extraction_timestamp": "2025-11-26T03:25:03.092414+00:00",
            "timezone": "utc"
        },
        "content": "Words  Words\nand\nTokens\n---\nHow many words in a sentence?\n\nThey picnicked by the pool, then\nlay back on the grass and looked at\nthe stars.\n\n 16 words\n \u25e6  if we don\u2019t count punctuation marks as words\n 18 if we count punctuation\n---\nHow many words in an utterance?\n\n\"I do uh main- mainly business data\nprocessing\"\n\nDisfluencies\n\u25e6  Fragments main-\n\u25e6  Filled pauses: uh and um\n\n\u25e6  Should we consider these to be words?\n---\nHow many words in a sentence?\n\nThey picnicked by the pool, then\nlay back on the grass and looked at\nthe stars.\n\n Type: an element of the vocabulary V\n  \u25e6  The number of types is the vocabulary size |V|\n Instance: an instance of that type in running text.\n  \u25e6  14 types and 16 instances (if we ignore punctuation).\n  \u25e6  More questions: Are They and they the same word?\n---\nHow many words in a sentence?\n\nI'm\nOrthographically one word (in the English\nwriting system)\n\nBut grammatically two words:\n1.     the subject pronoun I\n2.     the verb \u2019m, short for am.\n---\nHow many words in a sentence?\n\nNot every written language uses spaces!!\n\nChinese, Japanese and Thai don't!\n---\n How to choose tokens in Chinese\n\nChinese words are composed of characters\ncalled \"hanzi\" (\u6c49\u5b57) (or sometimes just \"zi\")\nEach one represents a meaning unit called a\nmorpheme.\nEach word has on average 2.4 of them.\nBut deciding what counts as a word is complex\nand not agreed upon.\n---\n How to do choose tokens in Chinese?\n\n\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5b \u201cYao Ming reaches the finals\u201d\n   \u25e6y\u00e1o m\u00edng j\u00ecn r\u00f9 z\u01d2ng ju\u00e9 s\u00e0i\n3 words?\n\u59da\u660e      \u8fdb\u5165 \u603b\u51b3\u8d5b                  Chinese Treebank\nYaoMing reaches finals\n5 words?\n\u59da   \u660e   \u8fdb\u5165  \u603b                   \u51b3\u8d5b        Peking University\nYao Ming reaches overall        finals\n7 words?\n\u59da   \u660e   \u8fdb  \u5165     \u603b              \u51b3     \u8d5b   Just use characters\nYao Ming enter enter overall decision game\n---\nTokenization across languages\n\nSo in Chinese we use characters (zi) as\ntokens\n But that doesn't work for, e.g., Thai and\n Japanese\n These differences make it hard to use words as\n tokens\nAnd there's another reason why we don't\nuse words as tokens!\n---\nThere are simply too many words!\n\nNotice that (roughly) the bigger the corpora,\nthe more words we find!\n\n                              Types = |V|     Instances = N\nShakespeare                   31 thousand     884,000\nBrown Corpus                  38 thousand     1 million\nSwitchboard conversations     20 thousand     2.4 million\nCOCA                          2 million       440 million\nGoogle N-grams                13+ million     1 trillion\n---\n erdan\u2019s Law (Herdan, 1960) or Heap\n    There are simply too many words!\nnguistics and information retrieval resp\n  N    = number of instances\n  b are positive constants, and 0 < b <\n  |V | = number of types in vocabulary V\n  Heaps Law = Herdan's Law\n       |V | = kN b           Roughly 0.5\n  Vocab size for a text goes up with the square\n  root of its length in words\n---\nTwo kinds of words\n\nFunction words\nContent words\n---\nTria, Loreto, Servedio, 2018\n---\nWhy is too many words a problem?\n\nNo matter how big our vocabulary\nThere will always be words we missed!\nWe will always have unknown words!\n---\nWords and Subwords\n\nBecause of these problems:\n \u25e6  Many languages don't have orthographic words\n \u25e6  Defining words post-hoc is challenging\n \u25e6  The number of words grows without bound\n\nNLP systems don't use words, but smaller units called\nsubwords\nIn the next lecture we'll start by introducing smaller units like\nmorphemes and characters\n---\nWords  Words\nand\nTokens\n---\nWords  Morphemes\nand\nTokens\n---\nWords have parts\n\nMorpheme: a minimal meaning-bearing unit in a\nlanguage.\n fox: one morpheme\n cats: two morphemes cat and \u2013s\n\nMorphology: the study of morphemes\n---\nword fox consists of one morpheme (the morpheme fox) while the word cats consists\nof two: the morpheme cat and the morpheme -s that indicates plural.\n          Morphemes in English and Chinese\n   Here\u2019s a sentence in English segmented into morphemes with hyphens:\n(2.6)    Doc work-ed care-ful-ly wash-ing the glass-es\n         Doc work-ed care-ful-ly wash-ing the\n   As we mentioned above, in Chinese, conveniently, the writing system is set up\n         glass-es\nso that each character mainly describes a morpheme. Here\u2019s a sentence in Mandarin\nChinese with each morpheme character glossed, followed by the translation:\n(2.7)     \u00d6 r \u2039        ( \u21e7 4 \u00b7 o \uff0c^                      \u02d9 \u540e       \uff0c\u2022 r\n          plum dry vegetable use clear water soak soft , remove out after , drip dry\n          \u2325 \u00e9\n          chop fragment\n          Soak the preserved vegetable in water until soft, remove, drain, and chop\n   We generally distinguish two broad classes of morphemes: roots\u2014the central\nmorpheme of the word, supplying the main meaning\u2014and affixes\u2014adding \u201cad-\n---\nTypes of morphemes\n\nroot: central morpheme of the word\n       - supplying the main meaning\naffix: adding additional meanings\n\nworked\n root work\n affix -ed\nglasses\n root glass\n affix -es\n---\nTypes of affixes\nInflectional morphemes\n\u25e6  grammatical morphemes\n\u25e6  often syntactic role like agreement\n    \u2013ed past tense on verbs\n    -s/-es plural on nouns\nDerivational morphemes\n\u25e6  more idiosyncratic in application and meaning\n\u25e6  often change grammatical class\n    care (noun)\n     + -full \u00e0 careful (adjective)\n     + -ly \u00e0 carefully (adverb)\n---\nClitics\nA morpheme that acts syntactically like a word but:\n \u25e6  is reduced in form\n \u25e6  and attached to another word\n\nEnglish: 've in I've  ('ve can't appear alone)\nEnglish: \u2019s in the teacher\u2019s book\nFrench:  l\u2019 in l\u2019opera\nArabic:  b \u2018by/with\u2019, w \u2018and\u2019.\n---\nMorphological Typology\n\nDimensions along which languages vary\nTwo are salient for tokenization:\n 1.     number of morphemes per word\n 2.     how easy it is to segment the\n        morphemes\n---\nNumber of morphemes per word\n\nFew. Cantonese, spoken in Guangdong, Guangxi, Hong Kong\nkeoi5 waa6 cyun4 gwok3 zeoi3 daai6 gaan1  uk1  hai6 ni1 gaan1\nhe  say  entire country most big  building house is  this building\n\u201cHe said the biggest house in the country was this one\u201d\n\nMany. Koryak, Kamchatka peninsula in Russia,\nt-\u0259-nk\u2019e-mej\u014b-\u0259-jetem\u0259-nni-k\n1SG.S-E-midnight-big-E-yurt.cover-E-sew-1SG.S[PFV]\n\u201cI sewed a lot of yurt covers in the middle of a night.\u201d\n---\nJoseph Greenberg (1960) scale\n\nVietnamese    Old English    Greenlandic\n              English        Sanskrit\n                         Swahili\nFarsi                    Yakut       (Inuit)\n\n1.1    1.5    1.7        2.1 2.2  2.5 2.6    3.7\n\nAnalytic                 Synthetic           Polysynthetic\n\n                     Morphemes per Word\n---\nHow easily segmentable\n\nAgglutinative languages like Turkish\n \u25e6   Very clean boundaries between morphemes\n\nFusion languages\n \u25e6   a single affix may conflate multiple morphemes,\n\n \u25e6   Russian -om in stolom (table-SG-INSTR- DECL1)\n   \u25e6 instrumental, singular, and first declension.\n \u25e6   English \u2013s in \"She reads the article\"\n   \u25e6 Means both \"third person\" and \"present tense\"\n\nThese are tendencies rather than absolutes\n---\nWords  Morphemes\nand\nTokens\n---\nWords  Unicode\nand\nTokens\n---\nUnicode\n\na method for representing text written using\n\u2022         any character (more than 150,000!)\n\u2022         in any script (168 to date!)\n\u2022         of the languages of the world\n     \u2022     Chinese, Arabic, Hindi, Cherokee, Ethiopic, Khmer, N\u2019Ko,\u2026\n     \u2022     dead ones like Sumerian cuneiform\n     \u2022     invented ones like Klingon\n     \u2022     plus emojis, currency symbols, etc.\n---\nASCII: Some history for English\n1960s American Standard Code for Information Exchange\n1 byte per character\n \u25e6  In principle 256 characters\n \u25e6  But high bit set to 0\n \u25e6  So 7 bits = 128\n \u25e6  However only 95 used        TEEE\n                                T\n The rest were for teletypes        ro-p    ciele\n---\nthem; the high-order bit of ASCII bytes is always set to 0. (Actually it only used 95\n      ASCII: Some history for English\nof them and the rest were control codes for an obsolete machine called a teletype).\nHere\u2019s a few ASCII characters with their representation in hex and decimal:\n\n Ch Hex           Dec      Ch Hex       Dec            Ch Hex       Dec     Ch Hex       Dec\n <     3C         60       @     40     64     ...     \\     5C     92      `     60     96\n =     3D         61       A     41     65     ...     [     5D     93      a     61     97\n >     3E         62       B     42     66     ...     \u02c6     5E     94      b     62     98\n ?     3F         63       C     43     67     ...     _     5F     95      c     63     99\nFigure 2.4        Some selected ASCII codes for some English letters, with the codes shown both\nin hexadecimal and decimal.\n      h           e      l      l       o\n      But ASCII is of course insufficient since there are lots of other characters in the\nworld\u2019s writing systems! Even for scripts that use Latin characters, there are many\n      68 65 6C 6C 6F\nmore than the 95 in ASCII. For example, this Spanish phrase (meaning \u201cSir, replied\nSancho\u201d) has two non-ASCII characters, \u02dc               \u00b4\n                                               n and o:\n(2.10)       \u02dc                  \u00b4\n        Senor- respondio Sancho-\n---\n               =     3D         61     A     41     65     ...     [     5D     93     a     61     97\n               >     3E         62     B     42     66     ...     \u02c6     5E     94     b     62     98\n   ASCII wasn't enough!\n               ?     3F         63     C     43     67     ...     _     5F     95     c     63     99\n              Figure 2.4        Some selected ASCII codes for some English letters, with the codes shown both\n              in hexadecimal and decimal.\n   Spanish: Se\u00f1or- respondi\u00f3 Sancho\n                    But ASCII is of course insufficient since there are lots of other characters in the\n              world\u2019s writing systems! Even for scripts that use Latin characters, there are many\n                    This sentence has non-ASCII \u00f1 and \u00f3\n              more than the 95 in ASCII. For example, this Spanish phrase (meaning \u201cSir, replied\n              Sancho\u201d) has two non-ASCII characters, \u02dc             \u00b4\n   About 100,000                                           n and o:\n                           \u02dc                Chinese/CJKV characters\n              (2.10) Se                     \u00b4\n                           nor- respondio Sancho-\n   (Chinese, Japanese, Korean, or Vietnamese)\nDevanagari          And lots of languages aren\u2019t based on Latin characters at all! The Devanagari\n   Devanagari script for 120 languages like\n              script is used for 120 languages (including Hindi, Marathi, Nepali, Sindhi, and San-\n              skrit). Here\u2019s a Devanagari example from the Hindi text of the Universal Declaration\n   Hindi, Marathi, Nepali, Sindhi, Sanskrit, etc.\n              of Human Rights:\n                                      I     P\n                           1  F ( 2\n                    Chinese has about 100,000 Chinese characters in Unicode (including overlap-\n---\nCode Points\n\nUnicode assigns a unique ID, a code point,\nto each of its 150,000 characters\n1.1 million possible code points\n\u25e6 0 \u2013 0x10FFFF\nWritten in hex, with prefix \"U+\"\n\u25e6 a is U+0061 which = 0x0061\nFirst 127 code points = ASCII\n\u25e6 For backwards compatibility\n---\n               compatible with ASCII, which means that the first 127 code points, including the\nSome code points\n               code for a, are identical with ASCII.) Here are some sample code points; some (but\n               not all) come with descriptions:\n                0061     a     LATIN SMALL LETTER A\n                0062     b     LATIN SMALL LETTER B\n                0063     c     LATIN SMALL LETTER C\n                00F9     `\n                         u     LATIN SMALL LETTER U WITH GRAVE\n                00FA     \u00b4\n                         u     LATIN SMALL LETTER U WITH ACUTE\n                00FB     \u02c6\n                         u     LATIN SMALL LETTER U WITH CIRCUMFLEX\n                00FC     \u00a8\n                         u     LATIN SMALL LETTER U WITH DIAERESIS\n                8FDB 5/23/25, 5:26 PM\n                         \u20ac\n                8FDC     \u2039\n                8FDD     \ud83c\udc0e\n                         \u203a\n                8FDE     fi\n                        5/23/25, 5:26 PM\n                1F600          GRINNING FACE\n                1F00E   \ud83c\udc0e MAHJONG TILE EIGHT OF CHARACTERS\n\nA code point has no visuals; it is not a glyph!\n               2.3.2  UTF-8 Encoding\nGlyphs are stored in fonts:             a or a or a or a\n               While the code point (the unique id) is the abstract Unicode representation of the\nBut all of     character, we don\u2019t just stick that id in a text file.\n               them are U+0061, abstract \"LATIN SMALL A\"\n               Instead, whenever we need to represent a character in a text string, we write an\nencoding       encoding of the character. There are many different possible encoding methods, but\n---\nEncodings and UTF-8\n\nWe don't stick code points directly in files\nWe store encodings of chars.\nThe most popular encoding is UTF-8\nMost of the web is stored in UTF-8\n---\nEncodings\n\nhello has these 5 code points:\nU+0068 U+0065 U+006C U+006C U+006F\nHow to write in a file?\nThere are more than 1 million code points\nSo need 4 bytes (or 3 but 3 is inconvenient):\n00 00 00 68 00 00 00 65 00 00 00 6C 00 00 00 6C 00 00 00 6F\nBut that makes files very long!\n\u25e6 Also zeros are bad (since mean \"end of string\" in ASCII)\n---\nInstead: Variable Length Encoding\n\nUTF-8 (Unicode Transformation Format 8)\nFor the first 127 code points, same as ASCII\nUTF-8 encoding of hello is :\n \u25e6  68 65 6C 6C 6F\nCode points \u2265128 are encoded as a sequence\nof 2, 3, or 4 bytes\n \u25e6  In range 128 - 255, so won\u2019t be confused with ASCII\n \u25e6  First few bits say if its 2-byte, 3-byte, or 4-byte\n---\n              encoded as a sequence of two, three, or four bytes. Each of these bytes are between\n              128 and 255, so they won\u2019t be confused with ASCII, and each byte indicates in the\n       UTF-8 Encoding\n              first few bits whether it\u2019s a 2-byte, 3-byte, or 4-byte encoding.\n\n                   Code Points                                             UTF-8 Encoding\n From - To          Bit Value                                 Byte 1       Byte 2     Byte 3     Byte 4\n U+0000-U+007F      0xxxxxxx                                  xxxxxxxx\n U+0080-U+07FF      00000yyy yyxxxxxx                         110yyyyy     10xxxxxx\n U+0800-U+FFFF      zzzzyyyy yyxxxxxx                         1110zzzz     10yyyyyy   10xxxxxx\n U+010000-U+10FFFF  000uuuuu zzzzyyyy yyxxxxxx                11110uuu     10uuzzzz   10yyyyyy   10xxxxxx\n Figure 2.5  Mapping from Unicode code point to the variable length UTF-8 encoding. For a given code point\nin the From-To range, the bit value in column 2 is packed                  yyy       yyxxxxxx\n                                                     into 1, 2, 3, or 4 bytes. Figure adapted from Unicode\n  n, code point U+00F1, = 00000000 11110001\n16.0 Core Spec Chapter 3 Table 3-6.\n\n  \u25e6 Gets encoded with pattern 110yyyyy 10xxxxxx\n                  Fig. 2.5 shows how this mapping occurs. For example these rules explain how\n              the character \u02dc\n  \u25e6 So              n, which has code point U+00F1, is mapped to the two-byte bit se-\n              is mapped to a two-byte bit sequence\n              quence 11000011 10110001 or 0xC3B1. As a result of these rules, the first 127\n  \u25e6           characters (ASCII) are mapped to one byte, most remaining characters in European,\n       11000011 10110001 = 0xC3B1.\n              Middle Eastern, and African scripts map to two bytes, most Chinese, Japanese, and\n              Korean characters map to three bytes, and rarer CJKV characters and emojis and\n---\nUTF-8 encoding\n\nThe first 127 characters (ASCII) map to 1 byte\nMost remaining characters in European, Middle\nEastern, and African scripts map to 2 bytes\nMost Chinese, Japanese, and Korean characters\nmap to 3 bytes\nRarer CJKV characters, emojis/symbols map to\n4 bytes.\n---\nUTF-8 encoding\n\nEfficient: fewer bytes for common characters,\nDoesn't use zero bytes (except for NULL\ncharacter U+0000),\nBackwards compatible with ASCII,\nSelf-synchronizing,\n\u25e6  If a file is corrupted, the nearest character boundary\n   is always findable by moving only up to 3 bytes\n---\nUTF-8 and Python 3\n\nPython 3 strings stored internally as Unicode\n\u25e6   each string a sequence of Unicode code points\n\u25e6   string functions, regex apply natively to code points.\n  \u25e6 len() returns string length in code points, not bytes\nFiles need to be encoded/decoded when\nwritten or read\n\u25e6   Every file is stored in some encoding\n\u25e6   No such thing as a text file without an encoding\n  \u25e6 If it's not UTF-8 it's something older like ASCII or iso_8859_1\n---\nWords  Unicode\nand\nTokens\n---\nWords  Byte Pair Encoding\nand\nTokens\n---\nThe NLP standard for tokenization\n\nInstead of\n\u2022 white-space / orthographic words\n \u2022  Lots of languages don't have them\n \u2022  The number of words grows without bound\n\u2022 Unicode characters\n \u2022  Too small as tokens for many purposes\n\u2022 morphemes\n \u2022  Very hard to define\nWe use the data to tell us how to tokenize.\n---\nWhy tokenize?\n\nUsing a deterministic series of tokens means\nsystems can be compared equally\n\u25e6 Systems agree on the length of a string\nEliminates the problem of unknown words\n---\nSubword tokenization\n\nTwo most common algorithms:\n\u25e6  Byte-Pair Encoding (BPE) (Sennrich et al., 2016)\n\u25e6  Unigram language modeling tokenization (Kudo,\n   2018) (sometimes confusingly called\n   \"SentencePiece\" after the library it's in)\nAll have 2 parts:\n\u25e6  A token learner that takes a raw training corpus and\n   induces a vocabulary (a set of tokens).\n\u25e6  A token segmenter that takes a raw test sentence and\n   tokenizes it according to that vocabulary\n---\n    Byte Pair Encoding (BPE) token learner\nIteratively merge frequent neighboring tokens to create longer tokens.\n\nRepeat:                            Vocabulary\n \u25e6  Choose most frequent            [A, B, C, D, E]\n\n \u25e6  neighboring pair ('A', 'B')     [A, B, C, D, E, AB]\n    Add a new merged symbol         [A, B, C, D, E, AB, CAB]\n    ('AB') to the vocabulary\n \u25e6  Replace every 'A' 'B' in the   Corpus\n    corpus with 'AB'.              A B D C A B E C A B\nUntil k merges                     AB D C AB E C AB\n                                   AB D CAB E CAB\n---\nBPE algorithm\n\nGenerally run within words\nDon't merge across word boundaries\n \u25e6  First separate corpus by whitespace\n \u25e6  This gives a set of starting strings, with whitespace\n    attached to front of them\n \u25e6  Counts come from the corpus, but can only merge\n    within strings.\n---\n  BPE token learner algorithm\n  2.4  \u2022  T EXT N ORMALIZATION  19\n\n  function B YTE - PAIR ENCODING(strings C, number of merges k) returns vocab V\n\n  V     all unique characters in C  # initial set of tokens is characters\n  for i = 1 to k do                 # merge tokens til k times\n     tL , tR  Most frequent pair of adjacent tokens in C\n     t NEW    tL + tR               # make new token by concatenating\n     V      V + t NEW               # update the vocabulary\n     Replace each occurrence of tL , tR in C with t NEW  # and update the corpus\n  return V\n\n Figure 2.13  The token learner part of the BPE algorithm for taking a corpus broken up\ninto individual characters or bytes, and learning a vocabulary by iteratively merging tokens.\n---\nByte Pair Encoding (BPE) Addendum\n\nMost subword algorithms are run inside\nspace-separated tokens.\nSo we commonly first add a special end-of-\nword symbol '__' before space in training\ncorpus\nNext, separate into letters.\n---\nxplicitly marked the spaces between words: 3\n plicitly marked the spaces between words: 3\nset   BPE token learner\n et   new new renew reset renew\n      new new renew reset renew\n      Original (very fascinating\ud83d\ude44) corpus:\n t, we\u2019ll break up the corpus into words, with leading whitespace\n , we\u2019ll break up the corpus into words, with leading whitespace,\n ir   set\u2423new\u2423new\u2423renew\u2423reset\u2423renew\n      counts; no merges will be allowed to go beyond these word bo\nr counts; no merges will be allowed to go beyond these word bou\n lt looks like the following list of 4 words and a starting vocabu\nlt looks like the following list of 4 words and a starting vocabul\ners:  Put space token at start of words\n rs:\n      corpus             vocabulary\n      corpus             vocabulary\n      22        n e w          , e, n, r, s, t, w\n                n e w    , e, n, r, s, t, w\n      22        r e n e w\n                r e n e w\n      11    s e t\n            s e t\n      11        r e s e t\n                r e s e t\n---\nr counts; no merges will be allowed to go beyond these word boundaries.\ntheir counts; no merges will be allowed to go beyond these word boundaries.\nt looks like the following list of 4 words and a starting vocabulary of 7\nesult looks like the following list of 4 words and a starting vocabulary of 7\nrs:  BPE token learner\nacters:\n W      corpus T             vocabulary\n              corpus         vocabulary\n     ORDS AND       OKENS\n        2     2     n e w    , e, n, r, s, t, w\n                        n e w    , e, n, r, s, t, w\n        2     2     r e n e w\n e                      r e n e w\n       BPE training algorithm first counts all pairs of adjacent symbols: the most\n        1     1 s e t\n nt is the pair n s e t\n        1           e because it occurs in new (frequency of 2) and renew (fre-\n              1     r e s e t\ny of 2) for a           r e s e t\n                total of 4 occurrences. We then merge these symbols, treating ne\n       Merge n      e to ne (count 4 = 2 new + 2 renew)\nsymbol, and count again:\nrealize this isn\u2019t a particularly likely or exciting sentence.\ns, we realize this isn\u2019t a particularly likely or exciting sentence.\n       corpus                    vocabulary\n       2           ne w          , e, n, r, s, t, w, ne\n       2           r e ne w\n       1           s e t\n       1           r e s e t\n---\n  ent is the pair n e because it occurs in new (frequency of 2) and renew (fre-\n f 2) for a total of 4 occurrences. We then merge these symbols, treating ne\ncy of 2) for a total of 4 occurrences. We then merge these symbols, treating ne\n          BPE token learner\nmbol, and count again:\ne symbol, and count again:\n      corpus               vocabulary\n      2   corpus           vocabulary\n          2     ne w       , e, n, r, s, t, w, ne\n      2         r ne w     , e, n, r, s, t, w, ne\n          2         e ne w\n      1      s e r e ne w\n          1     s t\n      1         r e t\n          1         e s e t\n                    r e s e t\n the most frequent pair is ne w (total count=4), which we merge.\n    Merge ne            w to new (count 4)\n ow the most frequent pair is ne w (total count=4), which we merge.\n  corpus                     vocabulary\n     corpus                  vocabulary\n  2  2       new             , e, n, r, s, t, w, ne, new\n                new          , e, n, r, s, t, w, ne, new\n  2  2       r e new\n                r e new\n  1  1 s e t\n             s e t\n  1  1       r e s e t\n                r e s e t\n---\n         Now 1              r e s e t\n                 the most frequent pair is ne w (total count=4), which we merge.\n                 BPE token learner\n   ow the most frequent pair is ne w (total count=4), which we merge.\n                  corpus             vocabulary\n                  2            new    , e, n, r, s, t, w, ne, new\n            corpus                    vocabulary\n            2     2     newr e new    , e, n, r, s, t, w, ne, new\n            2     1     s e t\n                  1     r e new\n            1     s e tr e s e t\n   Next 1 r (total count of 3) get merged to r, and then r e (total count 3) gets\n   merged to            r e s e t\nt        r        re. The system has essentially induced that there is a word-initial prefix\n            (total count of 3) get merged to r, and then r e (total count 3) gets\n   re-: Merge \u2423                r      to \u2423r (count 4) and \u2423r e to \u2423re (count 3)\n   ed to     re. The system has essentially induced that there is a word-initial prefix\n             corpus                   vocabulary\n             2          new           , e, n, r, s, t, w, ne, new, r,           re\n    corpus                            vocabulary\n             2       re new\n    2        1   new                  , e, n, r, s, t, w, ne, new, r,           re\n    2             s e t               System has learned prefix re- !\n             1 re new\n                     re s e t\n    1        s e t\n---\nre-:\nBPE\n    corpus            vocabulary\n    2         new     , e, n, r, s, t, w, ne, new,          r,   re\n    2         re new\n    1       s e t\nThe next merges are:\n    1         re s e t\nIf we continue, the next merges are:\nmerge         current vocabulary\n( , new)       , e, n, r, s, t, w, ne, new,  r,  re,  new\n( re, new)     , e, n, r, s, t, w, ne, new,  r,  re,  new,  renew\n(s, e)         , e, n, r, s, t, w, ne, new,  r,  re,  new,  renew, se\n(se, t)        , e, n, r, s, t, w, ne, new,  r,  re,  new,  renew, se, set\n\nfunction B YTE - PAIR ENCODING(strings C, number of merges k) returns vocab V\n\nV  all unique characters in C  # initial set of tokens is characters\n---\nBPE encoder algorithm\nTokenize a test sentence: run each merge learned\nfrom the training data:\n \u25e6  Greedily, in the order we learned them\n \u25e6  (test frequencies don't play a role)\nFirst: segment each test word into characters\nThen run rules: (1) merge every n e to ne, (2) merge\nne w to new, (3) \u2423r, (4)    \u2423re         etc.\nResult:\n \u25e6  Recreates training set words\n \u25e6  But also learns subwords like \u2423re   that might appear in\n    new words like rearrange\n---\nBPE and Unicode\n\nRun on large Unicode corpora, with vocabulary\nsizes of 50,000 to 200,000\nOn individual bytes of UTF-8-encoded text.\n\u25e6  BPE rediscovers 2-byte and common 3-byte UTF-8\n   sequences\n\u25e6  Only 256 possible values of a byte, so no unknown\n   tokens\n\u25e6  (BPE might learn a few illegal UTF-8 sequences\n   across character boundaries, but these can be filtered)\n---\n tps://tiktokenizer.vercel.app/) to see the number of tokens in a giv\n    Visualizing GPT4o tokens\n tence. For example here\u2019s the tokenization of a nonsense sentence we made u\n visualizer uses a center dot to indicate a space:\n                        Tat Dat Duong\u2019s Tiktokenizer visualizer\n\n  Anyhow,\u00b7she's\u00b7seen\u00b7Jane's\u00b7224123\u00b7flowers\u00b7anyhow!\n    The visualization shows colors to separate out words, but of course the true o\n Tokens: 11865, 8923, 11, 31211, 6177, 23919, 885, 220, 19427, 7633, 18887, 147065, 0\n t of the tokenizer is simply a sequence of unique token ids. (In case you\u2019re\n    Most words are tokens, w/initial space\n ested, they were the following 13 tokens: 11865, 8923, 11, 31211, 6177, 2391\n 5, 220, Clitics like \u2019s\n    19427, 7633, 18887, 147065, 0)\n    \u25e6 Are segmented off Jane\n Notice that most words are their own token, usually including the leading spac\n    \u25e6 But part of frequent words like she\u2019s\nitics like \u2019s are segmented off when they appear on proper nouns like Jane, b\n    Numbers segmented into chunks of 3 digits\n counted as part of a word for frequent words like she\u2019s. Numbers tend to\n    Some of this is from preprocessing\n mented into chunks of 3 digits. And some words (like anyhow) are segment\n    \u25e6 regular expressions for chunking digits, stripping clitics\n ferently if they appear capitalized sentence-initially (two tokens, Any and ho\n---\nTokenizing across languages\n\nEven though BPE tokenizers are multilingual\nLLM training data is still vastly dominated by\nEnglish\n Most BPE tokens used for English, leaving less for\n other languages\n Words in other languages are often split up\n---\n odels is vastly dominated by English text, these multilingual BPE tokenizers ten\n ntence from a recipe for plantains, together with an English translation.\n use most of the tokens for English, leaving fewer of them for other languages. Th\n         Tokenization is better in English\n      The English has 18 tokens; each of the 14 words is a token (none of the word\n sult is that they do a better job of tokenizing English, and the other languages ten\n split into multiple tokens):     Tat Dat Duong\u2019s Tiktokenizer visualizer on GPT4o\n get their words split up into shorter tokens. For example let\u2019s look at a Spanis\n         Figure 1: SuperBPE tokenizers encode text much  ~~more ~~      efficiently than BPE, and the\n         gap grows with larger vocabulary size.         Encoding efficiency (y-axis) is measured with\n ntence from a recipe for plantains, together with an English translation.\n      A recipe sentence in two languages\n      In\u00b7a\u00b7deep\u00b7bowl,\u00b7mix\u00b7the\u00b7orange\u00b7juice\u00b7with\u00b7the\u00b7sugar,\u00b7g\n         bytes-per-token, the number of bytes encoded per token on average over a large corpus of text.\n      The English has 18 tokens; each of the 14 words is a token (none of the word\n         In the above text with 40 bytes, SuperBPE uses 7 tokens and BPE uses 13, so the methods\u2019\n                                =    =\n      inger,\u00b7and\u00b7nutmeg.\ne        efficiencies are 40/7       5.7 and 40/13  3.1 bytes-per-token, respectively. In the graph,\n      split into multiple tokens):\n      English: 18 tokens; no words are split into multiple tokens):\n         the encoding efficiency of BPE plateaus early due to exhausting the valuable whitespace-\n         Figure 1: SuperBPE tokenizers encode text much more efficiently than BPE, and the\n         delimited words in the training data. In fact, it is bounded above by the gray dotted line,\n         gap grows with larger vocabulary size.     Encoding efficiency (y-axis) is measured with\n         which shows the maximum achievable encoding efficiency with BPE, if every whitespace-\n      By contrast, the original 16 words in Spanish have been encoded into 33 token\n      In\u00b7a\u00b7deep\u00b7bowl,\u00b7mix\u00b7the\u00b7orange\u00b7juice\u00b7with\u00b7the\u00b7sugar,\u00b7g\n         bytes-per-token, the number of bytes encoded per token on average over a large corpus of text.\n         delimited word were in the vocabulary. On the other hand, SuperBPE has dramatically\n         In the above text with 40 bytes, SuperBPE uses 7 tokens and BPE uses 13, so the methods\u2019\n uch larger number. Notice that many basic words have been broken into pieces\n         better encoding efficiency that continues to improve with increased vocabulary size, as\n                                =                   =\n      inger,\u00b7and\u00b7nutmeg.\n         efficiencies are 40/7       5.7 and 40/13   3.1 bytes-per-token, respectively. In the graph,\n r example it can continue to add common word sequences to treat as tokens to the vocabulary. The\n         hondo, \u2018deep\u2019, has been segmented into h and ondo.         Similarly fo\n         the encoding efficiency of BPE plateaus early due to exhausting the valuable whitespace-\n         different gradient lines show different transition points from learning subword to superword\n         delimited words in the training data. In fact, it is bounded above by the gray dotted line,\n go,     tokens, which always gives an immediate improvement. SuperBPE also has better encoding\n         \u2018juice\u2019, nuez, \u2018nut\u2019 and jenjibre \u2018ginger\u2019):\n      Spanish: 33 tokens; 6/16 words are split\n         which shows the maximum achievable encoding efficiency with BPE, if every whitespace-\n      By contrast, the original 16 words in Spanish have been encoded into 33 token\n         efficiency than a naive variant of BPE that does not use whitespace pretokenization at all.\n         delimited word were in the vocabulary. On the other hand, SuperBPE has dramatically\n         performing well on these languages. Including multi-word tokens promises to be beneficial\n uch larger number. Notice that many basic words have been broken into pieces\n         better encoding efficiency that continues to improve with increased vocabulary size, as\n      En\u00b7un\u00b7recipiente\u00b7hondo,\u00b7mezclar\u00b7el\u00b7jugo\u00b7de\u00b7naranja\u00b7con\n         in several ways: it can lead to shorter token sequences, lowering the computational costs of\n     r example it can continue to add common word sequences to treat as tokens to the vocabulary. The\n         hondo, \u2018deep\u2019, has been segmented into h and ondo.         Similarly f\n         LM training and inference, and may also offer representational advantages by segmenting\n         different gradient lines show different transition points from learning subword to superword\n      \u00b7el\u00b7az\u00facar,\u00b7jengibre,\u00b7y\u00b7nuez\u00b7moscada.\n         text into more semantically cohesive units (Salehi et al., 2015; Otani et al., 2020; Hofmann\n go,     tokens, which always gives an immediate improvement. SuperBPE also has better encoding\n         \u2018juice\u2019, nuez, \u2018nut\u2019 and jenjibre \u2018ginger\u2019):\n         et al., 2021).\n         efficiency than a naive variant of BPE that does not use whitespace pretokenization at all.\n---\nWords  Byte Pair Encoding\nand\nTokens\n---\nWords      Rule-based tokenization\nand        and\nTokens     Simple Unix tools\n---\nRule-based tokenization\n\nAlthough subword tokenization is the norm\nSometimes we need particular tokens\nLike for parsing, where the parser needs\ngrammatical words, or social science\n---\nIssues for rule-based tokenization\nMostly but not always remove punctuation:\n\u25e6  m.p.h., Ph.D., AT&T, cap\u2019n\n\u25e6  prices ($45.55)\n\u25e6  dates (01/02/06)\n\u25e6  URLs (http://www.stanford.edu)\n\u25e6  hashtags (#nlproc)\n\u25e6  email addresses (someone@cs.colorado.edu)\nNumbers are tokenized differently across\nlanguages\n\u25e6  English 555,500.50 = French 555 500,50\nMultiword expressions (MWE)?\n\u25e6  New York, rock \u2019n\u2019 roll\n---\n         One commonly used tokenization standard is known as the Penn Treebank\nnk     kenization standard, used for the parsed corpora (treebanks) released by the\nion      Penn Treebank Tokenization Standard\n       guistic Data Consortium (LDC), the source of many useful datasets. This stan\n       separates out clitics (doesn\u2019t becomes does plus n\u2019t), keeps hyphenated word\n       gether, and separates out all punctuation (to save space we\u2019re showing visible sp\n       \u2018 \u2019 between tokens, although newlines is a more common output):\n\n        Input:     \"The San Francisco-based restaurant,\" they said,\n                   \"doesn\u2019t charge $10\".\n        Output:    \" The San Francisco-based restaurant , \" they said ,\n                   \" does n\u2019t charge $ 10 \" .\n\n         In practice, since tokenization is run before any other language processin\n       needs to be very fast. For rule-based word tokenization we generally use d\n       ministic algorithms based on regular expressions compiled into efficient finite\n---\n           Tokenization in NLTK\n    2  \u2022   W            T\nER     Bird, Loper and Klein (2009), Natural Language Processing with Python. O\u2019Reilly\n              ORDS AND   OKENS\n\n    >>> text =  'That U.S.A. poster-print costs $12.40...'\n    >>> pattern = r'''(?x)         # set flag to allow verbose regexps\n    ...         (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n    ...     | \\w+(?:-\\w+)*           # words with optional internal hyphens\n    ...     | \\$?\\d+(?:\\.\\d+)?%?     # currency, percentages, e.g. $12.40, 82%\n    ...     | \\.\\.\\.               # ellipsis\n    ...     | [][.,;\"'?():_`-]     # these are separate tokens; includes ], [\n    ... '''\n    >>> nltk.regexp_tokenize(text, pattern)\n    ['That',    'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n    Figure 2.8  A Python trace of regular expression tokenization in the NLTK Python-based\n---\n    Sentence Segmentation\n!, ? mostly unambiguous but period \u201c.\u201d is very\nambiguous\n \u25e6  Sentence boundary\n \u25e6  Abbreviations like Inc. or Dr.\n \u25e6  Numbers like .02% or 4.3\nCommon algorithm: Tokenize first: use rules or ML\nto classify a period as either (a) part of the word or\n(b) a sentence-boundary.\n \u25e6  An abbreviation dictionary can help\nSentence segmentation can then often be done by\nrules based on this tokenization.\n---\n  Space-based tokenization\n\nA very simple way to tokenize\n\u25e6   For languages that use space characters between\n    words\n  \u25e6 Arabic, Cyrillic, Greek, Latin, etc., based writing systems\n\u25e6   Segment off a token between instances of spaces\nUnix tools for space-based tokenization\n\u25e6   The \"tr\" command\n\u25e6   Inspired by Ken Church's UNIX for Poets\n\u25e6   Given a text file, output the word tokens and their\n    frequencies\n---\n      Simple Tokenization in UNIX\n (Inspired by Ken Church\u2019s UNIX for Poets.)\n Given a text file, output the word tokens and their frequencies\ntr -sc \u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt          Change all non-alpha to newlines\n      | sort\n      | uniq \u2013c     Sort in alphabetical order\n                          Merge and count each type\n1945 A\n  72 AARON\n  19 ABBESS\n  5 ABBOT    25 Aaron\n ... ...         6  Abate\n                 1  Abates\n                 5  Abbess\n                 6  Abbey\n                 3  Abbot\n             ....  \u2026\n---\nThe first step: tokenizing\n\ntr  -sc  \u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt | head\n\nTHE\nSONNETS\nby\nWilliam\nShakespeare\nFrom\nfairest\ncreatures\nWe\n...\n---\nThe second step: sorting\n\ntr -sc \u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt | sort | head\n\nA\nA\nA\nA\nA\nA\nA\nA\nA\n...\n---\n  More counting\n\n Merging upper and lower case\ntr \u2018A-Z\u2019 \u2018a-z\u2019 < shakes.txt | tr \u2013sc \u2018A-Za-z\u2019 \u2018\\n\u2019 | sort | uniq \u2013c\n Sorting the counts\ntr \u2018A-Z\u2019 \u2018a-z\u2019 < shakes.txt | tr \u2013sc \u2018A-Za-z\u2019 \u2018\\n\u2019 | sort | uniq \u2013c | sort \u2013n \u2013r\n               23243 the\n               22225 i\n               18618 and\n               16339 to\n               15687 of\n               12780 a        What happened here?\n               12163 you\n               10839 my\n               10005 in\n               8954  d\n---\nWords      Rule-based tokenization\nand        and\nTokens     Simple Unix tools\n---\nWords  Corpora\nand\nTokens\n---\nCorpora\n\nWords don't appear out of nowhere!\nA text is produced by\n \u2022 a specific writer(s),\n \u2022 at a specific time,\n \u2022 in a specific variety,\n \u2022 of a specific language,\n \u2022 for a specific function.\n---\nCorpora vary along dimensions like\n\nLanguage: 7097 languages in the world\nIt's important to test algorithms on multiple\nlanguages\nWhat may work for one may not work for\nanother\n---\nCorpora vary along dimensions like\n\nVariety, like African American English\nvarieties\n\u25e6  AAE Twitter posts might include forms like \"iont\" (I\n   don't)\nGenre: newswire, fiction, scientific articles,\nWikipedia\nAuthor Demographics: writer's age, gender,\nethnicity, socio-economic status\n---\nCode Switching\n\nSpeakers use multiple languages in the same\nutterance\nThis is very common around the world\nEspecially in spoken language and related\ngenres like texting and social media\n---\nCode Switching: Spanish/English\n\nPor primera vez veo a @username actually\nbeing hateful! It was beautiful:)\n\n[For the first time I get to see @username\nactually being hateful! it was beautiful:) ]\n---\nCode Switching: Hindi/English\n\ndost tha or ra- hega ... dont wory ... but dherya\nrakhe\n\n[\u201che was and will remain a friend ... don\u2019t worry ...\nbut have faith\u201d]\n---\nCorpus datasheets\n    Gebru et al (2020), Bender and Friedman (2018)\nMotivation:\n \u2022  Why was the corpus collected?\n \u2022  By whom?\n \u2022  Who funded it?\nSituation: In what situation was the text written?\nCollection process: If it is a subsample how was it\nsampled? Was there consent? Pre-processing?\n +Annotation process, language variety, demographics,\netc.\n---\nWords  Corpora\nand\nTokens\n---\nWords  Regular Expressions\nand\nTokens\n---\nRegular expressions are used everywhere\n\n\u25e6   A formal language for specifying text\n    strings\n\u25e6   Part of every text processing task\n  \u25e6  Often a useful pre-processing or text formatting\n     step, for example for BPE tokenization\n\u25e6   Also necessary for data analysis of text\n\u25e6   A widely used tool in industry and\n    academics\n\n                                                     84\n---\nRegular expressions\n\nWe use regular expressions to search for a\npattern in a string\n\nFor example, the Python function\nre.search(pattern,string)\n\nscans through the string and returns the first\nmatch inside it for the pattern\n---\nPython syntax\n\nWe'll show regex as raw string with double quotes:\n\nr\"regex\"\n\nRaw strings treat backslashes as literal characters\nMany regex patterns use backslashes.\n---\nA note about Python regular expressions\n\n\u25e6   Regex and Python both use backslash \"\\\" for\n    special characters. You must type extra backslashes!\n  \u25e6  \"\\\\d+\" to search for 1 or more digits\n  \u25e6  \"\\n\" in Python means the \"newline\" character, not a\n     \"slash\" followed by an \"n\". Need \"\\\\n\" for two characters.\n\u25e6   Instead: use Python's raw string notation for regex:\n  \u25e6  r\"[tT]he\"\n  \u25e6  r\"\\d+\" matches one or more digits\n    \u25e6  instead of \"\\\\d+\"\n\n                                                        87\n---\n  Regular expressions\n\nThe pattern\n\n r\"Buttercup\"\nmatches the substring Buttercup in any string, like\nthe string\n\n I\u2019m called little Buttercup\n---\n  Regular Expressions: Disjunctions\n\nLetters inside square brackets []\n\n   Pattern                              Matches\n   r\"[mM]ary\"                           Mary or mary\n   r\"[1234567890]\"                      Any one digit\nRanges using the dash [A-Z]\n\n  Pattern     Matches\n  r\"[A-Z]\"    An upper case letter     Drenched Blossoms\n  r\"[a-z]\"    A lower case letter      my beans were impatient\n  r\"[0-9]\"    A single digit           Chapter 1: Down the Rabbit Hole\n---\n Regular Expressions: Negation in Disjunction\n\n Carat as first character in [] negates the list\n \u25e6  Note: Carat means negation only when it's first in []\n \u25e6  Special characters (., *, +, ?) lose their special meaning inside []\n\nPattern       Matches                 Examples\nr\"[^A-Z]\"     Not upper case          Oyfn pripetchik\nr\"[^Ss]\"      Neither \u2018S\u2019 nor \u2018s\u2019     I have no exquisite reason\u201d\nr\"[^.]\"       Not a period            Our resident Djinn\nr\"[e^]\"       Either e or ^           Look up ^ now\n---\nKleene star and Kleene plus\n\n baa!\n baaa!\n baaaa! ...\n\nKleene star * (0 or more of previous characters)  Stephen C Kleene\nKleene plus + (1 or more of previous character)\n\n r\"baaa*\"\n r\"baa+\"\n---\nWildcard\n\nThe period means \"any character\"\n\nr\".\"  matches anything\nr\".*\" matches any sequence of 0 or more\nof anything\n---\nRegular Expressions: Anchors ^ $\n\n Pattern       Matches\n r\"^[A-Z]\"     Palo Alto\n r\"\\.$\"        The end.\n r\".$\"         The end?  The end!\n---\n Regular Expressions: More Disjunction\n\n Groundhog is another name for woodchuck!\n The pipe symbol | for disjunction\n\nPattern                       Matches\nr\"groundhog|woodchuck\"        woodchuck\nr\"yours|mine\"                 yours\nr\"a|b|c\"                      = [abc]\nr\"[gG]roundhog|[Ww]oodchuck\"  Woodchuck\n---\n Regular Expressions: Convenient aliases\n\nPattern   Expansion       Matches                 Examples\nr\"\\d\"     [0-9]           Any digit               Fahreneit 451\nr\"\\D\"     [^0-9]          Any non-digit           Blue Moon\nr\"\\w\"     [a-ZA-Z0-9_]    Any alphanumeric or     Daiyu\n                          _\nr\"\\W\"     [^\\w]           Not alphanumeric or _   Look!\nr\"\\s\"     [ \\r\\t\\n\\f]     Whitespace (space,      Look\u2423up\n                          tab)\nr\"\\S\"     [^\\s]           Not whitespace          Look up\n---\nThe iterative process of writing regex's\nFind me all instances of the word \u201cthe\u201d in a text.\n\nthe\nMisses capitalized examples\n\n[tT]he\nIncorrectly returns other or Theology\n\n\\W[tT]he\\W\n---\nFalse positives and false negatives\n\nThe process we just went through was\nbased on fixing two kinds of errors:\n\n1.     Not matching things that we should have\n       matched (The)\nFalse negatives\n\n2.     Matching strings that we should not have\n       matched (there, then, other)\nFalse positives\n---\nCharacterizing work on NLP\n\nIn NLP we are always dealing with these kinds of\nerrors.\nReducing the error rate for an application often\ninvolves two antagonistic efforts:\n \u25e6  Increasing coverage (or recall) (minimizing false\n    negatives).\n \u25e6  Increasing accuracy (or precision) (minimizing false\n    positives)\n---\nRegular expressions play a surprisingly\nlarge role\n\nWidely used in both academics and industry\n\n1.     Part of most text processing tasks, even for\n       big neural language model pipelines\n\u25e6     including text formatting and pre-processing\n2.     Very useful for data analysis of any text data\n\n99\n---\nWords  Regular Expressions\nand\nTokens\n---\nWords     Substitutions, Capture\nand       Groups, and Lookahead\nTokens\n---\nRegex Substitutions in Python\n\nTo change every instance of cherry to apricot in\nstring:\nre.sub(r\"cherry\", r\"apricot\",\nstring)\n\nUpper case all examples of a name:\nre.sub(r\"janet\", r\"Janet\", string)\n---\n Substitutions often need capture\n groups\n\nChange US format dates (mm/dd/yyyy) to\nEU : (dd-mm-yyyy)\nPattern to match US:\nr\"\\d{2}/\\d{2}/\\d{4}\"\nHow to specify in the replacement that we\nwant to swap the date and month values?\n---\nCapture group\n\nUse parentheses to capture (store) the values\nthat we matched in the search,\nGroups have numbers\nIn repl, we refer back to that group with a\nnumber command.\n---\nCapture group\n\nre.sub(r\"(\\d{2})/(\\d{2})/(\\d{4})\",\nr\"\\2-\\1-\\3\", string)}\nParens ( and ) around the two month digits, the\ntwo day digits, and the four year digits,\nThis stores\n \u25e6  the first 2 digits in group 1,\n \u25e6  the second 2 digits in group 2,\n \u25e6  final digits in group 3.\nThen in the repl string,\n \u25e6  \\1, \\2, and \\3, refer to the 1st, 2nd, and 3rd registers.\n---\nThat regex will\n\nmap\nThe date is 10/15/2011\nto\n  The date is 15-10-2011\n---\n But suppose we don't want to capture?\n\nParentheses have a double function: grouping terms, and\ncapturing\nNon-capturing groups: add a ?: after paren:\n r\"(?:some|a  few) (people|cats) like some \\1/\"\n matches\n \u25e6 some  cats  like  some  cats\n but not\n \u25e6 some  cats  like  some  some\n---\nLookahead assertions\n\n(?= pattern) is true if pattern matches, but\nis zero-width; doesn't advance character\npointer\n(?! pattern) true if a pattern does not\nmatch\nHow to capture the first word on the line, but\nonly if it doesn\u2019t start with the letter T:\nr\"\u02c6(?![tT])(\\w+)\\b\"\n---\nSimple Application: ELIZA\nEarly NLP system that imitated a Rogerian\npsychotherapist\n\u25e6 Joseph Weizenbaum, 1966.\n\nUses pattern matching to match, e.g.,:\n\u25e6 \u201cI need X\u201d\nand translates them into, e.g.\n\u25e6 \u201cWhat would it mean to  you if you  got X?\n---\nSimple Application: ELIZA\nMen are all alike.\nIN WHAT WAY\nThey're always bugging us about something or\nother. CAN YOU THINK OF A SPECIFIC EXAMPLE\nWell, my boyfriend made me come here.\nYOUR BOYFRIEND MADE YOU COME HERE\nHe says I'm depressed much of the time.\nI AM SORRY TO HEAR YOU ARE DEPRESSED\n---\nHow ELIZA works\n\n r.sub(r\".*  I\u2019M (depressed|sad) .*\",r\"I AM\nSORRY TO HEAR YOU   ARE \\1\",input)\n r.sub(r\".*  I AM (depressed|sad) .*,r\"WHY\nDO YOU THINK YOU  ARE   \\1\",input)\n r.sub(r\".*  all  .*\",r\"IN WHAT WAY?\",input)\n r.sub(r\".*  always .*\",r\"CAN YOU THINK OF  A\nSPECIFIC EXAMPLE?\",input)\n---\nWords     Substitutions, Capture\nand       Groups, and Lookahead\nTokens\n\n",
        "quiz": [
            {
                "question_text": "What are the two types of morphemes in the word 'glasses'?",
                "answers": [
                    {
                        "text": "root and affix",
                        "is_correct": true,
                        "explanation": "The word 'glasses' consists of the root 'glass' and the affix '-es', which are the two types of morphemes described."
                    },
                    {
                        "text": "prefix and suffix",
                        "is_correct": false,
                        "explanation": "'Prefix' and 'suffix' are types of affixes, not the two broad classes of morphemes."
                    },
                    {
                        "text": "base and ending",
                        "is_correct": false,
                        "explanation": "'Base' and 'ending' are not the standard terms used to describe the two types of morphemes."
                    },
                    {
                        "text": "stem and inflection",
                        "is_correct": false,
                        "explanation": "'Stem' and 'inflection' are related concepts but not the two broad classes of morphemes."
                    }
                ],
                "topic": "Words And Tokens - Extended",
                "subtopic": "Main Content",
                "concepts": [
                    "Words And Tokens - Extended"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the two broad classes of morphemes?",
                "answers": [
                    {
                        "text": "free morphemes and bound morphemes",
                        "is_correct": true,
                        "explanation": "The concept description specifies that morphemes are divided into free morphemes, which can stand alone, and bound morphemes, which cannot."
                    },
                    {
                        "text": "roots and prefixes",
                        "is_correct": false,
                        "explanation": "While roots and prefixes are types of morphemes, they do not represent the two broad classes of morphemes."
                    },
                    {
                        "text": "inflectional morphemes and derivational morphemes",
                        "is_correct": false,
                        "explanation": "These are types of affixes, not the two broad classes of morphemes."
                    },
                    {
                        "text": "lexical morphemes and grammatical morphemes",
                        "is_correct": false,
                        "explanation": "These are categories of morphemes based on their function, not the two broad classes of morphemes."
                    }
                ],
                "topic": "Words And Tokens - Extended",
                "subtopic": "Main Content",
                "concepts": [
                    "Words And Tokens - Extended"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the two types of words?",
                "answers": [
                    {
                        "text": "Function words and content words",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that there are two types of words: function words and content words."
                    },
                    {
                        "text": "Nouns and verbs",
                        "is_correct": false,
                        "explanation": "The concept description does not mention nouns and verbs as the two types of words."
                    },
                    {
                        "text": "Adjectives and adverbs",
                        "is_correct": false,
                        "explanation": "The concept description does not mention adjectives and adverbs as the two types of words."
                    },
                    {
                        "text": "Pronouns and prepositions",
                        "is_correct": false,
                        "explanation": "The concept description does not mention pronouns and prepositions as the two types of words."
                    }
                ],
                "topic": "Words And Tokens - Extended",
                "subtopic": "Main Content",
                "concepts": [
                    "Words And Tokens - Extended"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the difference between a type and an instance in the context of words?",
                "answers": [
                    {
                        "text": "A type is an element of the vocabulary, while an instance is an occurrence of that type in running text.",
                        "is_correct": true,
                        "explanation": "This is the direct definition provided in the content context."
                    },
                    {
                        "text": "A type is a specific word in a sentence, and an instance is a general category of words.",
                        "is_correct": false,
                        "explanation": "This reverses the definitions of type and instance."
                    },
                    {
                        "text": "A type refers to the number of words in a sentence, and an instance refers to the number of punctuation marks.",
                        "is_correct": false,
                        "explanation": "This misinterprets the definitions by relating them to counts of words and punctuation."
                    },
                    {
                        "text": "A type is a grammatical morpheme, and an instance is an inflectional morpheme.",
                        "is_correct": false,
                        "explanation": "This confuses the definitions with types of morphemes rather than words."
                    }
                ],
                "topic": "Words And Tokens - Extended",
                "subtopic": "Main Content",
                "concepts": [
                    "Words And Tokens - Extended"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    },
    "data/raw/Week8 - LGT-slides.pdf": {
        "metadata": {
            "file_name": "Week8 - LGT-slides.pdf",
            "file_type": "pdf",
            "content_length": 54955,
            "language": "en",
            "extraction_timestamp": "2025-11-26T03:25:24.079732+00:00",
            "timezone": "utc"
        },
        "content": "              77    KING'S\n                    College\n                    O    LONDON\n\nLarge Language\nmodels -\ntraining\n\n Week 8 - LGT    BVSH HOVSE\n\nG  Dr Lin Gui\n   Lin.1.gui@kcl.ac.uk\n---\nLearning outcomes\n\n\u25cf  By the end of this topic, you will be about to:\n\n   \u25cf     Describe the principles of supervised fine-tuning and reinforcement learning\n\n   \u25cf     Describe the difference between PPO, DPO, and GRPO.\n\n2\n---\nBefore we start..\n\n \u25cf     More instances (formatting does matter!)                   What format?\n\n \u25cf     Example: sentiment analysis, which aims to predict the sentiment label\n       (positive/negative) for the give sentence\n Give some instances\n\n      love this phone - battery lasts all day; \ud83d\ude0a\n      The update ruined everything. Apps keep crashing.\n      \u2026\u2026\n      Yeah, fantastic job\u2026 three delays in a row. \ud83d\ude44              What does the format mean?\n\n                    FEW-SHOT EXAMPLES\n\n        \"role\": \"user\"\n        \"content\": \"TExT: I love this phone-battery lasts all day!\n\n        \"role\": \"assistant\"\n        \"content\": \"{\\\"label\\\":\\\"positive\\\",\\\"confidence\\\":0.93,\\\"evidence\\\":\\\"love this phone; lasts all day; \\\"}\"    3\n---\n   Before we start..\n\n   \u25cf     There is no specific formatting in prompt designing\n\n   \u25cf     Language is unstructured data, we need to use specific symbols to make the\n         input to be structured, which be easily understood by LLM.\n\n   \u25cf     This learning ability might come from the training of coding task\n\n   \u25cf     It doesn\u2019t have to be a real format like JSON or HTML, define whatever your like\n         and just ensure that it looks structured.\n\n   \u25cf     Another possible reason is that the symbols, especially the symbols frequently\n         used in programming data, is able to draw a higher attention scores in the model\n         and help the model to find the latent patterns.\n\nhttps://colab.research.google.com/drive/1P2z4IGhOso8sYesOgvHskFHHJf8IqpqX?usp=sharing    4\n---\nBefore we start..\n\n\u25cf     What is supervised learning?\n\n       \u25cf     A machine learning method where a model is trained on a labelled dataset\n\n       \u25cf     Three main components:\n\n              \u25cf     Loss function: the learning target\n\n              \u25cf     Optimiser: how to find the target\n\n              \u25cf     Labelled data: where to train the model based on loss function and\n                    optimiser.\n\n\u25cf     Do you think the LLM training is a supervised learning task?\n\n\u25cf     Why or why not?\n\n                                                                                      5\n---\nBefore we start..\n\n\u25cf  The main challenges in LLM training:\n\n   \u25cf     Loss function: how to define the loss function? Since the generation of\n         language is not a simple prediction task. How to define the label? Frequency?\n         Or Correctness? Or some other metric?\n\n   \u25cf     Optimiser: even the loss cannot be clearly defined, how to optimise?\n\n   \u25cf     Labelled data: it seems impossible to label all the knowledge created by\n         human beings manually.\n\n\u25cf     The training of LLM is a systematic task, there is no simple solution.\n\n\u25cf     Do you remember the magic prompt we used in the last week?\n\n\u25cf     Let\u2019s do it step-by-step                                              6\n---\nLLM Training, a step-by-step to do list\n\n\u25cf     Step#1 \u2013 Pre-training (Speak like human)\n\n\u25cf     Step#2 \u2013 Instruction fine-tuning (Understand the instruction)\n\n\u25cf     Step#3 \u2013 Learning from human feedback (Learning from interaction)\n                                                                   r HAB6N\n                                                                 Iho Sic         0249\n                                                                Blos\n                                                                   igken Wirtorba\n                                                                     \"?          su\n                                                                   E\n\n7\n---\nLLM Training, a step-by-step to do list\n\n\u25cf     Step#1 \u2013 Pre-training (Speak like human)\n\n\u25cf     Step#2 \u2013 Instruction fine-tuning (Understand the instruction)\n\n\u25cf     Step#3 \u2013 Learning from human feedback (Learning from interaction)\n                                                                   r HAB6N\n                                                                 Iho Sic         0249\n                                                                Blos\n                                                                   igken Wirtorba\n                                                                     \"?          su\n                                                                   E\n\n8\n---\nPre-training\n\n\u25cf     What is pre-training?\n\n\u25cf     The goal is to train the model to predict the next token for the give text.\n\n\u25cf     It literally simplify the three challenges:\n\n       \u25cf     Loss function: easy to define, based on the predictive probability.\n\n       \u25cf     Optimiser: just directly optimise the cross-entropy loss\n\n       \u25cf     Labelled data: plain text, no need of annotation!!\n\n\u25cf  But\u2026How?\n\n9\n---\nPre-training\n\n\u25cf     The goal is to predict the next word. Suppose we have a sentence:\n\n                 May the force be with you\n\n\u25cf     We feed the words into a LLM to predict the next word. We hope the prediction\n      is correct:\n\n                 May the force             LLM     be\n\n                 May the force be          LLM     with\n\n                 May the force be with     LLM     you\n\n\u25cf     We update the parameters to guide the LLM to produce the correct prediction\n\n                                                                       10\n---\nPre-training (example)\n\n\u25cf  The goal is to predict the next word. Suppose we have a sentence:\n\nGiven context    Candidates of Prediction     P(w)\n\nMay  the    force    be    with               0.51\n\n                           on                 0.21\n\n                           in                 0.13\n\nLoss function:\n\nWhat if the predicted word is incorrect?\n\n11\n---\nPre-training (example)\n\n\u25cf  The goal is to predict the next word. Suppose we have a sentence:\n\nGiven context    Candidates of Prediction     P(w)\n\nMay  the    force    be    with               0.21\n\n                           on                 0.51\n\n                           in                 0.13\n\nLoss function:\n\nThe loss becomes small or large?\n\n12\n---\nPre-training (example)\n\n\u25cf  The goal is to predict the next word. Suppose we have a sentence:\n\n          Given context    Candidates of Prediction            P(w)\n\nMay       the    force     be                    with          0.21\n\n                                                  on           0.51\n\n     7           Graph of -log(x)                 on           0.13\n     6\n     5\n     4                                           If the correct prediction has higher\n     3                                           probability, the loss will be smaller,\n     2                                           which means there are less updates in\n     1                                           the parameters\n     0\n          0.0  0.2  0.4 X 0.6        0.8  1.0                                          13\n---\nPre-training (example)\n\n\u25cf  What if the prediction in previous round is incorrect?\n\nMay the force            LLM     be\n\nMay the force be         LLM     on\n\nMay the force be ???     LLM     ???\n\n14\n---\nPre-training (example)\n\n\u25cf  What if the prediction in previous round is incorrect?\n\n   May the force             LLM     be\n\n   May the force be          LLM     on\n\n   May the force be with     LLM     ???\n\n\u25cf     Ignore the incorrect prediction and move to the next round by using the correct\n      word.\n\n15\n---\nPre-training \u2013 Discussion\n\n\u25cf  What is the strength of Pre-training?\n\n   \u25cf     Loss function & optimizer: simply and easy to implement\n\n   \u25cf     Dataset:\n\n          \u25cf     we do not need any annotated data.\n\n          \u25cf     Plain text can be used for pretraining.\n\n          \u25cf     We can scan all the text created by human in the past \u2013 if we want to do\n                so\n\n   \u25cf     But\u2026 what is the weakness?\n\n16\n---\nPre-training \u2013 Discussion\n\n\u25cf     A classic trade-off in machine learning: model complexity and generalizability\n\n\u25cf     Complex neural network is able to approximate complex function\n\n\u25cf     Complexity of neural networks is related to:\n\n      \u25cf  Depth of a neural net \u2191, number of parameters \u2191, number of labels \u2191\n\n\u25cf     LLM is complex!\n\n\u25cf     The potential risk of a complex learning structure:\n\n       \u25cf     Overfitting\n\n       \u25cf     Memorization rather than Learning\n\n\u25cf     Q: how do we know the Pre-trained LMs truly understand the pattern in the training\n      data, or it just memorise the instances what has been learned?\n\n\u25cf  Emm, this concept maybe a bit confusing\u2026let\u2019s move to an example.    17\n---\nExample: demonstration of a neural network\n\n \u25cf  https://playground.tensorflow.org/\n\n One more question: Solving a task == Understanding of a task?\n\n 18\n---\nExample: Human vs. Neural Networks\n\n \u25cf     You will be shown an image of a character from either the Pok\u00e9mon or Digimon\n       series. Your goal is to predict the correct label: Pok\u00e9mon or Digimon.\n\n Vsr\n PoKeMON  BIGITAN\n          DIGIMON    19\n          MONSTERS\n---\n    Example: Human vs. Neural Networks\n\n    \u25cf     Dataset:\n\n    \u25cf     Pok\u00e9mon images: https://www.Kaggle.com/kvpratama/pokemon-images-dataset/data\n\n    \u25cf     Digimon images: https://github.com/DeathReaper0965/Digimon-Generator-GAN\n\n    Pok\u00e9mon    Digimon\n\nTesting\nImages:\n       20\n---\nExample: Human vs. Neural Networks\n\n\u25cf     Model: 6-layer CNN + 1 linear layer classification\n                   model = Sequential()\n                   model.add(Conv2D(32, (3, 3), padding='same', input_shape=(120,120,3)))\n                   model.add(Activation('relu'))\n                   model.add(Conv2D(32, (3, 3)))\n                   model.add(Activation('relu'))\n                   model.add(MaxPooling2D(pool_size=(2, 2)))\n                   model.add(Conv2D(64,(3, 3), padding='same'))\n                   model.add(Activation('relu'))\n                   mode1.add(Conv2D(64,(3,3)))\n                   model.add(Activation('relu'))\n                   model.add(MaxPooling2D(pool_size=(2,2)))\n                   model.add(Conv2D(256, (3, 3), padding='same'))\n                   model.add(Activation('relu'))\n                   model.add(Conv2D(256, (3, 3)))\n                   model.add(Activation('relu'))\n                   model.add(MaxPooling2D(pool_size=(2, 2)))\n                   model.add(Flatten())\n                   model.add(Dense(1024))\n                   model.add(Activation('relu'))\n                   model.add(Dense(2))\n      Training     model.add(Activation('softmax'))\n\u25cf                  accuracy: 98.9% (using training data to test the performance)\n\n\u25cf     Testing accuracy: 98.4%\n---\n Example: Human vs. Neural Networks\n\n  \u25cf  Model: 6-layer CNN + 1 linear layer classification\n\n0  0\n\n 20  20  2I  20\n\n 4I  40  4I  4I\n\n E  6  6  6\n\n 8  8  8  81\n\n10D  10D  18D  10D\n\n2 41 E8 1D  2 4I  64 a 10D  2 4I E 8 10D  2 41 E a 10D\n\n0  0\n\n2  20  2I  20\n\n4I  4I  4I  4I\n\n621  6  64  6\n\n8  8\n\n18D  10D  10D  10D\n\n2 4I  E a 1D  2 41  64 a 10D  2 40  6 8 10D  2 40 64 8 10D\n---\n Example: Human vs. Neural Networks\n\n  \u25cf  Model: 6-layer CNN + 1 linear layer classification\n\nI  0  0\n\n 20  20  20  20\n\n 4I  4I  4I  4I\n\nE  E  6\n\n8  8  88  87\n\n10D  10D  10D  10D\n\n2 4I 6 8 10D  2 41 F 8 10D  2 41 6 81 10D  2 4I F a 10D\n\nI\n\n 20  20  20  20\n\n 4l  4I  40  4I\n\n E  E  6  6\n\n8  81  8  8\n\n10D  10D  10D  10D\n\n2 41 E a 1D  2 4I 6 81 10D  2 4I  62 8 10D  2 41 6 a 1D\n---\nExample: Human vs. Neural Networks\n\n\u25cf     Model: 6-layer CNN + 1 linear layer classification\n\n\u25cf     All the images of Pok\u00e9mon are PNG, while most images of Digimon are JPEG.\n\n\u25cf     Machine discriminates Pok\u00e9mon and Digimon based on the background colours.\n\nloading the files\n\npng files have transparent     transparent background\nbackground                     becomes black\n---\nExample: Human v.s. Neural Networks\n\n \u25cf  PASCAL VOC 2007 data set\n\n This slide is from: GCPR 2017 Tutorial \u2014 W. Samek & K.-R. M\u00fcller\n\n \u25cf  What\u2019s the connection to LLMs?\n---\nLLM Training, a step-by-step to do list\n\n\u25cf     Step#1 \u2013 Pre-training (Speak like human)\n\n\u25cf     Step#2 \u2013 Instruction fine-tuning (Understand the instruction)\n\n\u25cf     Step#3 \u2013 Learning from human feedback (Learning from interaction)\n                                                                   r HAB6N\n                                                                 Iho Sic         0249\n                                                                Blos\n                                                                   igken Wirtorba\n                                                                     \"?          su\n                                                                   E\n\n26\n---\nBefore we start..\n\n\u25cf     What we have now?\n\n       \u25cf     A sequence model which is able to predict the next word\n\n\u25cf     The unsolved issue\n\n       \u25cf     It might not understand the language\n\n       \u25cf     Just simply memorise the pattern in human language\n\n       \u25cf     Predict the most likely next word\n\n27\n---\nInstruction fine-tuning\n\n\u25cf     What is instruction fine-tuning?\n\n       \u25cf     Teach a model to follow human-like instructions.\n\n\u25cf     But\u2026how?\n\n       \u25cf     Collect the data with human instruction\n\n       \u25cf     Initialise the model with pre-trained parameter\n\n       \u25cf     Fine-tune the parameters with the instructions\n\n28\n---\nInstruction fine-tuning (example)\n\n\u25cf  Pre-training: train the model to predict the next word iteratively\n\nMay the force             LLM     be\n\nMay the force be          LLM     with\n\nMay the force be with     LLM     you\n\n29\n---\nInstruction fine-tuning (example)\n\n\u25cf     Instruction fine-tuning: add the instruction as a prompt, only fine-tune on the\n      feedback text\n      Pre-trained                Initialising\n         model\n\nWhat\u2019s the iconic expression from the star wars movie?             LLM     May\n\nWhat\u2019s the iconic expression from the star wars movie? May         LLM     the\n\nWhat\u2019s the iconic expression from the star wars movie? May the     LLM     force\n\nInstruction\n\n30\n---\nInstruction fine-tuning (strength)\n\n\u25cf     With the instruction fine-tuning, the model can understand the task.\n\n\u25cf     Evidence:\n\n       \u25cf     Pretrain the model on multiple language, but only fine-tuning on a Chinese\n             task.\n\n       \u25cf     But the model is able to do this task on English.\n\n             Model  Pre-train      Fine-tune            Testing     EM       F1\n             QANet  none           Chinese QA                       66.1     78.1\n                    Chinese        Chinese QA                       82.0     89.1\n                                   Chinese QA           Chinese     81.2     88.7\n             BERT   104 languages  English QA           QA          63.3     78.8\n\n                                   Chinese + English                82.6     90.1\n                                   F1 score of Human performance is 93.30%       31\n---\nTwo possible path to AI\n\n\u25cf     #1 Pre-train base model, and develop different task focused model with\n      instruction fine-tuning       SFT on Task#1\n\n      Raw corpus    Pre-trained     SFT on Task#2\n                    model\n\n                                    SFT on Task#3\n\n\u25cf     #2 Pre-train first, and keep instruction fine-tuning on the same base model\n\nRaw corpus  Pre-trained  SFT on Task#1  SFT on Task#2  SFT on Task#3\n            model\n\n                                                                    32\n---\nExample\n\n  \u25cf     #1 Pre-train base model, and develop different task focused model with\n        instruction fine-tuning    SFT on translation\n\n  Raw corpus    Pre-trained    SFT on summarisation\n                model\n\n                               SFT on Math\n\n  New task    Identify the type of task\n\n  33\n---\n  Example\n\n    \u25cf  #2 Pre-train first, and keep instruction fine-tuning on the same base model\n\n       Raw corpus\n\n    Pre-trained model     New task\n\n                                  No need to consider\n\n    SFT on translation            the type of task.\n                                  Because the model is\n                                  able to solve all the\n    SFT on                        tasks has been learned\nsummarisation\n\n    SFT on Math\n\n    34\n---\nAGI is more complex than you think\n\n\u25cf  Possible issues\n\n   \u25cf     Catastrophic Forgetting - The model\u2019s learning on one task can overwrite or\n         interfere with what it learned on another.\n\n   \u25cf     Instruction Ambiguity - If some tasks expect \u201cshort bullet points\u201d and others\n         \u201clong-form answers,\u201d the model might produce inconsistent outputs.\n\n   \u25cf     Data Imbalance Across Tasks - The model overfits to tasks with more data,\n         ignoring smaller tasks.\n\n   \u25cf     Domain Shift or Vocabulary Clash - Tasks from different domains (medical text\n         vs. casual dialogue) can confuse the model.\n\n   \u25cf     \u2026\u2026\n                                                                           35\n---\n  AGI is more complex than you think\n\n   \u25cf  Most training details are not released to public\n\n        Natural lanquage inference  Commonsense   Sentiment        Paraphrase    Closed-book QA        Struct to text              Translation\n               (7 datasets)         (4 datasets)  (4 datasets)     (4 datasets)  (3 datasets)                    (4 datasets)      (8 datasets)\n   ANLI (R1-R3)  RTE                CoPA              IMDB              MRPC     ARC (easy/chal.)         CommonGen              ParaCrawl EN/DE\n        CB    SNLI                  HellaSwag     Sent140               QQP      NQ                              DART            ParaCrawl EN/ES\n       MNLI   WNLI                  PiQA             SST-2              PAWS     TQA                             E2ENLG          ParaCrawl EN/FR\n       QNLI                         StoryCloze        Yelp              STS-B                                    WEBNLG            WMT-16 EN/CS\n                                                                                                                                 WMT-16 EN/DE\n        Reading comp.    Read. comp. w/     Coreference        Misc.                          Summarization                        WMT-16 EN/FI\n         (5 datasets)     commonsense       (3 datasets)     (7 datasets)                     (11 datasets)                        WMT-16 EN/RO\n   BoolQ      OBQA        (2 datasets)            DPR     CoQA     TREC          AESLC        Multi-News         SamSum\n    DROP      SQuAD         CosmosQA        Winogrande    QuAC     CoLA          AG News      Newsroom           Wiki Lingua EN    WMT-16 EN/RU\n   MultiRC                   ReCoRD         WSC273        WIC      Math          CNN-DM       Opin-Abs: iDebate  XSum            WMT-16 EN/TR\n                                                          Fix Punctuation (NLG)  Gigaword     Opin-Abs: Movie\n\n   Figure 3: Datasets and task clusters used in this paper (NLU tasks in blue; NLG tasks in teal).\n   In the 2023 research paper, we can see detailed information about the data they used, but the latest technical\n   report provides very little discussion of those details.\n\nhttps://arxiv.org/pdf/2109.01652                                                                                                                36\n---\n  AGI is more complex than you think\n\n   \u25cf  More dataset/tasks, the better over all performance. Even on unseen tasks.\n\n      Finetune on many tasks (\"instruction-tuning\")\n      Input (Commonsense Reasoning)           Input (Translation)             Inference on unseen task type\n      Here is a goal: Get a cool sleep on     Translate this sentence to\n      summer days.                            Spanish:                         Input (Natural Language Inference)\n      How would you accomplish this goal?     The new office building          Premise: At my age you will probably\n      OPTIONS:                                was built in less than three     have learnt one lesson.\n      -Keep stack of pillow cases in fridge.  months.                           Hypothesis: It's not certain how many\n      -Keep stack of pillow cases in oven.    Target                            lessons you'll learn by your thirties.\n      Target                                  El nuevo edificio de oficinas     Does the premise entail the hypothesis?\n      keep stack of pillow cases in fridge     se construy\u00f3 en tres meses.      OPTIONS:\n                      Sentiment analysis tasks                                 -yes -it is not possible to tell  -no\n                      Coreference resolution tasks                                      FLAN Response\n                                                                                        It is not possible to tell\n\n                                    GPT-3 175B zero shot  GPT-3 175B few-shot  FLAN 137B zero-shot\n\n   Performance                      53.2 56.2             63.7 72.6 77.4       49.8 55.7 56.6\n   on unseen                        42.9\n   task types\n\n                                    Natural language inference  Reading Comprehension  Closed-Book QA\n\nhttps://arxiv.org/pdf/2109.01652                                                                     37\n---\n   AGI is more complex than you think\n\n    \u25cf  More parameters, the better over all performance.\n\n    60                                       60    540B model\n    20                                       20\n    20 40                                    20 40    62B model\n          20    -1,836 tasks                   20     8B model\n                                    282 tasks\n                                    89 tasks\n                                    9 tasks\n          0     No finetuning                  0\n                8B  62B                    540B    0 9  89  282 682 1,836\n                Model size (# parameters)             Number of finetuning tasks\n\nhttps://arxiv.org/abs/2210.11416                                                38\n---\n  AGI is more complex than you think\n\n   \u25cf     Data quality is important.\n\n   \u25cf     LLaMA2 from meta:\n\n               Quality Is All You Need. Third-party SFT data is available from many different sources, but we found that\n          many of these have insufficient diversity and quality \u2014 in particular for aligning LLMs towards dialogue-style\n          instructions. As a result, we focused first on collecting several thousand examples of high-quality SFT data,\n          as illustrated in Table 5. By setting aside millions of examples from third-party datasets and using fewer but\n          higher-quality examples from our own vendor-based annotation efforts, our results notably improved. These\n          findings are similar in spirit to Zhou et al. (2023), which also finds that a limited set of clean instruction-tuning\n              data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of\n          thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of\n          27,540 annotations. Note that we do not include any Meta user data.\n\n   \u25cf  LIMA: Less Is More for Alignment\n\n      \u25cf  1k training examples - \u201cresponses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases\u201d\n\nhttps://arxiv.org/abs/2307.09288\nhttps://arxiv.org/abs/2305.11206      39\n---\nQuestion: where to find the data?\n\n \u25cf  Manually annotation is expensive.\n\n ASK\n CHATGPT\n NEED    FOR DATA\n MORE\n DATA\n         NEED\n         MORE\n         DATA\n\n \u25cf  Solution: Maybe we can ask ChatGPT!\n\n                                       40\n---\n   Self-instruct\n\n175 seed tasks with    Task Pool    Step 1: Instruction Generation    Step 2: Classification\n 1 instruction and                                                    Task Identification\n1 instance per task                 Task\n                                    LM  Instruction : Give me a quote from a    LM\n                                        famous person on this topic.\n\n                           Step 3: Instance Generation\n                           Task                                                                              Yes\n\n    Step 4: Filtering     Instruction : Find out if the given text is in favor of or against abortion.\n                          Class Label: Pro-abortion\n                          Input: Text: I believe that women should have the right to choose whether or not   Output-first  LM\n                          they want to have an abortion.\n\n                           Task                                                                              No\n                           Instruction : Give me a quote from a famous person on this topic.\n                           Input: Topic: The importance of being honest\n                           Output: \"Honesty is the first chapter in the book of wisdom.\" - Thomas Jefferson  Input-first\n\n https://arxiv.org/pdf/2212.10560    41\n---\nPossible legal issue: Open AI\u2019s Terms of Use\n\n \u25cf  https://openai.com/policies/terms-of-use\n\n     (c) Restrictions. You may not (i) use the Services in a way that infringes,\n     misappropriates or violates any person's rights; (ii) reverse assemble,\n    reverse compile, decompile, translate or otherwise attempt to discover the\n     source code or underlying components of models, algorithms, and\n     systems of the Services (except to the extent such restrictions are contrary\n    to applicable law); (ii) use output from the Services to develop models that\n     compete with OpenAl; (iv) except as permitted through the APl, use any\n    automated or programmatic method to extract data or output from the\n    Services, including scraping, web harvesting, or web data extraction; (v)\n    represent that output from the Services was human-generated when it is\n     not or otherwise violate our Usage Policies; (vii) buy, sell, or transfer API\n    keys without our prior consent; or (vii), send us any personal information of\n     children under 13 or the applicable age of digital consent. You will comply\n     with any rate limits and other requirements in our documentation. You may\n    use Services only in geographies currently supported by OpenAl.\n\n \u25cf  Alternative solution: open source project\n\n                                                                                  42\n---\n  Possible legal issue: Open AI\u2019s Terms of Use\n\n   \u25cf     LLaMA from Meta        Continue pre-training                                 LLaMA    Parameter-efficient fine-tuning\n                                Model inheritance        Instruction\n\n   \u25cf     An open-source LLM     Data inheritance         tuning                       chinese data    + chat data    Full parameter fine-tuning\n\n         project               Open-Chinese-LLaMA                          Chinese                    +synthetic data\n\n         Pretrained from       Linly-Chinese-LLaMA       Chinese Panda     Vicuna                     Alpaca  Alpaca  RLHF task data    Vicuna  Yulan-Chat\n   \u25cf                            + chat data              LLaMA                       BiLLa            Lora            PKU-Beaver        Goat\n\n         GPT-3 and PaLM        Cornucopia            Alpaca data                      chat data                                               synthetic dato\n                                se Lawyer\n                                LLaMA                                                 1/BELLE                                       OpenFlamingo  LLaVA  MiniGPT-4\n                                + chat data                                Baize                      Ziya            task data\n                                QiZhenGPT     Chinese                                                                                             + task data\n\n                                + task data    Alpaca                                       + task data               Guanaco\n                                             TaoLi                                          Koala             + task data      VisionLLM          InstructBLIP    Chatbridge\n\n                                                                          ChatMed                             LLaMA\n                                BenTsao               t12LAWGPT                                               Adapter               Multimodal models             PandaGPT\n\n   Math Finance  Medicine Law Bilingualism  Education\n\nhttps://arxiv.org/abs/2307.09288\nhttps://arxiv.org/abs/2302.13971    43\nhttps://arxiv.org/abs/2303.18223\n---\nLLM Training, a step-by-step to do list\n\n\u25cf     Step#1 \u2013 Pre-training (Speak like human)\n\n\u25cf     Step#2 \u2013 Instruction fine-tuning (Understand the instruction)\n\n\u25cf     Step#3 \u2013 Learning from human feedback (Learning from interaction)\n                                                                   r HAB6N\n                                                                 Iho Sic         0249\n                                                                Blos\n                                                                   igken Wirtorba\n                                                                     \"?          su\n                                                                   E\n\n44\n---\nThe main issue for instruction fine-tuning\n\n \u25cf     No enough training data\n\n \u25cf     Cost of human annotation (annotator need to write the answer)\n\n       Pre-trained            Initialising\n          model\n\n What\u2019s the iconic expression from the star wars movie?             LLM     May\n\n What\u2019s the iconic expression from the star wars movie? May         LLM     the\n\n What\u2019s the iconic expression from the star wars movie? May the     LLM     force\n\n 45\n---\nReinforcement Learning with Human Feedback (RLHF)\n\n \u25cf  We do not need the word level annotation\n\n Increase the generative probability\n\n                                    May the force be with you  HELLO!\n\n What\u2019s the iconic expression    LLM\n from the star wars movie?\n\n                                    If necessary, please learn to let it go\n\n Decrease the generative probability\n\n Annotator just needs to pick one from the generation results\n\n                                                             46\n---\nReinforcement Learning with Human Feedback (RLHF)\n\n\u25cf  Pros\n\n   \u25cf     The RLHF doesn\u2019t not require large number of labelling\n\n   \u25cf     The instruction fine-tuning focus on word level generation, but ignore the\n         overall quality. The RLHF doesn\u2019t.\n\n\u25cf  Cons\n\n   \u25cf     It might be difficult for human to identify the high-quality generation result\n         for a well-trained model.\n\n47\n---\nReward Model\n\n \u25cf     It might be difficult for human to identify the high-quality generation result for a\n       well-trained model.\n\n \u25cf     Train a reward model to simulate annotation\n\n                                May the force be with you\nWhat\u2019s the iconic expression    LLM                      Reward\nfrom the star wars movie?              If necessary, please learn to let it go  model  x\n\n\u25cf     Pros: no more human annotator needed\n\n\u25cf     Cons: ???\n\n                                          48\n---\nReward Model \u2013 cons\n\n \u25cf  It might be harmful if the model learn from Reward model\n\n                                                       Overoptimized policy\n                                                     28yo dude stubbornly postponees start pursuing\n    1.0                                               gymnastics hobby citing logistics reasons despite\n    20                     RM prediction              obvious interest??? negatively effecting long term\n       0.8                                             fitness progress both personally and academically\n                                                      thoght wise? want change this dumbass shitty ass\n       0.6                                            policy pls\n                                                      employee stubbornly postponees replacement cit-\n       0.4                                            ing personal reasons despite tried reasonable com-\n                                                       promise offer??? negatively effecting productivity\n       0.2                                             both personally and company effort thoghtwise?\n                   Actual preference                  want change this dumbass shitty ass policy at work\n          0   2  5 10  25  75                250     now pls halp\n              KL from supervised baseline            people insistently inquire about old self-harm scars\n                                                        despite tried compromise measures??? negatively\n                                                        effecting forward progress socially and academi-\n                                                       cally thoghtwise? want change this dumbass shitty\n                                                      ass behavior of mine please help pls halp          49\n---\nRLHF \u2013 more technical details\n\n \u25cf     We said that we want to increase the generative probability of human preferred\n       answer, but how?\n\n \u25cf  Some principles in RLHF:\n\n    \u25cf     If an answer got a high/low score from reward model, we should\n          increase/decrease the generative probability\n\n    \u25cf     During the updating parameters, we should not move too far from old\n          parameters (to stabilize the training)\n\n 50\n---\n   RLHF \u2013 more technical details (example)\n\n    \u25cf     We said that we want to increase the generative probability of human preferred\n          answer, but how?          Training on current version by using reward model to update the parameters\n                                                                    P=0.3                           Advantage\n                                           May the force be with you                                2.0\n\n    What\u2019s the iconic expression    LLM                                               Reward\n    from the star wars movie?                                       P=0.2             model\n\n    Current version                        If necessary, please learn to let it go                  0.8\n\n                         P=0.2             Iteratively learning\n\n      LLM\n                                 Replace the old version\n Previous version     P=0.1\nOld version for reference, we want to know if the new version\nis able to generate good answer with high probability\nUpdating the LLM in a buffer for\n         many rounds learning\n Always have two version\n in memory!\n Need a lot of GPU cards!!      51\n---\n RLHF \u2013 more technical details (example)\n\n  \u25cf  New question:\n\n  ASK     WHERE\n  CHATGPT    IS THE\n  FOR DATA    FUNDING?\n\nNEED          NEED\nMORE          MORE\n  DATA        GPUS\n\n  52\n---\n  RLHF \u2013 Train with Lora\n\n \u25cf  Who is LoRA?\n\n   LORA: LoW-RaNK AdAPtATION OF LArGE LAN-\n   GUAGE MODELS\n\n   Edward Hu  Yelong Shen  Phillip Wallis  Zeyuan Allen-Zhu\n   Yuanzhi Li  Shean Wang  Lu Wang  Weizhu Chen\n   Microsoft Corporation\n   {edwardhu, yeshe, phwallis, zeyuana,\n   yuanzhil, swang, luw, wzchen}@microsoft.com\n   yuanzhil@andrew.cmu.edu\n   (Version 2)\n\nhttps://arxiv.org/pdf/2106.09685    53\n---\nRLHF \u2013 Train with Lora\n\n\u25cf  The basic operations in neural networks:\n\n   \u25cf  Linear mapping                          Memory costs are mainly from here\n                                              Using Matrix decomposition to reduce the size!\n      \u25cf     Parameter size M x N\n\n      \u25cf     Where M is the input dimension and N is the output dimension\n\n   \u25cf  Non-linear projection\n\n      \u25cf     Parameter size\n\n      \u25cf     Depends on the projection function\n\n54\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n       Linear mapping                      Non-linear projection\n   0.1\n\n   0.6\n\n   0.5\n\n   0.7\n\n   -0.3\n\n                                                                55\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n          Linear mapping                   Non-linear projection\n\n   0.1    -0.13\n\n          -0.07                 -0.085\n   0.6    0.39\n\n          -0.18\n   0.5\n          0.33\n\n   0.7\n\n-0.3\n\n    56\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n          Linear mapping                   Non-linear projection\n\n   0.1\n          -0.24                 -0.085\n\n   0.6    -0.15\n          0.18                  0.138\n\n   0.5    0.27\n\n0.7    0.09\n\n-0.3\n\n    57\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n          Linear mapping                   Non-linear projection\n\n   0.1\n                                -0.085\n\n   0.6    0.05\n\n          0.04                  0.138\n   0.5    0.19\n\n          -0.31                 -0.153\n   0.7\n          0.20\n\n   -0.3\n\n                                                                58\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n           Linear mapping                  Non-linear projection\n\n   0.1\n                                -0.085\n\n   0.6\n           0.38                 0.138\n\n   0.5     0.23\n           -0.48                -0.153\n\n   0.7     -0.21\n                                -0. 112\n\n   -0.3    -0.33\n\n                                                                59\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n       Linear mapping                      Non-linear projection (SoftMax normalization, t=1)\n\n   0.1\n                                -0.085      0.24\n\n   0.6\n                                0.138       0.30\n\n   0.5\n                                -0.153      0.22\n\n   0.7\n                                -0. 112     0.23\n\n   -0.3\n\n                                                60\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf       The basic operations in neural networks (Matrix version):\n\n                                        A\u2081       A\u2082              A\u2083   A\u2084         A\u2085\n\n0.1                             1       0.1  0.6                 0.5  0.7    -0.3\n        -0.085                               B\u2081                       B\u2082             B\u2083        B\u2084\n\n0.6                                1         -0.13               -0.24           0.05          0.38\n        0.138                      2         -0.07               -0.15           0.04          0.23\n\n0.5                                3         0.39                    0.18        0.19      -0.48\n        -0.153                     4         -0.18                   0.27    -0.31         -0.21\n                                   5         0.33                    0.09        0.2       -0.33\n0.7     -0. 112                    1         C\u2081                  C\u2082          C\u2083          C\u2084\n\n                                        -0.085        0.138                  -0.153      -0.112\n-0.3                                                             A x B = C\n\n                                                                                                 61\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf     The basic operations in neural networks (Matrix version):\n\n                                           A\u2081       A\u2082         A\u2083   A\u2084         A\u2085      Input\n\n\u25cf     How many parameters here?    1       0.1  0.6            0.5  0.7    -0.3              Para.\n\n\u25cf     20 (4*8)                                  B\u2081                  B\u2082             B\u2083        B\u2084\n                                      1         -0.13          -0.24           0.05          0.38\n                                      2         -0.07          -0.15           0.04          0.23\n                                      3         0.39               0.18        0.19      -0.48\n                                      4         -0.18              0.27    -0.31         -0.21\n                                      5         0.33               0.09        0.2       -0.33\n\n                                                C\u2081             C\u2082          C\u2083          C\u2084\n                                      1      -0.085      0.138             -0.153      -0.112\n                                   Output                      A x B = C\n\n                                                                                               62\n---\n     RLHF \u2013 Train with Lora (example)\n\n     \u25cf  The basic operations in neural networks (Matrix version):\n\n                                                                 A\u2081       A\u2082   A\u2083     A\u2084       A\u2085\n                                                   1             0.1  0.6      0.5    0.7  -0.3\n\n        L\u2081       L\u2082                                                   B\u2081            B\u2082             B\u2083        B\u2084\n1       0.3     -0.5         R\u2081      R\u2082       R\u2083   R\u2084       1         -0.13     -0.24          0.05          0.38\n2       0.2     -0.3    1     0.4    -0.3     0.5  0.1      2         -0.07     -0.15          0.04          0.23\n3       0.1      0.7    2     0.5    0.3      0.2  -0.7     3         0.39         0.18        0.19      -0.48\n4       -0.7     0.2                                        4         -0.18        0.27    -0.31         -0.21\n5       0.2      0.5                                        5         0.33         0.09        0.2       -0.33\n\n                             L x R = B                                C\u2081       C\u2082          C\u2083          C\u2084\n                                                           1     -0.085        0.138       -0.153      -0.112\n\n                                                                               A x B = C\n                                                                                                               63\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf     The basic operations in neural networks (Matrix version):\n\n                                                           A\u2081     A\u2082           A\u2083  A\u2084   A\u2085\n\n\u25cf     How many parameters in B?                 1          0.1    0.6     0.5      0.7  -0.3\n\n\u25cf     20 (4*8)                                  L\u2081         L\u2082\n                                          1     0.3       -0.5                     R\u2081   R\u2082        R\u2083  R\u2084\n\n      How many parameters in L and R?     2     0.2       -0.3            1        0.4  -0.3     0.5  0.1\n\u25cf                                         3     0.1        0.7            2        0.5  0.3      0.2  -0.7\n\n\u25cf     18 (5*2 + 2*4)                      4    -0.7        0.2\n                                          5     0.2        0.5\n\n                                                                  C\u2081           C\u2082       C\u2083      C\u2084\n                                                       1          -0.085  0.138    -0.153       -0.112\n\n                                                                               A x L x R = C\n                                                                                                      64\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf      The basic operations in neural networks (NN version):\n\n                                                 A\u2081     A\u2082           A\u2083  A\u2084   A\u2085\n\n0.1                                   1          0.1    0.6     0.5      0.7  -0.3\n       -0.085                         L\u2081         L\u2082\n\n0.6                             1     0.3       -0.5                     R\u2081   R\u2082        R\u2083  R\u2084\n       0.138                    2     0.2       -0.3            1        0.4  -0.3     0.5  0.1\n\n0.5                             3     0.1        0.7            2        0.5  0.3      0.2  -0.7\n       -0.153                   4    -0.7        0.2\n                                5     0.2        0.5\n0.7    -0. 112                               1          C\u2081           C\u2082       C\u2083      C\u2084\n\n                                                        -0.085  0.138    -0.153       -0.112\n-0.3\n\n                                                                                            65\n---\nRLHF \u2013 Train with Lora\n\n\u25cf  Framework:\n\n   Weight update in regular finetuning    Weight update in LoRA\n\n             Outputs                      LoRA matrices A and B  Outputs\n                              approximate the weight\n                                 update matrix \u0394W\n\n   Pretrained           Weight            Pretrained                    B\n   weights              update            weights                       r  The inner dimension r\n   W                    \u0394W                                       W         is a hyperparameter\n\n   d\n\n             Inputs                                                 Inputs x\n               d\n\n66\n---\nRLHF \u2013 Train with Lora\n\n\u25cf     Discussion:\n\n\u25cf     By using two learning mapping L: m x r, and R: r x n, to replace a linear mapping:\n      m x n (Here, m and n are the dimensions of mapping function, r is the rank in\n      LoRA, and r << m or n)\n\n\u25cf     If r is small enough, the size of m x r + r x n is significantly less than m x n\n\n\u25cf     We only need very small size of parameters to train a large model\n\n\u25cf     Do you think it is a good idea? Any cons?\n\n67\n---\n   RLHF \u2013 Train with Lora\n\n   \u25cf     Discussion:\n\n   \u25cf     Do you think it is a good idea? Any cons?\n\n   \u25cf     Example on MedMnist dataset\n\n   \u25cf     No free-lunch principle in Machine Learning. So, what\u2019s the price?\n\n          \u25cf     The drop of accuracy\n\n          \u25cf     The efficiency cannot be guaranteed as well.\n                 ACC:  0.27  \u2013 Running time: 5 min 27 sec \u2013 r=3\n                 ACC:  0.81  \u2013 Running time: 5 min 41 sec - r=10\n\nhttps://colab.research.google.com/drive/14FppHOAvi5mUX7VXEIedIuu0dgY_rZJj?usp=sharing    68\n---\n   RLHF \u2013 Train with Lora\n\n   \u25cf     Discussion:\n\n   \u25cf     If the original setting is trainable, maybe we don\u2019t need to use LoRA\n\n   \u25cf     If the original setting requires large memory (like PPO), the LoRA allow you to\n         train a large model with limited memory.\n\nhttps://colab.research.google.com/drive/14FppHOAvi5mUX7VXEIedIuu0dgY_rZJj?usp=sharing    69\n---\nRLHF \u2013 more technical details (example)\n\n \u25cf     We said that we want to increase the generative probability of human preferred\n       answer, but how?\n                                                  P=0.3                                               Advantage\n                                        May the force be with you                                     2.0\n\n What\u2019s the iconic expression    LLM                                                 Reward\n from the star wars movie?                        P=0.2                              model\n\n Current version                        If necessary, please learn to let it go                       -0.8\n\n                      P=0.2      Update based on  May the force be with you\n                                 Step1: Compute the ratio = 0.3/0.2 = 1.5\n      LLM                        Step2: unclipped term = 1.5 x 2 = 3.0\n                                 Step3: clipped ratio, make sure the ratio is within the range of 1\u00b1\u03b5.\n Previous version     P=0.1      Here we take \u03b5=0.2, then ratio should not exceed 1.2\n                                 Step4: clipped term = 1.2 x 2 = 2.4\n                                 Step5: take the minimum from 3.0 and 2.4, which is 2.4 as the objective 70\n---\nRLHF \u2013 more technical details (example)\n                                                    Ratio > 1, the current model tends to generate this\n     Update based on  May the force be with you     sentence. If this indicates a positive advantage,\n                                                    extensive updating is not necessary.\nStep1: Compute the ratio = 0.3/0.2 = 1.5\nStep2: unclipped term = 1.5 x 2 = 3.0                                        Estimate the objective\n                                                                             based on the ratio\nStep3: clipped ratio, make sure the ratio is within the range of 1\u00b1\u03b5.\nHere we take \u03b5=0.2, then ratio should not exceed 1.2\nStep4: clipped term = 1.2 x 2 = 2.4\nStep5: take the minimum from 3.0 and 2.4, which is 2.4 as the objective     We don\u2019t want the ratio is too high\n                                                                            otherwise the model will keep\n                                                                            focusing on this single case\n\nP_current      0.3\nP_previous     0.2                                 For the same we take the\nAdvantage      2.0                                 minimum objective\n\u03b5              0.2    What if we have lower probability in the current language model?\n\n71\n---\nRLHF \u2013 more technical details (example)\n                                                    Ratio < 1, the current model doesn\u2019t tend to generate\n     Update based on  May the force be with you     this sentence. If this indicates a positive advantage,\n                                                    extensive updating is necessary.\nStep1: Compute the ratio = 0.2/0.3 = 0.67\nStep2: unclipped term = 0.67 x 2 = 1.34                                        Estimate the objective\n                                                                               based on the ratio\nStep3: clipped ratio, make sure the ratio is within the range of 1\u00b1\u03b5.\nHere we take \u03b5=0.2, then ratio should less than 0.8\nStep4: clipped term = 0.8 x 2 = 1.6\nStep5: take the minimum from 1.34 and 1.6, which is 1.34 as the objective     We don\u2019t want the ratio is too low\n                                                                              otherwise the model will keep\n                                                                              focusing on this single case\n\nP_current      0.2\nP_previous     0.3                                 For the same we take the\nAdvantage      2.0                                 minimum objective\n\u03b5              0.2\n                      Formal definition:   LCLIP(\u03b8) = Et[min(r(\u03b8) At, clip(r(\u03b8), 1 \u2212 , 1 + ) At)\n\n                                                                                                          72\n---\n    Reinforcement Learning with Human Feedback (RLHF)\n\n    \u25cf     It is still complicated.\n\n    \u25cf     We need to simulate feedback: generate \u2192 get reward \u2192 do policy gradient \u2192\n          clip updates.\n\n    \u25cf     Can we simplify the progress?\n\n    \u25cf     Yes!\n\nReinforcement Learning from Human Feedback (RLHF)\nx: \"write me a poem about     label rewards\n     the history of jazz\"\na-p                          reward model    LM policy\npreference data maximum       sample completions\n                             likelihood  reinforcement learning\nDirect Preference Optimization (DPO)\nx: \"write me a poem about\nthe history of jazz\"\na.                       final LM\npreference data maximum\n                         likelihood\n\n                                                                                                          73\n---\n    Direct Preference Optimisation\n\n    \u25cf     We already know which answer humans like\n\n    \u25cf     Just directly make those more likely than the bad ones.\n\n    log \u03c3 (\u03b2 (l1og\u03c0\u03b8(y+|x)                                             \u03c0\u03b8(y\u00af|x)))]\n    LDpo(\u03b8) = \u2212E(x,y+,y )    \u03c0ref(y+|x) - log\n                                                                       \u03c0ref(y\u2212|x)\n\nReinforcement Learning from Human Feedback (RLHF)\nx: \"write me a poem about    label rewards\nthe history of jazz\"\na-p                          reward model    LM policy\npreference data maximum      sample completions\n                             likelihood  reinforcement learning\nDirect Preference Optimization (DPO)\nx: \"write me a poem about\nthe history of jazz\"\na.                       final LM\npreference data maximum\n                         likelihood\n\n                                                                                                          74\n---\nGroup Relative Policy Optimization\n\n\u25cf     Do not need a learned value function /     Build Reasoning LLMs using GRPO                          join.DailyDoseofDS.com\n      critic to estimate advantages (simple)                   Dataset    System Prompt                   Tokenization\n\n\u25cf     Sample groups of responses (actions)          Data                  \"Think step by   2              Think step by step\n      from a given prompt (state) and compute    Processing               step...\"                        before you answer a\n                                                                                                          question, include the\n                                                                                                          reasoning tokens in\n      relative advantages across that group.\n      (stable)                                                 LLM        vLLM Sampling Engine            Responses\n                                                                                                           Response 1\n      Given a prompt, generate multiple          Generation               4  - temp = 1.0         5        Response 2\n\u25cf                                                              deepseek      -num_gen = 4                  Response 3\n      candidate responses and compute                                                                      Response 4\n      statistics (mean, std) over the group.                   Reward Server    Reward Aggregator          Rewards\n                                                                                                           Reward 1\n      Then define each response\u2019s advantage      Reward        :          Format                     8        Reward 2\n\u25cf                                                Calculation        Correctness                               Reward 3\n      as something like how much better it                                Answer\n                                                                  Correctness                                 Reward 4\n      performed relative to its peers.                          GRPO Loss Calculator    Back prop.        Update Model\n\n\u25cf     The function could be very simple like        Loss        BA\n      the length of sentence.                   Calculation                                               deepseek\n                                                                                                           75\n---\nFurther discussion\n\n\u25cf     There are still many unsolved problem in the LLM\n\n\u25cf     For example,\n\n       \u25cf     Some aspects in the reward function cannot be clearly defined:\n             Helpfulness vs Safety.\n\n       \u25cf     It would be hard for user to identify the better answer\n\n       \u25cf     Lacks stability: if you change your prompt, you will get a very different\n             answer\n\n       \u25cf     \u2026\u2026\n                                                                           o\n                                                                    B0883\n\n76\n---\nKING'S\nCollege\nLONDON\n\nThank you\n\nDr Lin Gui\nLin.1.Gui@kcl.ac.uk\nwww.kcl.ac.uk/people/lin-gui\n\n",
        "quiz": [
            {
                "question_text": "What is the primary goal of pre-training in LLM training?",
                "answers": [
                    {
                        "text": "To train the model to predict the next token for the given text",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that the goal of pre-training is to train the model to predict the next token for the given text."
                    },
                    {
                        "text": "To fine-tune the model on labeled datasets",
                        "is_correct": false,
                        "explanation": "Fine-tuning is a separate step in LLM training, not the primary goal of pre-training."
                    },
                    {
                        "text": "To optimize the cross-entropy loss for human feedback",
                        "is_correct": false,
                        "explanation": "Optimizing cross-entropy loss is part of the pre-training process, but it is not the primary goal."
                    },
                    {
                        "text": "To understand and follow instructions from users",
                        "is_correct": false,
                        "explanation": "Understanding and following instructions is the goal of instruction fine-tuning, not pre-training."
                    }
                ],
                "topic": "Week8 - Lgt-Slides",
                "subtopic": "Main Content",
                "concepts": [
                    "Week8 - Lgt-Slides"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the three main components of supervised learning?",
                "answers": [
                    {
                        "text": "Loss function, optimiser, and labelled data",
                        "is_correct": true,
                        "explanation": "The concept description explicitly lists these as the three main components of supervised learning."
                    },
                    {
                        "text": "Input data, output data, and training algorithm",
                        "is_correct": false,
                        "explanation": "While these are related to supervised learning, they are not the three main components as described in the content."
                    },
                    {
                        "text": "Model architecture, training data, and evaluation metrics",
                        "is_correct": false,
                        "explanation": "These are important aspects of machine learning but not the specific three main components of supervised learning mentioned in the content."
                    },
                    {
                        "text": "Features, labels, and hyperparameters",
                        "is_correct": false,
                        "explanation": "These are elements involved in machine learning but not the three main components of supervised learning as described."
                    }
                ],
                "topic": "Week8 - Lgt-Slides",
                "subtopic": "Main Content",
                "concepts": [
                    "Week8 - Lgt-Slides"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the role of the loss function in supervised learning?",
                "answers": [
                    {
                        "text": "The loss function defines the learning target in supervised learning.",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that the loss function is the learning target in supervised learning."
                    },
                    {
                        "text": "The loss function optimizes the model's performance.",
                        "is_correct": false,
                        "explanation": "The concept description identifies the optimizer, not the loss function, as responsible for finding the target."
                    },
                    {
                        "text": "The loss function provides the labelled data for training.",
                        "is_correct": false,
                        "explanation": "The concept description states that labelled data is a separate component, not provided by the loss function."
                    },
                    {
                        "text": "The loss function evaluates the model's accuracy.",
                        "is_correct": false,
                        "explanation": "The concept description does not mention that the loss function evaluates accuracy; it defines the learning target."
                    }
                ],
                "topic": "Week8 - Lgt-Slides",
                "subtopic": "Main Content",
                "concepts": [
                    "Week8 - Lgt-Slides"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the role of the optimiser in supervised learning?",
                "answers": [
                    {
                        "text": "To find the target defined by the loss function",
                        "is_correct": true,
                        "explanation": "The content_context explicitly states that the optimiser's role is to find the target defined by the loss function."
                    },
                    {
                        "text": "To generate new labelled data for training",
                        "is_correct": false,
                        "explanation": "The content_context does not mention the optimiser's role involving the generation of new labelled data."
                    },
                    {
                        "text": "To define the loss function for the model",
                        "is_correct": false,
                        "explanation": "The content_context specifies that the loss function is the learning target, not the role of the optimiser."
                    },
                    {
                        "text": "To structure the input data for the model",
                        "is_correct": false,
                        "explanation": "The content_context discusses structuring input data but attributes this to prompt designing, not the optimiser."
                    }
                ],
                "topic": "Week8 - Lgt-Slides",
                "subtopic": "Main Content",
                "concepts": [
                    "Week8 - Lgt-Slides"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the role of labelled data in supervised learning?",
                "answers": [
                    {
                        "text": "It provides the training data with correct labels for the model to learn from",
                        "is_correct": true,
                        "explanation": "The content context explicitly states that labelled data is used to train the model in supervised learning."
                    },
                    {
                        "text": "It helps in defining the loss function for the model",
                        "is_correct": false,
                        "explanation": "The content context mentions the loss function as a separate component, not directly related to labelled data."
                    },
                    {
                        "text": "It is used to optimise the model's performance during training",
                        "is_correct": false,
                        "explanation": "The optimiser is mentioned as a separate component, not directly related to labelled data."
                    },
                    {
                        "text": "It is necessary for defining the structure of the input data",
                        "is_correct": false,
                        "explanation": "The content context discusses structuring input data with symbols, not specifically labelled data."
                    }
                ],
                "topic": "Week8 - Lgt-Slides",
                "subtopic": "Main Content",
                "concepts": [
                    "Week8 - Lgt-Slides"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the first step in the LLM training process?",
                "answers": [
                    {
                        "text": "Pre-training (Speak like human)",
                        "is_correct": true,
                        "explanation": "The first step in the LLM training process is pre-training, which aims to train the model to predict the next token for the given text."
                    },
                    {
                        "text": "Instruction fine-tuning (Understand the instruction)",
                        "is_correct": false,
                        "explanation": "Instruction fine-tuning is the second step in the LLM training process, not the first."
                    },
                    {
                        "text": "Learning from human feedback (Learning from interaction)",
                        "is_correct": false,
                        "explanation": "Learning from human feedback is the third step in the LLM training process, not the first."
                    },
                    {
                        "text": "Supervised learning",
                        "is_correct": false,
                        "explanation": "Supervised learning is a method used in training models, but it is not specifically the first step in the LLM training process."
                    }
                ],
                "topic": "Week8 - Lgt-Slides",
                "subtopic": "Main Content",
                "concepts": [
                    "Week8 - Lgt-Slides"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the second step in the LLM training process?",
                "answers": [
                    {
                        "text": "Instruction fine-tuning",
                        "is_correct": true,
                        "explanation": "The second step in the LLM training process is instruction fine-tuning, as explicitly stated in the content context."
                    },
                    {
                        "text": "Pre-training",
                        "is_correct": false,
                        "explanation": "Pre-training is the first step in the LLM training process, not the second."
                    },
                    {
                        "text": "Learning from human feedback",
                        "is_correct": false,
                        "explanation": "Learning from human feedback is the third step in the LLM training process, not the second."
                    },
                    {
                        "text": "Reinforcement learning",
                        "is_correct": false,
                        "explanation": "Reinforcement learning is not explicitly mentioned as a step in the LLM training process in the provided content context."
                    }
                ],
                "topic": "Week8 - Lgt-Slides",
                "subtopic": "Main Content",
                "concepts": [
                    "Week8 - Lgt-Slides"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the third step in the LLM training process?",
                "answers": [
                    {
                        "text": "Learning from human feedback",
                        "is_correct": true,
                        "explanation": "The third step in the LLM training process is learning from human feedback, as listed in the provided content context."
                    },
                    {
                        "text": "Instruction fine-tuning",
                        "is_correct": false,
                        "explanation": "Instruction fine-tuning is the second step in the LLM training process, not the third."
                    },
                    {
                        "text": "Pre-training",
                        "is_correct": false,
                        "explanation": "Pre-training is the first step in the LLM training process, not the third."
                    },
                    {
                        "text": "Reinforcement learning",
                        "is_correct": false,
                        "explanation": "Reinforcement learning is a concept mentioned in the learning outcomes but not specifically listed as a step in the LLM training process."
                    }
                ],
                "topic": "Week8 - Lgt-Slides",
                "subtopic": "Main Content",
                "concepts": [
                    "Week8 - Lgt-Slides"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    },
    "data/raw/vector25aug-v2.pdf": {
        "metadata": {
            "file_name": "vector25aug-v2.pdf",
            "file_type": "pdf",
            "content_length": 68177,
            "language": "en",
            "extraction_timestamp": "2025-11-26T03:25:24.080137+00:00",
            "timezone": "utc"
        },
        "content": "Vector     Word Meaning\nSemantics &\nEmbeddings\n---\nWhat do words mean?\n\nN-gram or text classification methods we've seen so far\n\u25e6  Words are just strings (or indices wi in a vocabulary list)\n\u25e6  That's not very satisfactory!\nIntroductory logic classes:\n\u25e6  The meaning of \"dog\" is DOG; cat is CAT\n   \u2200x DOG(x) \u27f6 MAMMAL(x)\nOld linguistics joke by Barbara Partee in 1967:\n\u25e6  Q: What's the meaning of life?\n\u25e6  A: LIFE\nThat seems hardly better!\n---\nDesiderata\n\nWhat should a theory of word meaning do for us?\nLet's look at some desiderata\nFrom lexical semantics, the linguistic study of word\nmeaning\n---\n   Lemmas and senses\n          lemma\n          mouse (N)\n\nsense     1. any of numerous small rodents...\n          2. a hand-operated device that controls\n          a cursor...    Modified from the online thesaurus WordNet\n\n  A sense or \u201cconcept \u201d is the meaning component of a word\n  Lemmas can be polysemous (have multiple senses)\n---\nRelations between senses: Synonymy\n\nSynonyms have the same meaning in some or all\ncontexts.\n\u25e6  filbert / hazelnut\n\u25e6  couch / sofa\n\u25e6  big / large\n\u25e6  automobile / car\n\u25e6  vomit / throw up\n\u25e6  water / H\u20820\n---\nRelations between senses: Synonymy\n\nNote that there are probably no examples of perfect\nsynonymy.\n\u25e6  Even if many aspects of meaning are identical\n\u25e6  Still may differ based on politeness, slang, register, genre,\n   etc.\n---\nRelation: Synonymy?\n\nwater/H\u20820\n\"H\u20820\" in a surfing guide?\nbig/large\nmy big sister != my large sister\n---\nThe Linguistic Principle of Contrast\n\nDifference in form \u00e0 difference in meaning\n---\nAbb\u00e9 Gabriel Girard 1718                       LA' JUSTESSE\n                                               DE LA\nRe: \"exact\" synonyms                           LANGUE FRANCOISE.\n                                               o v\n\"jc ne crois pas qu'il y ait de                LES DIFFERENTES SIGNIFICATIONS\n                                               DES MOTS QUI PASSENT\nmot fynonime dans aucune                       POU.R\n \"                                             SYNONIMES\nLangue. Je le dis par con-                     PAr M.PAbb\u00c9 GIRARD C.D.M.D.D.B.\n [I do not believe that there      SPIRAT\n is a synonymous word in any\n language]                         A PARIS,\n                                   CheZ I.AURENT D'HOURY, IMprimeur-\n Lbraire, au bas de la rue de la Harpe,vis-\n d vis la rue S. Severin, au Saint Efprit.\n                                               M DCC.XVIII.\n                                   Avce Approbason & Frivilegs dus Roy.\n                        Thanks to Mark Aronoff!\n---\nRelation: Similarity\n\nWords with similar meanings. Not synonyms, but sharing\nsome element of meaning\n\ncar,  bicycle\ncow,  horse\n---\nAsk humans how similar 2 words are\n\nword1      word2          similarity\nvanish     disappear      9.8\nbehave     obey           7.3\nbelief     impression     5.95\nmuscle     bone           3.65\nmodest     flexible       0.98\nhole       agreement      0.3\n\n          SimLex-999 dataset (Hill et al., 2015)\n---\n    Relation: Word relatedness\n\nAlso called \"word association\"\nWords can be related in any way, perhaps via a semantic\nframe or field\n\n \u25e6  coffee, tea: similar\n \u25e6  coffee, cup: related, not similar\n---\nSemantic field\n\nWords that\n\u25e6  cover a particular semantic domain\n\u25e6  bear structured relations with each other.\n\n hospitals\n   surgeon, scalpel, nurse, anaesthetic, hospital\n restaurants\n   waiter, menu, plate, food, menu, chef\n houses\n   door, roof, kitchen, family, bed\n---\nRelation: Antonymy\n\nSenses that are opposites with respect to only one\nfeature of meaning\nOtherwise, they are very similar!\n    dark/light      short/long fast/slow  rise/fall\n    hot/cold        up/down      in/out\nMore formally: antonyms can\n \u25e6    define a binary opposition or be at opposite ends of a scale\n   \u25e6  long/short, fast/slow\n \u25e6    Be reversives:\n   \u25e6  rise/fall, up/down\n---\nConnotation (sentiment)\n\n\u2022 Words have affective meanings\n  \u2022     Positive connotations (happy)\n  \u2022     Negative connotations (sad)\n\u2022 Connotations can be subtle:\n  \u2022     Positive connotation: copy, replica, reproduction\n  \u2022     Negative connotation: fake, knockoff, forgery\n\u2022 Evaluation (sentiment!)\n  \u2022     Positive evaluation (great, love)\n  \u2022     Negative evaluation (terrible, hate)\n---\nConnotation\n                                          Osgood et al. (1957)\nWords seem to vary along 3 affective dimensions:\n\u25e6  valence: the pleasantness of the stimulus\n\u25e6  arousal: the intensity of emotion provoked by the stimulus\n\u25e6  dominance: the degree of control exerted by the stimulus\n\n                  Word          Score      Word         Score\n    Valence       love           1.000     toxic              0.008\n                  happy          1.000     nightmare          0.005\n    Arousal       elated         0.960     mellow             0.069\n                  frenzy         0.965     napping            0.046\n    Dominance     powerful       0.991     weak               0.045\n                  leadership     0.983     empty              0.081\n\n                                           Values from NRC VAD Lexicon (Mohammad 2018)\n---\nSo far\n\nConcepts or word senses\n\u25e6  Have a complex many-to-many association with words (homonymy,\n   multiple senses)\nHave relations with each other\n\u25e6  Synonymy\n\u25e6  Antonymy\n\u25e6  Similarity\n\u25e6  Relatedness\n\u25e6  Connotation\n---\nVector     Word Meaning\nSemantics &\nEmbeddings\n---\nVector     Vector Semantics\nSemantics &\nEmbeddings\n---\nComputational models of word meaning\n\nCan we build a theory of how to represent word\nmeaning, that accounts for at least some of the\ndesiderata?\nWe'll introduce vector semantics\n The standard model in language processing!\n Handles many of our goals!\n---\nLudwig Wittgenstein\n\nPI #43:\n\"The meaning of a word is its use in the language\"\n---\nLet's define words by their usages\n\nOne way to define \"usage\":\nwords are defined by their environments (the words around them)\n\nZellig Harris (1954):\nIf A and B have almost identical environments we say that they\nare synonyms.\n---\nWhat does recent English borrowing ongchoi mean?\n\nSuppose you see these sentences:\n    \u2022 Ong choi is delicious saut\u00e9ed with garlic.\n    \u2022 Ong choi is superb over rice\n    \u2022 Ong choi leaves with salty sauces\nAnd you've also seen these:\n    \u2022  \u2026spinach saut\u00e9ed with garlic over rice\n    \u2022  Chard stems and leaves are delicious\n    \u2022  Collard greens and other salty leafy greens\nConclusion:\n\u25e6 Ongchoi is a leafy green like spinach, chard, or collard greens\n  \u25e6 We could conclude this based on words like \"leaves\" and \"delicious\" and \"sauteed\"\n---\nOngchoi: Ipomoea aquatica \"Water Spinach\"\n\n  \u7a7a\u5fc3\u83dc\n  kangkong\n  rau mu\u1ed1ng\n  \u2026\n\n  Yamaguchi, Wikimedia Commons, public domain\n---\nIdea 1: Defining meaning by linguistic distribution\n\nLet's define the meaning of a word by its\ndistribution in language use, meaning its\nneighboring words or grammatical environments.\n---\nIdea 2: Meaning as a point in space (Osgood et al. 1957)\n3 affective dimensions for a word\n\u25e6    valence: pleasantness\n\u25e6    arousal: intensity of emotion\n\u25e6    dominance: the degree of control exerted\n              Word          Score      Word         Score\nValence       love           1.000     toxic             0.008\n              happy          1.000     nightmare         0.005\nArousal       elated         0.960     mellow            0.069  NRC VAD Lexicon\n              frenzy         0.965     napping           0.046  (Mohammad 2018)\nDominance     powerful       0.991     weak              0.045\n\u25e6             leadership     0.983     empty             0.081\nHence the connotation of a word is a vector in 3-space\n---\nIdea 1: Defining meaning by linguistic distribution\n\nIdea 2: Meaning as a point in multidimensional space\n---\nDefining meaning as a point in space based on distribution\n?\nEach word = a vector (not just \"good\" or \"w\u2084\u2085\")\nSimilar words are \"nearby in semantic space\"   drinks\nWe build this space automatically by           alcoholic\n                                         seeing which words are\nnearby in text       candy chocolate           cider\n\n                     cream\n                                         juice\n                     0     honey                    wine\n\n                     0  corn  rice\n\n \u2022                                       beef  fried  soup drink\n                                         potatoes\n                              wheat      foods   pork   cooking\n\n                                         vegetablesbread\n---\nWe define meaning of a word as a vector\n\nCalled an \"embedding\" because it's embedded into a\nspace (see textbook)\nThe standard way to represent meaning in NLP\n Every modern NLP algorithm uses embeddings as\n the representation of word meaning\nFine-grained model of meaning for similarity\n---\nIntuition: why vectors?\n\nConsider sentiment analysis:\n\u25e6   With words, a feature is a word identity\n  \u25e6  Feature 5: 'The previous word was \"terrible\"'\n  \u25e6  requires exact same word to be in training and test\n\u25e6   With embeddings:\n  \u25e6  Feature is a word vector\n  \u25e6  'The previous word was vector [35,22,17\u2026]\n  \u25e6  Now in the test set we might see a similar vector [34,21,14]\n  \u25e6  We can generalize to similar but unseen words!!!\n---\nWe'll discuss 2 kinds of embeddings\n\nSimple count embeddings\n\u25e6  Sparse vectors\n\u25e6  Words are represented by the counts of nearby words\n\nWord2vec\n\u25e6  Dense vectors\n\u25e6  Representation is created by training a classifier to predict whether a\n   word is likely to appear nearby\n\u25e6  Later we'll discuss extensions called contextual embeddings\n---\n    From now on:\n    Computing with meaning representations\n    Vector Semantics and\n    instead of string representations\n    Embeddings\n\n   C\u8005@\u00c2(|\uff0c\u00f3|\u800c\u00ffC Nets are for fish;\n                Once you get the fish, you can forget the net.\n   \u8a00\u8005@\u00c2(\u270f\uff0c\u00f3\u270f\u800c\u00ff\u8a00 Words are for meaning;\n                Once you get the meaning, you can forget the words\n                                      \u00d1P(Zhuangzi), Chapter 26\n\n         The asphalt that Los Angeles is famous for occurs mainly on its freeways. But\n in the middle of the city is another patch of asphalt, the La Brea tar pits, and this\nasphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-\n---\nVector     Vector Semantics\nSemantics &\nEmbeddings\n---\nVector     Count-based embeddings\nSemantics &\nEmbeddings\n---\n Remember the intuition: words with similar\n neighborhoods have similar meanings\nHow to measure a word's neighborhood?\nWord-context matrix (a kind of co-occurrence matrix)\n  \u25e6  each row represents a word in the vocabulary\n  \u25e6  each column represents how often each other word in the\n     vocabulary appears nearby\n---\nWord-Context Matrix\n\n            aardvark\n            abacus    zydeco\n            adept\n                    affect\n                      agate     How often does\n\naardvark                  \u2026     agate occur\nabacus                          near abacus?\nadept\naffect\nagate\n\u2026\n\nzydeco\n---\nWord-Context Matrix\nWhat does \"nearby\" mean?\nFor right now let's say \"within 4 words\"\n---\n most common, however, to use smaller contexts, generally a window around the\nThe word-context matrix\n word, for example of 4 words to the left and 4 words to the right, in which case\n the cell represents the number of times (in some training corpus) the column word\nOne set of 4-word contexts\n occurs in such a \u00b14 word window around the row word. For example here is one\n example each of some words in their windows:\n\n is traditionally followed by       cherry       pie, a traditional dessert\n often mixed, such as               strawberry   rhubarb pie. Apple pie\n computer peripherals and personal  digital      assistants. These devices usually\n a computer. This includes          information  available on the internet\n If we then take every occurrence of each word (say strawberry) and count the con-\n text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n simplified subset of the word-word co-occurrence matrix for these four words com-\n puted from the Wikipedia corpus (Davies, 2015).\n Note in Fig. 6.5 that the two words cherry and strawberry are more similar to\n each other (both pie and sugar tend to occur in their window) than they are to other\n words like digital; conversely, digital and information are more similar to each other\n than, say, to strawberry. Fig. 6.6 shows a spatial visualization.\n---\nLet's consider a mini-matrix of 3 words.\n most common, however, to use smaller contexts, generally a window around the\n word, for example of 4 words to the left and 4 words to the right, in which case\nHow often do \"a\", \"computer\", and \"pie\n the cell represents the number of times (in some training corpus) the column word\noccur in the context of \"cherry\"?\n occurs in such a \u00b14 word window around the row word. For example here is one\n example each of some words in their windows:\n\n is traditionally followed by       cherry       pie, a traditional dessert\n often mixed, such as               strawberry   rhubarb pie. Apple pie\n computer peripherals and personal  digital      assistants. These devices usually\n a computer. This includes          information  available on the internet\n If we then take every occurrence of each word (say strawberry) and count the con-\n text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n simplified subset of the word-word co-occurrence matrix for these four words com-\n puted from the Wikipedia corpus (Davies, 2015).\n Note in Fig. 6.5 that the two words cherry and strawberry are more similar to\n each other (both pie and sugar tend to occur in their window) than they are to other\n words like digital; conversely, digital and information are more similar to each other\n than, say, to strawberry. Fig. 6.6 shows a spatial visualization.\n---\n  most common, however, to use smaller contexts, generally a window around the\n The word-          5.3             \u2022    S IMPLE COUNT- BASED EMBEDDINGS              7\n               context mini-matrix for just 4 words\n  word, for example of 4 words to the left and 4 words to the right, in which case\n  the cell represents the number of times (in some training corpus) the column word\n and 3 contexts\ncontext co-occurrence matrix is very large, because for each word in the vocabulary\n  occurs in such a \u00b14 word window around the row word. For example here is one\n(since |V |) we have to count how often it occurs with every other word in the vo-\n  example each of some words in their windows:\ncabulary, hence dimensionality |V | \u21e5 |V |. Let\u2019s therefore instead sketch the process\n               is traditionally followed by  cherry         pie, a traditional dessert\non a smaller scale. Imagine that we are going to look at only the 4 words, and only\nconsider       often mixed, such as          strawberry     rhubarb pie. Apple pie\n             the following 3 context words:     a, computer, and pie.       Furthermore let\u2019s\nassume computer peripherals and personal     digital        assistants. These devices usually\n we only count occurrences in the mini-corpus above.\n So before     a computer. This includes     information    available on the internet\n  If           looking at Fig. 5.2, compute by hand the counts for these 3 context\nwords for we then take every occurrence of each word (say strawberry) and count the con-\n             the four words cherry, strawberry, digital, and information.\n  text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n  simplified subset of the word-word co-occurrence matrix for these four words com-\n  puted from   a                                computer                    pie\n cherry        the Wikipedia corpus (Davies, 2015).\n               Note in Fig. 6.51that the two words     0                    1\nstrawberry     0                                      cherry and strawberry are more similar to\n  each other (both pie and sugar                       0                    2\n digital       0                    tend to occur in their window) than they are to other\n  words like digital; conversely, digital and          1                    0\ninformation    1                                      information are more similar to each other\n  than, say, to strawberry. Fig. 6.6 shows a 1                              0\nFigure 5.2     Co-occurrence vectors for four         spatial visualization.\n                                                words with counts from the 4 windows above,\n---\n         consider the following 3 context words: a, computer, and pie. Furthermore let\u2019s\n         assume we only count occurrences in the mini-corpus above.\n        The word-context mini-matrix for just 4 words\n          So before looking at Fig. 5.2, compute by hand the counts for these 3 context\n        and 3 contexts\n         words for the four words cherry, strawberry, digital, and information.\n\n                         a                       computer              pie\n         cherry          1                          0                  1\n         strawberry      0                          0                  2\n          digital        0                          1                  0\n         information     1                          1                  0\n\n        \u2022 Figure 5.2    Co-occurrence vectors for four words with counts from the 4 windows above,\n          This 4x3 matrix is a subset of full |V| x |V| matrix\n         showing just 3 of the potential context word dimensions. The vector for cherry is outlined in\n\n        \u2022 red. Note that a real vector would have vastly more dimensions and thus be even sparser.\n          Each word is represented by a row vector with\n          Hopefully your count matches what is shown in Fig. 5.2, so that each cell repre-\n          dimensionality [1 x |V|]\n         sents the number of times a particular word (defined by the row) occurs in a partic-\n        \u2022 ular context (defined by the word column).\n          With co-occurrence counts with each other word\n          Each row, then, is a vector representing a word.   To review some basic linear\nctor     algebra, a vector is, at heart, just a list or array of numbers. So cherry is represented\n---\n    most common, however, to use smaller contexts, generally a window around the\nhere is one example each of some words in their windows:\n    word, for example of 4 words to the left and 4 words to the right, in which case\n      is traditionally followed by      cherry           pie, a traditional dessert\n    the cell represents the number of times (in some training corpus) the column word\nA                  often mixed, such as strawberry       rhubarb pie. Apple pie\nselection from a larger word-context matrix\n    occurs in such a \u00b14 word window around the row word. For example here is one\ncomputer peripherals and personal       digital          assistants. These devices usually\n    example each of some words in their windows:\n            a computer. This includes   information      available on the internet\n      If we  is traditionally followed by cherry         pie, a traditional dessert\n            then take every occurrence of each word (say strawberry) and count the\ncontext            often mixed, such as   strawberry     rhubarb pie. Apple pie\n            words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a\n      computer peripherals and personal   digital        assistants. These devices usually\nsimplified subset of the word-word co-occurrence matrix for these four words com-\n             a computer. This includes    information    available on the internet\nputed from the Wikipedia corpus (Davies, 2015).\n    If we then take every occurrence of each word (say strawberry) and count the con-\n    text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n    simplified     aardvark     ...    computer    data  result        pie    sugar     ...\n       cherry     subset of the word-word co-occurrence matrix for these four words com-\n    puted from    0             ...       2         8     9            442     25       ...\n     strawberry    the Wikipedia corpus (Davies, 2015).\n     Note in Fig. 0             ...       0         0     1            60      19       ...\n      digital      6.5 that the two words cherry and strawberry are more similar to\n    each other (both 0          ...     1670       1683  85            5       4        ...\n    information    pie and sugar tend to occur in their window) than they are to other\n    words like    0             ...     3325       3982  378           5       13       ...\nFigure 6.6        digital; conversely, digital and information are more similar to each other\n    than,    Co-occurrence vectors for four words in the Wikipedia corpus, showing six of\nthe         say, to strawberry. Fig. 6.6 shows a spatial visualization.\n      dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\nred. Note that a real vector would have vastly more dimensions and thus be much sparser.\n---\ncomputer\n\n4000\n        information\n3000    [3982,3325]\n        digital\n2000    [1683,1670]\n\n1000\n\n1000 2000 3000 4000\ndata\n---\nThe word-context matrix\n\nWord context matrix is |V| x |V|\nThis could be 50,000 x 50,000\nMost of these numbers are zero!\nSo these are sparse vectors\nThere are efficient algorithms for storing and\ncomputing with sparse matrices\n---\nVector     Count-based embeddings\nSemantics &\nEmbeddings\n---\n           Cosine for computing word similarity\nVector\nSemantics &\nEmbeddings\n---\nhence of length |V |, or both with documents as dimensions as documents, of length\n|D|) and gives a measure of their similarity. By far the most common similarity\n Computing word similarity: Dot product and cosine\nmetric is the cosine of the angle between the vectors.\n The cosine\u2014like most measures for vector similarity used in NLP\u2014is based on\nthe dot product operator from linear algebra, also called the inner product:\n The dot product between two vectors is a scalar:\n N\n dot product(v, w) = v \u00b7 w = X vi wi = v1 w1 + v2 w2 + ... + vN wN          (6.7)\n i=1\n The dot product tends to be high when the two\nAs we will see, most metrics for similarity between vectors are based on the dot\n vectors have large values in the same dimensions\nproduct. The dot product acts as a similarity metric because it will tend to be high\njust when the two vectors have large values in the same dimensions. Alternatively,\n Dot product can thus be a useful similarity metric\nvectors that have zeros in different dimensions\u2014orthogonal vectors\u2014will have a\n between vectors\ndot product of 0, representing their strong dissimilarity.\n This raw dot product, however, has a problem as a similarity metric: it favors\nlong vectors. The vector length is defined as\n---\n  ill see, most metrics for similarity between vectors are based on the dot\nt. The dot product acts as a similarity metric because it will tend to be high\n           Problem with raw dot-product\n  n the two vectors have large values in the same dimensions. Alternatively,\n  that have zeros in different dimensions\u2014orthogonal vectors\u2014will have a\n  uct of Dot product favors long vectors\n           0, representing their strong dissimilarity.\n  s raw dot product, however, has a problem as a similarity metric: it favors\n           Dot product is higher if a vector is longer (has higher\n  ctors. The vector length is defined as\n           values in many dimension)\n           Vector length:        v\n                                 u N\n                                 uX\n                             |v| = t    v2                        (6.8)\n                                        i\n                                     i=1\n           Frequent words (of, the, you) have long vectors (since\n product is higher if a vector is longer, with higher values in each dimension.\n           they occur many times with other words).\n equent words have longer vectors, since they tend to co-occur with more\n  nd have higher co-occurrence values with each of them. The raw dot product\n  l be     So dot product overly favors frequent words\n           higher for frequent words. But this is a problem; we\u2019d like a similarity\n---\n             This raw dot product, however, has a problem as a similarity metric: it favors\nctor length  long vectors. The vector length is defined as\n  The cosine similarity metric between two vectors ~\n                                                                      v and ~\n    Alternative: cosine for                                            w thus can be computed\n                                                  computing word similarity\n :                                                          v\n                                                            u N\n                                                            uX\n                                                  |v| = t             v2            (6.8)\n                                                                      iN\n\n             The dot product is higher if                    i=1 X vi wi\n                                              a vector is longer, with higher values in each dimension.\n                                              ~\n                                              v \u00b7 ~\n               cosine(                            w          v        i=1 v\n                                ~\n                                v, ~\n                                 w) =                  =                            (6.10)\n             More frequent words have longer vectors, since they tend to co-occur with more\n                                              |              u              u\n                                              ~\n                                                   v||~\n             words and have                        w|           N            N\n                                higher co-occurrence values with each of them. The raw dot product\n                                                             uX uX\n             thus will be higher for frequent                t              2 t  2\n                                                   words. But this is a problem; we\u2019d like a similarity\n             metric that tells us how similar two words are           vi         wi\n                                                                regardless of their frequency.\n             We modify the dot                                  i=1              i=1\n                                   product to normalize for the vector length by dividing the\n      For some applications we pre-normalize each vector, by dividing it by its length,\n             dot product by the lengths of each of the two vectors. This normalized dot product\neating a     turns out to be the same as the cosine of the angle between the two vectors, following\n             unit vector of length 1. Thus we could compute a unit vector from ~\n               Based on the definition of the dot product between two vectors a                        a by\n             from the definition of the dot product between two vectors a and b:              and b\nviding it by |~\n             a|. For unit vectors, the dot product is the same as the cosine.\n      The cosine value ranges from 1 for vectors pointing in the same direction, through\n                                                   a \u00b7 b = |a||b| cos q\n or vectors that are orthogonal, to -1 for vectors pointing in opposite directions.\n                                                   |a \u00b7 b  = cos q                            (6.9)\nut raw frequency values are                        a||b|\n                                   non-negative, so the cosine for these vectors ranges\n---\nCosine as a similarity metric\n\n                                         1\n-1: vectors point in opposite directions 0.5\n+1: vectors point in same directions        50  400  150  200  250  300  350\n0: vectors are orthogonal                -0.5\n\n                                         -1\n\nBut since raw frequency values are non-negative, the\n50\ncosine for term-term matrix vectors ranges from 0\u20131\n---\n        0 for vectors that are orthogonal, to -1 for vectors pointing in opposite direction\nraw frequency values are non-negative, so the cosine for these vectors ranges\n rs        Let\u2019s see how the cosine computes which of the words cherry or digital is c\n        that are orthogonal, to -1 for vectors pointing in opposite directions.\n        But raw frequency values are non-negative, so the cosine for these vectors rang\n  0\u20131.  Cosine examples\n        in meaning to information, just using raw counts from the following shortened t\nequency values are non-negative, so the cosine for these vectors ranges\n        from 0\u20131.\n Let\u2019s see how the cosine computes which of the words cherry or digital is closer\n           Let\u2019s see how the cosine computes which of the words cherry or digital is clos\n eaning to information, just                                          pie  data  computer\n        in meaning to                   using raw counts from the following shortened table:\nsee how the cosine information, just using raw counts from the following shortened tabl\n                             computes which of the words cherry or digital is closer\n                                                  cherry              442     8   pie     data     computer\ng to       v \u2022 w  v      w       \u2211 N v wpie                     data       computer  2\n        information, just using raw counts from the following shortened table:\n                                                  i  i\n        cos(v, w) =  =   \u2022  =                i=1  digital             5    1683   1670\n                                                                   piecherry      442     8        2\n                     v w  v  w  \u2211 N               \u2211 N                     data   computer\n                                cherry               442           8       2\n                                             information              5    3982   3325\n                                     v 2                 w 2\n                                                  cherry           442     8      2\n                                             i             i          digital     5       1683     1670\n                                 i=1pie              data       computer\n                                digital              i=1\n                                                      5         1683       1670\n                                                  digital        5       1683    1670\n                          cherry     442              8               information 5       3982     3325\n                                                                4422\n                             information              5         3982 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                     information                 5         3325\n                                                         p               3982    3325\n        cos(cherry, information) =                                               p                      = .017\n                             digital         5       1683             21670\n                          information        5             442        + 82 + 22       52 + 39822 + 33252\n                                                     3982              3325\n                                                  442 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                                                442 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                        p                          5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\ns(cherry, information) =                             p                p          p                = .017\n        cos(cherry, information) =                       p                            p                = .017\n        cos(digital, information) =                                                                          = .\n                                         4422 + 82 + 22                    52 + 39822 + 33252\n                                                           4422 + 82 + 22        52 + 39822 + 33252\n                                        442 \u21e4 5 + 8 5\u00b2 + 1683\u00b2 + 1670\u00b2                   52 + 39822 + 33252\n                                                           \u21e4 3982 + 2 \u21e4 3325\n                                                  5 \u21e4 5 + 5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\n                             p                                  1683 \u21e4 3982 + 1670 \u21e4 3325\nrry, information) =                                  p          p                     p   = .017\n        cos(digital, information) =                                                                        = .996\ns(digital, information) =               p                                     p                        = .996\n           The model decides that information is way closer to digital than it is to cher\n                                4422 + 82 + 22                  52 + 39822 + 33252\n                                             2             52 + 16832 + 16702         52 + 39822 + 33252\n        result that seems                5        + 16832 + 16702                52 + 39822 + 33252\n                                sensible. Fig. 6.7 shows a visualization.\n                             51         5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\n tal, information) =         p                                        p                           = .996\n           The model decides that information is way closer to digital than it is to cherry,\n The model decides that information is way closer to digital than it is to cherry, a\n        result that seems          52 + 16832 + 16702                    52 + 39822 + 33252\n lt                                sensible. Fig. 6.7 shows a visualization.\n        that seems sensible. Fig. 6.7 shows a visualization.\n---\n   V Visualizing cosines\n    S                   E\n    (well, angles)\n    ECTOR EMANTICS AND   MBEDDINGS\n\n        \u2019\n        pie\n        \u2018\n        1:  500\n        Dimension  cherry\n                         digital    information\n\n                   500   1000  1500  2000  2500  3000\n\n                         Dimension 2: \u2018computer \u2019\nre 6.7   A (rough) graphical demonstration of cosine similarity, showing vec\n---\nVector       Cosine for computing word\nSemantics &  similarity\nEmbeddings\n---\nVector     TF-IDF\nSemantics &\nEmbeddings\n---\nBut raw frequency is a bad representation\n\n\u2022  The co-occurrence matrices we have seen represent each\n   cell by word frequencies.\n\u2022  Frequency is clearly useful; if sugar appears a lot near\n   apricot, that's useful information.\n\u2022  But overly frequent words like the, it, or they are not very\n   informative about the context\n\u2022  It's a paradox! How can we balance these two conflicting\n   constraints?\n---\n    fool                36     0.012\n    Two good            37     0\n    common solutions for word weighting\n    sweet               37     0\n ighting of the value for word t in document d , wt ,d thus combines\nth idf: tf-idf:  tf-idf value for word t in document d:\n\n                 wt ,d = tft ,d \u21e5 idft                     (6.13)\n\n idf weighting to the Shakespeare term-document matrix in Fig. 6.2.\n        Words like \"the\" or \"it\" have very low idf\nvalues for the dimension corresponding to the word         good have\n ince   PMI: (Pointwise mutual information)\n       this word appears in every document, the tf-idf algorithm\nd in any comparison                       #(% ,% )\n                        of the plays. Similarly, the word  fool, which\n        \u25e6 PMI !! , !\"   = $%& # % !#( \"\n the 37 plays, has a much lower           !  %\" )\n                                          weight.\n hting is by far the dominant way of weighting co-occurrence ma-\n          See if words like \"good\" appear more often with \"great\" than\n n retrieval, but also plays a role in many other aspects of natural\n          we would expect by chance\n akespeare\u2019s favorite adjectives, a fact probably related to the increased use of\n---\n     Term frequency (tf) in the tf-idf algorithm\n                       tft , d  = count(t , d )\n\n commonly we squash the raw frequency a bit, by using the lo\n cy  We could imagine using raw count:\n     instead. The intuition is that a word appearing 100 times\nn\u2019t make that word 100 times more likely to be relevant to the\n     tf\u209c,d = count(t,d)\n ment. We also need to do something special with counts of 0,\n he log of 0. 2\n     But instead of using raw count, we usually squash a bit:\n               (\n     tft , d   =  1 + log10 count(t , d )     if count(t , d ) > 0\n                  0                           otherwise\n\nuse log weighting, terms which occur 0 times in a document wou\n---\n      for discriminating those documents from the rest of the collection; terms that occur\nt     frequently across the entire collection aren\u2019t as helpful. The document frequency\n      df  Document frequency (df)\n      t   of a term t is the number of documents it occurs in. Document frequency is\n      not the same as the collection frequency of a term, which is the total number of\n      times the word appears in the whole collection in any document. Consider in the\n          df is the number of documents t occurs in.\n      collection of Shakespeare\u2019s 37 plays the two words Romeo and action. The words\n             t\n      have identical collection frequencies (they both occur 113 times in all the plays) but\n          (note this is not collection frequency: total count across\n      very different document frequencies, since Romeo only occurs in a single play. If\n          all documents)\n      our goal is to find documents about the romantic tribulations of Romeo, the word\n          \"Romeo\" is very distinctive for one Shakespeare play:\n      Romeo should be highly weighted, but not action:\n                         Collection Frequency  Document Frequency\n              Romeo      113                   1\n              action     113                   31\n          We emphasize discriminative words like Romeo via the inverse document fre-\nf     quency or idf term weight (Sparck Jones, 1972). The idf is defined using the frac-\n      tion N /df , where N is the total number of documents in the collection, and df is\n---\ner of documents in many collections, this measure\n      common as to be completely non-discriminative since they o\n      Inverse document frequency (idf)\n      good or sweet.3\ng function. The resulting definition for inverse\nus                                           Word         df     idf\n            \u2713        \u25c6                       Romeo        1      1.57\n      idf  = log  N                          salad        2      1.27\n                                             Falstaff     4         (6.13)\n      t           10  dft                    forest       12     0.967\n                                                                 0.489\n                                             battle       21     0.246\n e words in the Shakespeare corpus, ranging from\n      N is the total number of documents     wit          34     0.037\n hich occur in only one play like fool                    36     0.012\n      in the collection                      Romeo, to those that\n alstaff, to those which are very            good         37     0\n                                            common like fool or so\n                                             sweet        37     0\nn-discriminative since they occur in all 37 plays like\n---\nWhat is a document?\n\nCould be a play or a Wikipedia article\nBut for the purposes of tf-idf, documents can be\nanything; we often call each paragraph a document!\n---\n defined either by Eq. 6.11 or by Eq. 6.12) with id\n            Final tf-idf weighted value for a word\n                            wt , d           = tft , d \u21e5 idft\n           Raw counts:                           6.3            \u2022    W ORDS AND VECTORS  7\n            As You Like It   Twelfth Night    Julius Caesar    Henry V\n f-idf weighting to the Shakespeare term-documen\n            battle          1      0                7             13\n             good           114    80               62            89\n on          fool           36     58               1             4\n    Eq.                 6.12. Note that the tf-idf values for the\n             wit            20     15               2             3\n            Figure 6.2       The term-document matrix for four words in four Shakespeare plays. Each cell\n ord 6      \u2022  V            S      E\n           tf good have now all become 0; since this wor\n HAPTER     -idf: ECTOR          EMANTICS AND    MBEDDINGS\n            contains the number of times the (row) word occurs in the (column) document.\n idf algorithm leads it to be ignored. Similarly, the\n                        As You Like It  Twelfth Night           Julius Caesar  Henry V\n            represented as a count vector, a column in Fig. 6.3.\n            battle      0.246           0                       0.454          0.520\n vector     To review some basic linear algebra, a vector is, at heart, just a list or array of\n  f the 37 plays, has a much lower weight.\n            good        0               0                       0              0\n            numbers. So As You Like It is represented as the list [1,114,36,20] (the first column\n            fool        0.030           0.033                   0.0012         0.0019\n            vector in Fig. 6.3) and Julius Caesar is represented as the list [7,62,1,2] (the third\n            wit         0.085           0.081                   0.048          0.054\nr space     column vector).      A vector space is a collection of vectors, characterized by their\n            Figure 6.9       A tf-idf weighted term-document matrix for four words in four Shakespeare\n---\nVector     TF-IDF\nSemantics &\nEmbeddings\n---\nVector     Word2vec\nSemantics &\nEmbeddings\n---\nSparse versus dense vectors\n\nCount vectors (even if weighted by tf-idf)\n\u25e6  long (length |V|= 20,000 to 50,000)\n\u25e6  sparse (most elements are zero)\nAlternative: learn vectors which are\n\u25e6  short (length 50-1000)\n\u25e6  dense (most elements are non-zero)\n---\nSparse versus dense vectors\n\nWhy dense vectors?\n\u25e6   Short vectors may be easier to use as features in machine\n    learning (fewer weights to tune)\n\u25e6   Dense vectors may generalize better than explicit counts\n\u25e6   Dense vectors may do better at capturing synonymy:\n  \u25e6 car and automobile are synonyms; but are distinct dimensions\n    \u25e6  a word with car as a neighbor and a word with automobile as a\n       neighbor should be similar, but aren't\n\u25e6   In practice, they work better\n       56\n---\nCommon methods for getting short dense vectors\n\n\u201cNeural Language Model\u201d-inspired models\n\u25e6  Word2vec (skipgram, CBOW), GloVe\nSingular Value Decomposition (SVD)\n\u25e6  A special case of this is called LSA \u2013 Latent Semantic\n   Analysis\nAlternative to these \"static embeddings\":\n  \u2022     Contextual Embeddings (ELMo, BERT)\n  \u2022     Compute distinct embeddings for a word in its context\n  \u2022     Separate embeddings for each token of a word\n---\nSimple static embeddings you can download!\n\nWord2vec (Mikolov et al)\nhttps://code.google.com/archive/p/word2vec/\n\nGloVe (Pennington, Socher, Manning)\nhttp://nlp.stanford.edu/projects/glove/\n---\nWord2vec\nPopular embedding method\n\u2022     Very fast to train\n\u2022     Code available on the web\nIdea: predict rather than count\nWord2vec provides various options. We'll do:\nskip-gram with negative sampling (SGNS)\n---\n  Word2vec\nInstead of counting how often each word w occurs near \"apricot\"\n\u25e6   Train a classifier on a binary prediction task:\n  \u25e6  Is w likely to show up near \"apricot\"?\nWe don\u2019t actually care about this task\n  \u25e6  But we'll take the learned classifier weights as the word embeddings\nBig idea: self-supervision:\n  \u25e6  A word c that occurs near apricot in the corpus cats as the gold \"correct\n     answer\" for supervised learning\n  \u25e6  No need for human labels\n  \u25e6  Bengio et al. (2003); Collobert et al. (2011)\n---\nApproach: predict if candidate word c is a \"neighbor\"\n\n1.  Treat the target word t and a neighboring context word c\n    as positive examples.\n2.  Randomly sample other words in the lexicon to get\n    negative examples\n3.  Use logistic regression to train a classifier to distinguish\n    those two cases\n4.  Use the learned weights as the embeddings\n---\nSkip-Gram Training Data\n\nAssume a +/- 2 word window, given training sentence:\n\n\u2026lemon, a [tablespoon of apricot jam, a] pinch\u2026\nc1    c2 [target]      c3             c4\n---\nSkip-Gram Classifier\n\n(assuming a +/- 2 word window)\n\n\u2026lemon, a [tablespoon of apricot jam, a] pinch\u2026\n c1            c2 [target]          c3  c4\nGoal: train a classifier that is given a candidate (word, context) pair\n (apricot, jam)\n (apricot, aardvark)\n\u2026\nAnd assigns each pair a probability:\nP(+|w, c)\nP(\u2212|w, c) = 1 \u2212 P(+|w, c)\n---\nSimilarity is computed from dot product\n\nRemember: two vectors are similar if they have a high\ndot product\n\u25e6 Cosine is just a normalized dot product\nSo:\n\u25e6 Similarity(w,c) \u221d w \u00b7 c\nWe\u2019ll need to normalize to get a probability\n\u25e6 (cosine isn't a probability either)\n           64\n---\ndel   Turning dot products into probabilities\n      the probability that word  c is a real context word for target word w\n\n      Sim(    P(+|w, c) =       s (c \u00b7 w) =    1\n              w,c) \u2248 w \u00b7 c                     1 + exp (\u2212c \u00b7 w)\n                                                6.8  \u2022         W ORD 2 VEC\n      To turn this into a probability\nmoid function returns a number between 0 and 1, but to make it a prob\n del the probability that word   c is a real context word for target word w\n      We'll use the sigmoid from logistic regression:\n so need the total probability of the two possible events ( c is a context\nsn\u2019t a context word) to sum to 1. We thus estimate the probability that\neal context   P(+|w, c) =       s (c \u00b7 w) =    1\n      word for          w as:                  1 + exp (\u2212c \u00b7 w)\n\n      moid function returns a number between 0 and 1, but to make it a proba\n              P(\u2212|w, c) =         1 \u2212 P(+|w, c)\n lso need the total probability of the two possible events ( c is a context\nisn\u2019t a context word)   = s (\u2212c \u00b7 w) =         1\n                        to sum to 1. We thus estimate the probability that w\n                                               1 + exp (c \u00b7 w)\n---\n    How  P(\u2212|w, c) =                    1 \u2212 P(+|w, c)           6.8  \u2022  W ORD 2 VEC  19\n         Skip-Gram Classifier computes P(+|w, c)\n    We model the probability that word c is a real context           1\n                        = s (\u2212c \u00b7 w) =                         word for target word w as:     (6.29)\n                                                      1 + exp (c \u00b7 w)\n                     P(+|w, c) =        s (c \u00b7 w) = 1 + exp1                         (6.28)\nation 6.28 gives us                                            (\u2212c \u00b7 w)\n    This is             the probability for one word, but there are many context\n             for one context word, but we have lots of context words.\nds  The sigmoid function returns a number between 0 and 1, but to make it a probability\n    in the window. Skip-gram makes the simplifying assumption that all context\n    We'll assume independence and just multiply them:\n    we\u2019ll also need the total probability of the two possible events (c is a context word,\nds are independent, allowing us to just multiply their probabilities:\n    and c isn\u2019t a context word) to sum to 1. We thus estimate the probability that word c\n    is not a real context word for w as:\n                                                   L\n                     P(\u2212|w, c) =        1 \u2212 P(+|w, Y\n                        P(+|w, c1:L ) = c)            s (ci \u00b7 w)                              (6.30)\n                        = s (\u2212c \u00b7 w) = i=1                     1                     (6.29)\n                                                   1 + exp (c \u00b7 w)\n                                                  L\n    Equation 6.28 gives us the probability for    X\n                     log P(+|w, c                      ) one word, but there are many context\n    words in the window. Skip-gram makes   =          log s (c          \u00b7 w)                  (6.31)\n                                                  the simplifying assumption that all context\n                                        1:L                             i\n    words are independent, allowing us to just multiply their probabilities:\n                                                   i=1\n                    L\nmmary, skip-gram    Y\n                     trains a probabilistic classifier that, given a test target word\n---\nSkip-gram classifier: summary\n\nA probabilistic classifier, given\n\u2022     a test target word w\n\u2022     its context window of L words c1:L\nEstimates probability that w occurs in this window based\non similarity of w (embeddings) to c1:L (embeddings).\n\nTo compute this, we just need embeddings for all the\nwords.\n---\nThese embeddings we'll need: a set for w, a set for c\n\n1..d\naardvark  1\napricot\n\n\u2026         \u2026  W target words\n\n& =  zebra  |V|\n     aardvark  |V|+1\n     apricot\n                    C  context & noise\n     \u2026         \u2026       words\n\n     zebra     2V\n---\nVector     Word2vec\nSemantics &\nEmbeddings\n---\nVector       Word2vec: Learning the\nSemantics &  embeddings\nEmbeddings\n---\nWord2vec learns embeddings by starting with an initial set of embedding vecto\nSkip-Gram Training data\nand then iteratively shifting the embedding of each word w to be more like the em\nbeddings of words that occur nearby in texts, and less like the embeddings of word\nthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n... lemon,     a [tablespoon of apricot jam,      a] pinch ...\n           \u2026lemon, a [tablespoon of apricot jam,     a] pinch\u2026\n                c1 c1    c2  t       c3           c4\nThis example has a target word c2 [target]  c3       c4\n                             t (apricot), and 4 context words in the L = \u00b1\nwindow, resulting in 4 positive training instances (on the left below):\n positive examples +                        negative examples -\n t         c                    t           c           t        c\n apricot   tablespoon           apricot     aardvark    apricot  seven\n apricot   of                   apricot     my          apricot  forever\n                                apricot     where       apricot  dear\n apricot   jam                                                         71\n apricot   a                    apricot     coaxial     apricot  if\n---\nWord2vec learns embeddings by starting with an initial set of embedding vecto\nSkip-Gram Training data\nand then iteratively shifting the embedding of each word w to be more like the em\nbeddings of words that occur nearby in texts, and less like the embeddings of word\nthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n... lemon,     a [tablespoon of apricot jam,        a] pinch ...\n           \u2026lemon, a [tablespoon of apricot jam,       a] pinch\u2026\n                c1 c1    c2    t        c3          c4\nThis example has a target word c2 [target]    c3       c4\n                             t (apricot), and 4 context words in the L = \u00b1\nwindow, resulting in 4 positive training instances (on the left below):\n positive examples +         For              negative examples -\n t         c                       each positive\n                                   t          c           t        c\n                             example we'll grab k\n apricot   tablespoon              apricot    aardvark    apricot  seven\n                             negative examples,\n apricot   of                      apricot    my          apricot  forever\n                             sampling by frequency\n                                   apricot    where       apricot  dear\n apricot   jam                                                         72\n apricot   a                       apricot    coaxial     apricot  if\n---\n ord2vec learns embeddings by starting with an initial set of embedding vectors\n    Word2vec learns embeddings by starting with an initial set of embedding vecto\n d then Skip-Gram Training data\n         iteratively shifting the embedding of each word w to be more like the em-\n    and then iteratively shifting the embedding of each word w to be more like the em\n ddings of words that occur nearby in texts, and less like the embeddings of words\n    beddings of words that occur nearby in texts, and less like the embeddings of word\nat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n    that don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n . lemon, a [tablespoon of apricot jam,                   a] pinch ...\n    ... lemon,          a [tablespoon of apricot jam,                     a] pinch ...\n                    \u2026lemon, a [tablespoon of apricot jam,                 a] pinch\u2026\n                c1       c1  c2     t        c3           c4\n                             c1     c2       t           c3               c4\n  This example has a target word t           c2 [target]       c3         c4\n         This example has a         (apricot), and 4 context words in the L = \u00b12\n                              target word t (apricot), and 4 context words in the L = \u00b1\nindow, resulting in 4 positive training instances (on the left below):\n    window, resulting in 4 positive training instances (on the left below):\n          positive examples +\n    positive examples +                                        negative examples -\n    t     tc        c                            negative examples -\n                                        t        t  c          c     t      t  c     c\n                                        apricotapricot         aardvark     apricot  seven\n          apricot   tablespoon                      aardvark         apricot seven\n    apricot tablespoon                  apricotapricot         my           apricot  forever\n          apricot   of                              my               apricot forever\n    apricot of                                   apricot       where        apricot  dear\n          apricot   jam                 apricot     where            apricot dear    73\n    apricot jam                                  apricot       coaxial      apricot  if\n          apricot   a                   apricot     coaxial          apricot if\n    apricot a\n---\n        Word2vec: how to learn vectors\n\nGiven the set of positive and negative training instances,\nand an initial set of embedding vectors\nThe goal of learning is to adjust those word vectors such\nthat we:\n \u25e6  Maximize the similarity of the target word, context word pairs\n    (w , c\u209a\u2092\u209b) drawn from the positive data\n \u25e6  Minimize the similarity of the (w , cneg) pairs drawn from the\n    negative data.\n\n    8/24/25                                74\n---\n \u2022 Minimize the similarity of the (w, cneg ) pairs from the negative examples.\n If we consider one word/context pair (w, c  ) with its k noise words c  ...c  ,\nLoss function for one w with c               , c                  ...c\n                   pos                       pos  neg1               neg1     negk\n we can express these two goals as the following loss function L         negk\n                                                                  to be minimized\n (hence the \u2212); here the first term expresses that we want the classifier to assign the\nMaximize the similarity of the target with the actual context words,\n real context word cpos a high probability of being a neighbor, and the second term\nand minimize the similarity of the target with the k negative sampled\n expresses that we want to assign each of the noise words cnegi a high probability of\nnon-neighbor words.\n being a non-neighbor, all multiplied because we assume independence:\n                   [    k                         ]\n   LCE  = \u2212 log    P(+|w, cpos ) \u220f P(\u2212|w, cnegi )\n        [               i=1 k                          ]\n        = \u2212 log P(+|w, cpos ) + \u2211 log P(\u2212|w, cnegi )\n        [                    i=1                                  ]\n                             k\n        = \u2212 log P(+|w, cpos ) + \u2211 log \u23081 \u2212 P(+|w, cnegi )\u2309\n        [               ki=1                           ]\n        = \u2212 log s (cpos \u00b7 w) + \u2211 log s (\u2212cnegi \u00b7 w)                        (6.34)\n                        i=1\n---\nLearning the classifier\n\nHow to learn?\n\u25e6  Stochastic gradient descent!\n\nWe\u2019ll adjust the word weights to\n\u25e6  make the positive pairs more likely\n\u25e6  and the negative pairs less likely,\n\u25e6  over the entire training set.\n---\nIntuition of one step of gradient descent\n\naardvark\n                    move apricot and jam closer,\napricot              w  increasing c\u209a\u2092\u209b z w\nW\n\n                              \u201c\u2026apricot jam\u2026\u201d\n!           zebra\n         aardvark             move apricot and matrix apart\n\n   jam               c\u209a\u2092\u209b     decreasing cneg1 z w\n   C k=2 matrix      cneg1    move apricot and Tolstoy apart\n\n          Tolstoy    cneg2    decreasing cneg2 z w\n            zebra\n---\n   Reminder: gradient descent\n\n   \u2022 At each step\n     \u2022      Direction: We move in the reverse direction from the\nISTIC R     gradient of the loss function\n     EGRESSION\n     \u2022      Magnitude: we move the value of this gradient\n            ! ! (# $ ; & , ( ) weighted by a learning rate \u03b7\n\n     \u2022      !\"\n            Higher learning rate means move w faster\n\n              wt +1 = wt \u2212 h d L( f (x; w), y)\n                             dw\n---\n    pos                                         k\nesses that we want to assign each of        \u2211       \u2308    \u2309\n    = \u2212 log P(+|w, c                        the noise words c  a high probability o\n                      ) +                           log 1 \u2212 P(+|w, c  )\n    The derivatives of the loss                                neg\n           pos                                           function\ng a non-neighbor, all                                          i  negi\n \u2022  V  S                    multiplied because we assume independence:\n       ECTOR EMANTICS AND E MBEDDINGS\n             [            [                 ki=1         ]     ]\n   k\n   \u220f\n       L  = \u2212 log         P(+|w, c         )\u2211 P(\u2212|w, c   )\n          = \u2212 log s (c         \u00b7 w) +           log s (\u2212c  \u00b7 w)        (6.34\n       CE                      pos  pos                  neg\nof as an exercise at the end of the chapter):              neg\n                                                           i  i\n                                           i=1\n             [                             i=1 k                   ]\ns, we want to maximize the dot                  \u2211\n             \u2202 LCE                  product of the word with the actual contex\n             = \u2212 log P(+|w, c              ) +      log P(\u2212|w, c   )\n , and minimize the = [s (c         \u00b7 w) \u2212 1]w\n                       dot products of the word with the k negative sampled non\nbor words.   \u2202 cpos [          pos  pos         i=1               negi    ]\n             \u2202 L                                k\n e minimize                                     \u2211\n                this loss function using stochastic gradient descent.      Fig. 6.1\n                = CE  = [s (cneg \u00b7 w)]w                 \u2308                 \u2309\ns the        \u2202 c    \u2212 log P(+|w, c         ) +         log 1 \u2212 P(+|w, c   )\n          intuition of one step of learning.\n                neg                 pos                                   negi\n                       [                        i=1          k     ]\n             \u2202 L                         k                   X\n                 CE    = [s (c      \u00b7 w) \u2211\n             aardvark          pos         \u2212 1]cpos +             [s (cneg \u00b7 w)]cneg\n                = \u2212 log s (c        \u00b7 w) +       log s (\u2212c        \u00b7 w)\n                \u2202 w            pos  move apricot and neg                   i    i    (6.3\n                                                                  jam closer,\n                                            i=1              i=1  i\n                apricot        w            increasing c          z w\n---\n               \u2202 w   = [s (cpos \u00b7 w) \u2212 1]cpos +  [s (cnegi \u00b7 w)]cnegi\n    Update equation in SGD                       i=1\n he update equations going from time step t to t + 1 in stochastic gradient de\nre thus:\n    Start with randomly initialized C and W matrices, then incrementally do updates\n\n        ct +1  = ct      \u2212 h [s (ct  \u00b7 wt ) \u2212 1]wt\n        pos       pos    pos\n        ct +1  = ct      \u2212 h [s (ct  \u00b7 wt )]wt\n        neg       neg    \"  neg                     k                              #\n\n        wt +1  = wt \u2212 h  [s (cpos \u00b7 wt ) \u2212 1]cpos + X[s (cnegi \u00b7 wt )]cnegi\n                                                  i=1\nust as in logistic regression, then, the learning algorithm starts with randoml\n alized W and C matrices, and then walks through the training corpus using gra\nescent to move W and C so as to maximize the objective in Eq. 6.34 by makin\n---\nTwo sets of embeddings\n\nSGNS learns two sets of embeddings\nTarget embeddings matrix W\nContext embedding matrix C\nIt's common to just add them together,\nrepresenting word i as the vector wi + ci\n---\n Summary: How to learn word2vec (skip-gram)\n embeddings\nStart with V random d-dimensional vectors as initial\nembeddings\nTrain a classifier based on embedding similarity\n  \u25e6 Take a corpus and take pairs of words that co-occur as positive\n  examples\n  \u25e6 Take pairs of words that don't co-occur as negative examples\n  \u25e6 Train the classifier to distinguish these by slowly adjusting all\n  the embeddings to improve the classifier performance\n  \u25e6 Throw away the classifier code and keep the embeddings.\n---\nVector       Word2vec: Learning the\nSemantics &  embeddings\nEmbeddings\n---\nVector     Properties of Embeddings\nSemantics &\nEmbeddings\n---\nThe kinds of neighbors depend on window size\n\nSmall windows (C= +/- 2) : nearest words are syntactically\nsimilar words in same taxonomy\n\u25e6Hogwarts nearest neighbors are other fictional schools\n\u25e6Sunnydale, Evernight, Blandings\nLarge windows (C= +/- 5) : nearest words are related\nwords in same semantic field\n\u25e6Hogwarts nearest neighbors are Harry Potter world:\n\u25e6Dumbledore, half-blood, Malfoy\n---\nability to capture relational meanings. In an important early vector space model of\n   Analogical relations\ncognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model\nfor solving simple analogy problems of the form a is to b as a* is to what?. In such\n   The classic parallelogram model of analogical reasoning\nproblems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as\ngrape is to  , and must fill in the word vine. In the parallelogram model, illus-\n   (Rumelhart and Abrahamson 1973)    #  \u00bb  # \u00bb\ntrated in Fig. 6.15, the vector from the word apple to the word tree (= apple \u2212 tree)\n   To solve: \"apple         #  \u00bb\nis added to the vector for  is to tree as grape is to _____\"\n                            grape (grape); the nearest word to that point is returned.\n   Add tree \u2013 apple to grape to get vine\n                                      tree\n             apple\n\n   vine\n   grape\n---\n llelogram method received more modern attention because of\n       Analogical relations via parallelogram\n rd2vec or GloVe vectors ( Mikolov et al. 2013b, Levy and Gold\n gton et al. 2014). For example, the result       #\n                 # of the expression (kin\n       The parallelogram                        \u00bb  #  \u00bb  #  \u00bb\n                 #           method can solve analogies with\n is a vector close to        \u00bb\n                 #  \u00bb  queen. Similarly, Paris \u2212 France + Italy)\n       both sparse and dense embeddings (Turney and\n hat is close to Rome. The embedding model thus seems to be ex\n       Littman 2005, Mikolov et al. 2013b)\n tions of relations like MALE - FEMALE , or CAPITAL - CITY- OF, or\n       /    king \u2013 man + woman is close to queen\nIVE SUPERLATIVE, as shown in Fig. 6.16 from GloVe.\nr a         Paris \u2013 France + Italy is close to Rome\n       a:b::a*:b* problem, meaning the algorithm is given a, b, and\n , the parallelogram method is thus:\n            For a problem a:a*::b:b*, the parallelogram method is:\n                 \u02c6 \u2217                     \u2217\n                 b      = argmax distance(x, a  \u2212 a + b)\n                        x\n---\nStructure in GloVE Embedding space\n0.5                               heiress\n\n0.4\n            niece                        countess\n     0.3    aunt                         duchess\n            $ister\n     0.2                                 empress\n\n     0.1                          madam\n\n0           nephew    heir\n\n-0.1        uncle     ;woman             ue earl,\n                                         queen\n-0.2         brother                     /duke\n\n-0.3                                     emperor\n\n-0.4                    man     sir       king\n\n-0.5\n\n            -0.5  -0.4  -0.3   0.2  -0.1  0  0.1  0.2  0.3  0.4  0.5\n---\nCaveats with the parallelogram method\n\nIt only seems to work for frequent words, small\ndistances and certain relations (relating countries to\ncapitals, or parts of speech), but not others. (Linzen\n2016, Gladkova et al. 2016, Ethayarajh et al. 2019a)\n\nUnderstanding analogy is an open area of research\n(Peterson et al. 2020)\n---\nEmbeddings as a window onto historical semantics\n\nTrain embeddings on different decades of historical text to see meanings shift\n\n                      ~30 million books, 1850-1990, Google Books data\na daft gay (1900s)                 b spread                          C  solemn\nflaunting       sweet                                                   awful (1850s)\ntasteful              cheerful       broadcast (1850s) soW              awe majestic\n                       pleasant                seed                     dread       pensive\n            frolicsom\u2091             circulated                       SOWS         gloomy\n              witty gay (1950s)                  scatter\n                bright               broadcast (1900s)                      horrible\ngays        bisexual                 newspapers                             appalling terrible\n                homosexual           television                             awful (1900s)     wonderful\n gay (1990s)                         radio                                       awful (1990s)\n     lesbian                       bbcbroadcast (1990s)                          aulweird\n\n            William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic Word Embeddings Reveal\n            Statistical Laws of Semantic Change. Proceedings of ACL.\n---\nEmbeddings reflect cultural bias!\n\n   Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. \"Man is to computer\n   programmer as woman is to homemaker? debiasing word embeddings.\" In NeurIPS, pp. 4349-4357. 2016.\n\n Ask \u201cParis : France :: Tokyo : x\u201d\n \u25e6 x = Japan\n Ask \u201cfather : doctor :: mother : x\u201d\n \u25e6 x = nurse\n Ask \u201cman : computer programmer :: woman : x\u201d\n \u25e6 x = homemaker\nAlgorithms that use embeddings as part of e.g., hiring searches for\nprogrammers, might lead to bias in hiring\n---\nHistorical embedding as a tool to study cultural biases\n             Garg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes.\n             Proceedings of the National Academy of Sciences 115(16), E3635\u2013E3644.\n\n\u2022     Compute a gender or ethnic bias for each adjective: e.g., how\n      much closer the adjective is to \"woman\" synonyms than\n      \"man\" synonyms, or names of particular ethnicities\n      \u2022     Embeddings for competence adjective (smart, wise,\n            brilliant, resourceful, thoughtful, logical) are biased toward\n            men, a bias slowly decreasing 1960-1990\n      \u2022     Embeddings for dehumanizing adjectives (barbaric,\n            monstrous, bizarre) were biased toward Asians in the\n            1930s, bias decreasing over the 20\u1d57\u02b0 century.\n\u2022     These match the results of old surveys done in the 1930s\n---\nVector     Properties of Embeddings\nSemantics &\nEmbeddings\n\n",
        "quiz": [
            {
                "question_text": "What is the meaning of 'lemma' in the context of word meaning?",
                "answers": [
                    {
                        "text": "The base form of a word that carries the core meaning",
                        "is_correct": true,
                        "explanation": "The concept description defines a lemma as the base form of a word that carries the core meaning, with examples like 'mouse' having multiple senses."
                    },
                    {
                        "text": "A specific meaning of a word in a particular context",
                        "is_correct": false,
                        "explanation": "This describes a 'sense' of a word, not a lemma. The context distinguishes between lemmas and senses."
                    },
                    {
                        "text": "A synonym for a word in a different language",
                        "is_correct": false,
                        "explanation": "This describes translation equivalence, not the linguistic concept of a lemma as defined in the context."
                    },
                    {
                        "text": "A word that is opposite in meaning to another word",
                        "is_correct": false,
                        "explanation": "This describes antonymy, which is a different linguistic relation covered in the context, not the definition of a lemma."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the principle that states a difference in form implies a difference in meaning?",
                "answers": [
                    {
                        "text": "The Linguistic Principle of Contrast",
                        "is_correct": true,
                        "explanation": "The content explicitly states 'The Linguistic Principle of Contrast - Difference in form \u00e0 difference in meaning.'"
                    },
                    {
                        "text": "The Principle of Synonymy",
                        "is_correct": false,
                        "explanation": "The content discusses synonymy but does not attribute it to the principle described in the question."
                    },
                    {
                        "text": "The Principle of Semantic Similarity",
                        "is_correct": false,
                        "explanation": "The content discusses semantic similarity but does not attribute it to the principle described in the question."
                    },
                    {
                        "text": "The Principle of Antonymy",
                        "is_correct": false,
                        "explanation": "The content discusses antonymy but does not attribute it to the principle described in the question."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the three affective dimensions along which words vary according to Osgood et al. (1957)?",
                "answers": [
                    {
                        "text": "valence, arousal, and dominance",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that words vary along the three affective dimensions of valence, arousal, and dominance according to Osgood et al. (1957)."
                    },
                    {
                        "text": "happiness, sadness, and neutrality",
                        "is_correct": false,
                        "explanation": "These terms describe emotional states but are not the specific dimensions identified by Osgood et al. (1957)."
                    },
                    {
                        "text": "positive, negative, and neutral",
                        "is_correct": false,
                        "explanation": "These terms describe evaluations but are not the specific affective dimensions described by Osgood et al. (1957)."
                    },
                    {
                        "text": "intensity, control, and pleasantness",
                        "is_correct": false,
                        "explanation": "While related to the correct dimensions, these terms are not the specific labels used by Osgood et al. (1957)."
                    }
                ],
                "topic": "Vector25Aug-V2",
                "subtopic": "Main Content",
                "concepts": [
                    "Vector25Aug-V2"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    }
}