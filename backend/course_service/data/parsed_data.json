{
    "data/raw/Ngram-LMs.pdf": {
        "metadata": {
            "file_name": "Ngram-LMs.pdf",
            "file_type": "pdf",
            "content_length": 63263,
            "language": "en",
            "extraction_timestamp": "2025-11-29T21:52:53.844593+00:00",
            "timezone": "utc"
        },
        "content": "N-gram    Introduction to N-gram\nLanguage  Language Models\nModeling\n---\nPredicting words\n\nThe water of Walden Pond is beautifully ...\n\nblue            *refrigerator\ngreen           *that\nclear\n---\nLanguage Models\n\nA language model is a machine learning model\nthat predicts upcoming words.\n \u2022 More formally:\n     \u2022   An LM assigns a probability to each potential next word\n       \u2022 = gives a probability distribution over possible next words\n     \u2022   An LM assigns a probability to a whole sentence\n Two paradigms\n \u2022  N-gram language models (this lecture)\n \u2022  Large language models (LLMs, neural models, future lectures)\n---\nWhy word prediction?\n\nIt's a helpful part of language tasks\n\u2022 Grammar or spell checking\n Their are two midterms  Their There are two midterms\n Everything has improve  Everything has improve improved\n\n\u2022 Speech recognition\n I will be back soonish  I will be bassoon dish\n---\nWhy word prediction?\n\nIt's how large language models (LLMs) work!\nLLMs are trained to predict words\n\u2022 Left-to-right (autoregressive) LMs learn to predict next word\nLLMs generate text by predicting words\n\u2022 By predicting the next word over and over again\n---\n Language Modeling (LM) more formally\n\n Goal: compute the probability of a sentence or\n sequence of words W:\n P(W) = P(w\u2081,w\u2082,w\u2083,w\u2084,w\u2085\u2026w\u2099)\n Related task: probability of an upcoming word:\n P(w\u2085|w\u2081,w\u2082,w\u2083,w\u2084) or P(w\u2099|w\u2081,w\u2082\u2026w\u2099\u208b\u2081)\n An LM computes either of these:\n P(W)  or  P(w\u2099|w\u2081,w\u2082\u2026w\u2099\u208b\u2081)\n\n*Note: let's call these words for now, but for LLMs we instead use tokens\n---\n    P(blue|The water of Walden Pond is so beautifully)  (3.1)\n e way to estimate this probability is directly from relative frequency counts: take a\n    How to estimate these probabilities\n e way to estimate this probability is directly from relative frequency counts: take a\nry large corpus, count the number of times we see The water of Walden Pond\nry large corpus, count the number of times we see The water of Walden Pond\nso beautifully , and count the number of times this is followed by blue. This\nso beautifully , and count the number of times this is followed by blue. This\n uld be answering the question \u201cOut of the times we saw the history h, how many\n    Could we just count and divide?\n uld be answering the question \u201cOut of the times we saw the history h, how many\n es was it followed by the word w\u201d, as follows:\n es was it followed by the word w\u201d, as follows:\n    P(blue|The water of Walden Pond is so beautifully) ==\n    P(blue|The water of Walden Pond is so beautifully) =\n    C(The water of Walden Pond is so beautifully blue)\n    C(The water of Walden Pond is so beautifully blue)   (3.2\n    C(The water of Walden Pond is so beautifully)        (3.2)\n e had a C(The water of Walden Pond is so beautifully)\n e had a large enough corpus, we could compute these two counts and estimate\n    large enough corpus, we could compute these two counts and estimate\n probability from Eq. 3.2. But even the entire web isn\u2019t big enough to give us\n    No! Too many possible sentences!\n probability from Eq. 3.2. But even the entire web isn\u2019t big enough to give us\nod estimates for counts of entire sentences. This is because language is creative\n    We\u2019ll never see enough data for estimating these\nod estimates for counts of entire sentences. This is because language is creative;\nw sentences are invented all the time, and we can\u2019t expect to get accurate count\nw sentences are invented all the time, and we can\u2019t expect to get accurate counts\n such large objects as entire sentences. For this reason, we\u2019ll need more clever\n such large objects as entire sentences. For this reason, we\u2019ll need more clever\n---\nHow to compute P(W) or P(w\u2099|w\u2081, \u2026w\u2099\u208b\u2081)\nwith an N-gram Language Model\n\nFirst let's do the joint probability P(W):\n\nP(The, water, of, Walden, Pond, is, so, beautifully, blue)\n\nIntuition: let\u2019s rely on the Chain Rule of Probability\n---\nReminder: The Chain Rule\n\nRecall the definition of conditional probabilities\nP(B|A) = P(A,B)/P(A)  Rewriting: P(A,B) = P(A) P(B|A)\n\nMore variables:\nP(A,B,C,D) = P(A) P(B|A) P(C|A,B) P(D|A,B,C)\nThe Chain Rule in General\nP(x\u2081,x\u2082,x\u2083,\u2026,x\u2099) = P(x\u2081)P(x\u2082|x\u2081)P(x\u2083|x\u2081,x\u2082)\u2026P(x\u2099|x\u2081,\u2026,x\u2099\u208b\u2081)\n---\n       The Chain Rule applied to compute   3.1  \u2022  N-G RAM\n                                           joint\n       probability of words in sentence\nlying the chain rule to words, we get\n       P(w1:n ) = P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P(wn |w1:n\u22121 )\n                  n\n       = Y P(wk |w1:k\u22121 )\n                  k=1\n hain rule shows the link between computing the joint probability of a seq\n       P(\u201cThe water of Walden Pond\u201d) =\n omputing the conditional probability of a word given previous words.\n3.4    P(The) \u00d7 P(water|The) \u00d7 P(of|The water)\n     suggests that we could estimate the joint probability of an entire seque\n s     \u00d7 P(Walden|The water of) \u00d7    P(Pond|The water of Walden)\n       by multiplying together a number of conditional probabilities. But us\nn rule doesn\u2019t really seem to help us! We don\u2019t know any way to comp\n---\ntuition of the n-gram model is that instead of computing the proba\n   wn |wn\u22121 ). In other words, instead of computing the proba\n iven its entire history, we can approximate the history by just t\nm       Markov Assumption\n.  model, for example, approximates the probability of a word gi\ne water of Walden Pond is so beautifully)\n words  P(wn |w1:n\u22121 ) by using only the conditional probability of\n e bigram model, for example, approximates the probability of a\nd P(wSimplifying assumption:\n        |w  ). In other words, instead of computing the probabili\n ith the probability\n        n  n\u22121\n   revious words P(wn |w1:n\u22121 ) by using only the conditional probab\n ing word P(w |w  ). In other words, instead of computing the pr\ne|The water of Walden            A. A. Ma\u03c1Ron (1886).\n              n  n\u22121      Pond is so beautifully)    (\n   P(blue     P(blue|beautifully)  Andrei Markov\nte it      |The water of Walden Pond is so beautifully)\n        with the probability\n m model to predict the conditional probability of the next w\n roximate it with the probability\nhe          \u2248    P(blue|beautifully)                 (\n     following approximation:\n   igram model to     P(blue|beautifully)\n            P(w predict the conditional probability of the next word\n   ing the       n |w1:n\u22121 ) \u2248 P(wn |wn\u22121 )\nwe use following approximation:\n        a bigram model to predict the conditional probability of the\n                                           Wikimedia commons\n---\na complete word sequence by substituting Eq.  3.\n    Bigram Markov Assumption\n    n\n    P(w ) \u2248 Y P(w |w        )\n g the chain rule to words, we get\n        1:n    k      k \u22121\n    P(w ) =    k =1\n    1:n        P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P\n    instead of:    n\n                   = Y P(wk |w1:k\u22121 )\n                   k=1\nin rule shows the link between computing the join\n    We approximate each component in the product\n---\n next word in a sequence. We\u2019ll use      N here to\n    General N-gram model for each component\n eans bigrams and N = 3 means trigrams. Then w\n ord given its entire context as follows:\n    P(wn |w1:n\u22121 ) \u2248 P(wn |wn\u2212N +1:n\u22121 )\n assumption for the probability of an individual wo\nity of a complete word sequence by substituting Eq\n   n\n   Y\n---\n Bigram model\n     P ( w i | w\u2081w 2 \u2026 w i \u2212\u2081 ) \u2248 P ( w i | w i \u2212\u2081 )\n\nSome automatically generated sentences from Shakespeare bigram models\n Why dost stand forth thy canopy, forsooth; he                       is\n this palpable hit the King Henry. Live king.\n Follow.\n\n What means, sir. I confess she? then all sorts,                       he\n is  trim, captain.\n---\n  Even simpler Markov assumption: Unigram model\n\n              P ( w\u2081w 2 \u2026 w n ) \u2248 \u220f P ( w i )\n                                 i\n\nSome automatically generated sentences from Shakespeare unigram models\n  To  him swallowed confess   hear both . Which .\n  Of  save on trail for are   ay device and  rote\n  \u20ac\n  life have\n\n  Hill he late speaks   ; or  !  a more to leg less\n  first you   enter\n---\n More complex Markov assumption: Trigram model\n\n \ud835\udc43 \ud835\udc64! \ud835\udc64\" \ud835\udc64# \u2026 \ud835\udc64! $\"  \u2248 \ud835\udc43 \ud835\udc64! \ud835\udc64! $# \ud835\udc64! $\"\n\nSome automatically generated sentences from Shakespeare trigram models\n Fly, and will rid me these news of price.\n Therefore the sadness of parting,  as they say,\n \u2019tis done.\n\n This shall forbid it  should be branded, if    renown\n made it  empty\n---\n             of modeling the training corpus as we increase the value of N .\n             We can use the sampling method from the prior section to visualize both of\n4 different N-gram models\n             these facts! To give an intuition for the increasing power of higher-order n-grams,\n             Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-\n             gram models trained on Shakespeare\u2019s works.\n\n 1          \u2013To him swallowed confess hear both. Which. Of save on trail for are ay device and\n            rote life have\n gram       \u2013Hill he late speaks; or! a more to leg less first you enter\n 2          \u2013Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\n            king. Follow.\n gram       \u2013What means, sir. I confess she? then all sorts, he is trim, captain.\n 3          \u2013Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n            \u2019tis done.\n gram       \u2013This shall forbid it should be branded, if renown made it empty.\n 4          \u2013King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\n            great banquet serv\u2019d in;\n gram       \u2013It cannot be but so.\nFigure 3.4  Eight sentences randomly generated from four n-grams computed from Shakespeare\u2019s works. All\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\nfor capitalization to improve readability.\n---\nProblems with N-gram models\n1.         N-grams can't handle long-distance dependencies:\n\u201cThe soups that I made from that new cookbook I bought yesterday\n      were amazingly delicious.\"\n2.         N-grams don't do well at modeling new sequences (even\n           if they have similar meanings to sequences they've seen)\nThe solution: Large language models\n      \u2022     can handle much longer contexts\n      \u2022     because they use neural embedding spaces, can model\n            meaning better\n---\nWhy study N-gram models?\n\nThey are the \"fruit fly of NLP\", a simple model that\nintroduces important topics for large language models\n \u2022     training and test sets\n \u2022     the perplexity metric\n \u2022     sampling to generate sentences\n \u2022     ideas like interpolation and backoff\n N-grams are efficient and fast and useful in situations\n where LLMs are too heavyweight\n---\nN-gram    Introduction to N-gram\nLanguage  Language Models\nModeling\n---\nN-gram    Estimating N-gram\nLanguage  Probabilities\nModeling\n---\n etween 0 and 1.\n he bigrams that share the same first word  wn\u22121 :\nto compute a particular bigram probability of a word  w\n    Estimating bigram probabilities                   n\n    , we\u2019ll compute the   C(w  w )\n                          count of the bigram C(w  w ) and\nn\u22121  P(wn |wn\u22121 ) = P     n\u22121  n            n\u22121    n\nl the bigrams that share the same first word  w    :\n     The Maximum          C(wn\u22121 w)                n\u22121\n                   Likelihood Estimate\n                          w\ny this equation, since the sum of all bigram counts that star\n                          C(wn\u22121 wn )\n must be  P(w |w   ) = P\n          equal to the unigram count for that word w  (the\n nt to be   n   n\u22121        w C(wn\u22121 w)             n\u22121\nify this    convinced of this):\n          equation, since the sum of all bigram counts that st\n   must be equal to the   C(w  w )\n                     unigram count for that word w    (th\n 1          P(wn |wn\u22121 ) =     n\u22121 n                  n\u22121\n ent to be convinced of this):\n                            C(wn\u22121 )\n ugh an example using a mini-corpus of three sentences.\n                               C(wn\u22121 wn )\n---\n  An example\n\n   ~~ I am Sam ~~      P ( w i | w i\u2212\u2081 ) = c ( w i\u2212\u2081, w i )\n   ~~ Sam I am ~~                          c ( w i\u2212\u2081 )\n   ~~ I do not like green eggs and ham ~~  \n\nP(I| <s>=    \u20ac\n  2          P(Sam <s>)=                      P(am| I) =2\n             =.67    1= .33                               =.67\n             2                                P(do| I) = 1\nP(</s> | Sam) = = 0.5  P(Sam | am) =\u00b9 = .5                = .33\n                       2\n---\nMore examples:\nBerkeley Restaurant Project sentences\n\n can you tell me about any good cantonese restaurants close by\n tell me about chez panisse\n i\u2019m looking for a good place to eat breakfast\n when is caffe venezia open during the day\n---\n        i\u2019m looking for a good place to eat breakfast\n      Raw bigram counts\n        when is caffe venezia open during the day\n          Figure 3.1 shows the bigram counts from part of a bigram grammar from text-\nnormalized Berkeley Restaurant Project sentences.       Note that the majority of the\n      Out of 9222 sentences\nvalues are zero. In fact, we have chosen the sample words to cohere with each other;\na matrix selected from a random set of eight words would be even more sparse.\n\n             i      want     to      eat         chinese     food     lunch     spend\n i           5      827      0       9           0           0        0         2\n want        2      0        608     1           6           6        5         1\n to          2      0        4       686         2           0        6         211\n eat         0      0        2       0           16          2        42        0\n chinese     1      0        0       0           0           82       1         0\n food        15     0        15      0           1           4        0         0\n lunch       2      0        0       0           0           1        0         0\n spend       1      0        1       0           0           0        0         0\nFigure 3.1  Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-\n---\n means that want followed i 827 times in the corpus.\n         Raw bigram probabilities\n         Figure 3.2 shows the bigram probabilities after normalization (dividing each c\n in Fig. 3.1 by the appropriate unigram for its row, taken from the following set\n unigram counts):\n         Normalize by unigrams:\n                         i         want        to           eat  chinese        food  lunch    spend\nC          3       \u2022     2533      927         2417         746  158            1093  341      278\n   HAPTER               N-GRAM L ANGUAGE M ODELS\n         Result:\n Here are a few other useful probabilities:\n                              i           want        to          eat      chinese     food     lunch     spend\n   P(i|<s>) = 0.25                                   P(english|want) = 0.0011\n              i               0.002       0.33        0           0.0036   0           0        0         0.00079\n   P(         want            0.0022      0           0.66        0.0011   0.0065      0.0065   0.0054    0.0011\n         food|english) = 0.5                         P(</s>|food) = 0.68\n              to              0.00083     0           0.0017      0.28     0.00083     0        0.0025    0.087\n         Now we can compute the probability of sentences like I want English food\n I            eat             0           0           0.0027      0        0.021       0.0027   0.056     0\n       want Chinese food by simply multiplying the appropriate bigram probabilities t\n              chinese         0.0063      0           0           0        0           0.52     0.0063    0\n              food            0.014       0           0.014       0        0.00092     0.0037   0         0\n gether, as follows:\n              lunch           0.0059      0           0           0        0           0.0029   0         0\n                        P( ~~ i want english food ~~  )\n              spend           0.0036      0           0.0036      0        0           0        0         0\n---\nBigram estimates of sentence probabilities\n\nP( ~~ I want english food ~~  ) =\nP(I|<s>)\n\u00d7 P(want|I)\n\u00d7 P(english|want)\n\u00d7 P(food|english)\n\u00d7 P(</s>|food)\n= .000031\n---\nWhat kinds of knowledge do N-grams represent?\n\nP(to|want) = .66                  Knowledge of grammar\nP(want | spend) = 0               (\"want\" is followed by infinitive \"to\")\nP(of | to) = 0                    (\"of\" is not a verb)\n\nP (dinner|lunch or) = .83         Knowledge of meaning\nP(dinner|for) ~ P(lunch|for)     The words \"dinner\" or \"lunch\" are\n                                 semantically related.\n\nP(chinese|want) > P(english|want)     Knowledge about the world\n                                      Chinese food is very popular\n---\n.1.3  Dealing with scale in large n-gram models\nn     Dealing with scale in large n-grams\n    practice, language models can be very large, leading to practical issues.\nog probabilities  Language model probabilities are always stored and com\nn     LM probabilities are stored and computed in\n    log format, i.e., as log probabilities. This is because probabilities are (b\n      log format, i.e. log probabilities\nnition) less than or equal to 1, and so the more probabilities we multiply to\nhe smaller the product becomes. Multiplying enough n-grams together would\n      This avoids underflow from multiplying many\nn numerical underflow. Adding in log space is equivalent to multiplying in\npace, small numbers\n      so we combine log probabilities by adding them. By adding log proba\nnstead of multiplying probabilities, we get results that are not as small. We\n      log( p\u2081 \u00d7 p\u2082 \u00d7 p\u2083 \u00d7 p\u2084 ) = log p\u2081 + log p\u2082 + log p\u2083 + log p\u2084\nomputation and storage in log space, and just convert back into probabilitie\need to report probabilities at the end by taking the exp of the logprob:\n      If we need probabilities we can do one exp at the end\n      p1 \u21e5 p2 \u21e5 p3 \u21e5 p4 = exp(log p1 + log p2 + log p3 + log p4 )\n---\nLarger ngrams\n\n4-grams, 5-grams\nLarge datasets of large n-grams have been released\n \u2022  N-grams from Corpus of Contemporary American English (COCA)\n    1 billion words (Davies 2020)\n \u2022  Google Web 5-grams (Franz and Brants 2006) 1 trillion words)\n \u2022  Efficiency: quantize probabilities to 4-8 bits instead of 8-byte float\n Newest model: infini-grams (\u221e-grams) (Liu et al 2024)\n \u2022  No precomputing! Instead, store 5 trillion words of web text in\n    suffix arrays. Can compute n-gram probabilities with any n!\n---\nN-gram LM Toolkits\n\nSRILM\n\u25e6 http://www.speech.sri.com/projects/srilm/\nKenLM\n\u25e6 https://kheafield.com/code/kenlm/\n---\nN-gram    Estimating N-gram\nLanguage  Probabilities\nModeling\n---\n        Evaluation and Perplexity\n\nLanguage\nModeling\n---\nHow to evaluate N-gram models\n\n\"Extrinsic (in-vivo) Evaluation\"\nTo compare models A and B\n1.  Put each model in a real task\n \u2022  Machine Translation, speech recognition, etc.\n2.  Run the task, get a score for A and for B\n \u2022  How many words translated correctly\n \u2022  How many words transcribed correctly\n3.  Compare accuracy for A and B\n---\nIntrinsic (in-vitro) evaluation\n\nExtrinsic evaluation not always possible\n\u2022  Expensive, time-consuming\n\u2022  Doesn't always generalize to other applications\nIntrinsic evaluation: perplexity\n\u2022  Directly measures language model performance at\n   predicting words.\n\u2022  Doesn't necessarily correspond with real application\n   performance\n\u2022  But gives us a single general metric for language models\n\u2022  Useful for large language models (LLMs) as well as n-grams\n---\nTraining sets and test sets\n\nWe train parameters of our model on a training set.\nWe test the model\u2019s performance on data we\nhaven\u2019t seen.\n \u25e6   A test set is an unseen dataset; different from training set.\n   \u25e6 Intuition: we want to measure generalization to unseen data\n \u25e6   An evaluation metric (like perplexity) tells us how well\n     our model does on the test set.\n---\nChoosing training and test sets\n\n\u2022 If we're building an LM for a specific task\n  \u2022  The test set should reflect the task language we\n     want to use the model for\n\u2022 If we're building a general-purpose model\n  \u2022  We'll need lots of different kinds of training\n     data\n  \u2022  We don't want the training set or the test set to\n     be just from one domain or author or language.\n---\nTraining on the test set\n\nWe can\u2019t allow test sentences into the training set\n \u2022  Or else the LM will assign that sentence an artificially\n    high probability when we see it in the test set\n \u2022  And hence assign the whole test set a falsely high\n    probability.\n \u2022  Making the LM look better than it really is\nThis is called \u201cTraining on the test set\u201d\nBad science!\n\n                                                            38\n---\nDev sets\n\n\u2022 If we test on the test set many times we might\nimplicitly tune to its characteristics\n \u2022 Noticing which changes make the model better.\n\u2022 So we run on the test set only once, or a few times\n\u2022 That means we need a third dataset:\n \u2022 A development test set or, devset.\n \u2022 We test our LM on the devset until the very end\n \u2022 And then test our LM on the test set once\n---\n Intuition of perplexity as evaluation metric:\n How good is our language model?\nIntuition: A good LM prefers \"real\" sentences\n\u2022     Assign higher probability to \u201creal\u201d or \u201cfrequently\n      observed\u201d sentences\n\u2022     Assigns lower probability to \u201cword salad\u201d or\n      \u201crarely observed\u201d sentences?\n---\n Intuition of perplexity 2:\n Predicting upcoming words\n                    The Shannon Game: How well can we        time      0.9\n                    predict the next word?                   dream     0.03\n                    \u2022  Once upon a ____                      midnight 0.02\n                    \u2022  That is a picture of a ____           \u2026\n                    \u2022  For breakfast I ate my usual ____\n                                                             and       1e-100\nClaude   233355     Unigrams are terrible at this game (Why?)\n         Shannon\n\nA good LM is one that assigns a higher probability\nto the next word that actually occurs\n                                                         Picture credit: Historiska bildsamlingen\n                                                         https://creativecommons.org/licenses/by/2.0/\n---\nIntuition of perplexity 3: The best language model\nis one that best predicts the entire unseen test set\n \u2022  We said: a good LM is one that assigns a higher\n    probability to the next word that actually occurs.\n \u2022  Let's generalize to all the words!\n     \u2022     The best LM assigns high probability to the entire test\n           set.\n \u2022  When comparing two LMs, A and B\n     \u2022     We compute PA(test set) and PB(test set)\n     \u2022     The better LM will give a higher probability to (=be less\n           surprised by) the test set than the other LM.\n---\nIntuition of perplexity 4: Use perplexity instead of\nraw probability\n\u2022  Probability depends on size of test set\n    \u2022     Probability gets smaller the longer the text\n    \u2022     Better: a metric that is per-word, normalized by length\n\u2022  Perplexity is the inverse probability of the test set,\n   normalized by the number of words\n\n          PP (W )  = P (w w ...w )\u2212 1\n                     1 2         N  N\n\n                   = N P (w 1\n                       1w2 ...w N )\n---\nIntuition of perplexity 5: the inverse\n\nPerplexity is the inverse probability of the test set,\nnormalized by the number of words\n\nPP(W ) = P(w w ... w )\u2212 1\n         1 2         N  N\n\n       = N P(w1w1 ... w )\n              2         N\n(The inverse comes from the original definition of perplexity\nfrom cross-entropy rate in information theory)\nProbability range is [0,1], perplexity range is [1,\u221e]\nMinimizing perplexity is the same as maximizing probability\n---\nIntuition of perplexity 6: N-grams\n\nPP (W )  = P ( w w ...w )\u2212 1\n           1 2          N  N\n\n         = N P ( w1w1 ...w )\n                   2       N\n\n                   N        1\nChain rule:  PP(W)  N II\n             =      i=1P(wi| 1 . . . Wi\u22121)\n\n                    N       1\nBigrams:     PP(W)  N \u2161IP(i|i\u22121\n                    i=1\n---\n   Intuition of perplexity 7:\n   Weighted average branching factor\n\nPerplexity is also the weighted average branching factor of a language.\nBranching factor: number of possible next words that can follow any word\nExample: Deterministic language L = {red,blue, green}\n   Branching factor = 3 (any word can be followed by red, blue, green)\nNow assume LM A where each word follows any other word with equal probability \u2153\nGiven a test set T = \"red red red red blue\"\nPerplexityA(T) = PA(red red red red blue)-1/5 =  ((\u2153)\u2075)-1/5  = (\u2153)\u207b\u00b9  =3\nBut now suppose red was very likely in training set, such that for LM B:\n\u25e6  P(red) = .8 p(green) = .1 p(blue) = .1\nWe would expect the probability to be higher, and hence the perplexity to be smaller:\nPerplexityB(T) = PB(red red red red blue)-1/5\n   = (.8 * .8 * .8 * .8 * .1) -1/5  =.04096 -1/5     = .527\u207b\u00b9  = 1.89\n---\n Holding test set constant:\n Lower perplexity = better language model\n\nTraining 38 million words, test 1.5 million words, WSJ\n\nN-gram     Unigram  Bigram  Trigram\nOrder\nPerplexity 962      170     109\n---\n        Evaluation and Perplexity\n\nLanguage\nModeling\n---\n        Sampling and Generalization\n\nLanguage\nModeling\n---\nThe Shannon (1948) Visualization Method\nSample words from an LM\n\n Unigram:                                          233355\n                                             Claude Shannon\n\n REPRESENTING AND SPEEDILY IS AN GOOD APT OR COME\n CAN DIFFERENT NATURAL HERE HE THE A IN CAME THE TO\n OF TO EXPERT GRAY COME TO FURNISHES THE LINE\n MESSAGE HAD BE THESE.\n\n Bigram:\nTHE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER\nTHAT THE CHARACTER OF THIS POINT IS THEREFORE\nANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHO\nEVER TOLD THE PROBLEM FOR AN  UNEXPECTED.\n---\nHow Shannon sampled those words in 1948\n\n\"Open a book at random and select a letter at random on the page.\nThis letter is recorded. The book is then opened to another page\nand one reads until this letter is encountered. The succeeding\nletter is then recorded. Turning to another page this second letter\nis searched for and the succeeding letter recorded, etc.\"\n---\nSampling a word from a distribution  8  2\n                                     %8 36 99 0\n                                     5230 12984\n                                     291082 64 402223\n                                     11 87 69 5341\n                                     2228899 88 8 g2\n                                        A2 L9G8\n\npolyphonic\n\nthe  of  a  to in    \u2026 however  p=.0000018\n                     (p=.0003)\n\n0.06  0.03  0.02 0.02 0.02\n                          \u2026     \u2026\n\n0     .06  .09 .11 .13 .15      .66       .99 1\n---\n  Visualizing Bigrams the Shannon Way\n\n Choose a random bigram (<s>, w)       <s> I\n  according to its probability p(w|<s>)  I        want\n Now choose a random bigram            (w, x)     want to\naccording to its probability p(x|w)                   to eat\n And so on until we choose </s>                          eat Chinese\n Then string the words together                             Chinese food\n                                                                           food </s>\n                                                 I want to eat Chinese food\n---\nNote: there are other sampling methods\n\nUsed for neural language models\nMany of them avoid generating words from the very\nunlikely tail of the distribution\nWe'll discuss when we get to neural LM decoding:\n \u25e6  Temperature sampling\n \u25e6  Top-k sampling\n \u25e6  Top-p sampling\n---\n             We can use the sampling method from the prior section to visualize both of\n            Approximating Shakespeare\n             these facts! To give an intuition for the increasing power of higher-order n-grams,\n             Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-\n             gram models trained on Shakespeare\u2019s works.\n\n1           \u2013To him swallowed confess hear both. Which. Of save on trail for are ay device and\n            rote life have\ngram        \u2013Hill he late speaks; or! a more to leg less first you enter\n2           \u2013Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\n            king. Follow.\ngram        \u2013What means, sir. I confess she? then all sorts, he is trim, captain.\n3           \u2013Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n            \u2019tis done.\ngram        \u2013This shall forbid it should be branded, if renown made it empty.\n4           \u2013King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\n            great banquet serv\u2019d in;\ngram        \u2013It cannot be but so.\nFigure 3.4  Eight sentences randomly generated from four n-grams computed from Shakespeare\u2019s works. All\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\n---\nShakespeare as corpus\n\nN=884,647 tokens, V=29,066\nShakespeare produced 300,000 bigram types out of\nV\u00b2= 844 million possible bigrams.\n \u25e6  So 99.96% of the possible bigrams were never seen (have\n    zero entries in the table)\n \u25e6  That sparsity is even worse for 4-grams, explaining why\n    our sampling generated actual Shakespeare.\n---\nThe Wall Street Journal is not Shakespeare\n            3.5                        \u2022  G ENERALIZATION AND Z EROS               13\n\n1           Months the my and issue of year foreign new exchange\u2019s september\ngram        were recession exchange new endorsed a acquire to six executives\n2           Last December through the way to preserve the Hudson corporation N.\n            B. E. C. Taylor would seem to complete the major central planners one\ngram        point five percent of U. S. E. has already old M. X. corporation of living\n            on information such as more frequently fishing to keep her\n3           They also point to ninety nine point six billion dollars from two hundred\n            four oh six three percent of the rates of interest stores as Mexico and\ngram        Brazil on market conditions\nFigure 3.5  Three sentences randomly generated from three n-gram models computed from\n40 million words of the Wall Street Journal, lower-casing all characters and treating punctua-\n---\nCan you guess the author? These 3-gram sentences\nare sampled from an LM trained on who?\n 1)  They also   point to ninety nine point\n six  billion dollars from two hundred four\n oh  six  three  percent of the rates of\n interest   stores  as Mexico and gram Brazil\n on  market conditions\n 2)  This shall  forbid it should be branded,\n if  renown made    it empty.\n 3)  \u201cYou are uniformly   charming!\u201d cried he,\n with  a  smile  of associating and  now and\n then  I  bowed  and they perceived  a chaise\n and  four  to wish for.\n                                        58\n---\nChoosing training data\n\nIf task-specific, use a training corpus that has a similar\ngenre to your task.\n\u2022   If legal or medical, need lots of special-purpose documents\nMake sure to cover different kinds of dialects and\nspeaker/authors.\n\u2022   Example: African-American Vernacular English (AAVE)\n  \u2022  One of many varieties that can be used by African Americans and others\n  \u2022  Can include the auxiliary verb finna that marks immediate future tense:\n  \u2022  \"My phone finna die\"\n---\nThe perils of overfitting\n\nN-grams only work well for word prediction if the\ntest corpus looks like the training corpus\n \u2022  But even when we try to pick a good training\n    corpus, the test set will surprise us!\n \u2022  We need to train robust models that generalize!\nOne kind of generalization: Zeros\n   \u2022 Things that don\u2019t ever occur in the training set\n    \u2022 But occur in the test set\n---\nZeros\nTraining set:           \u2022 Test set\n\u2026 ate lunch               \u2026 ate lunch\n\u2026 ate dinner              \u2026 ate breakfast\n\u2026 ate a\n\u2026 ate the\nP(\u201cbreakfast\u201d | ate) = 0\n---\nZero probability bigrams\n\nBigrams with zero probability\n \u25e6  Will hurt our performance for texts where those words\n    appear!\n \u25e6  And mean that we will assign 0 probability to the test set!\nAnd hence we cannot compute perplexity (can\u2019t\ndivide by 0)!\n---\n        Sampling and Generalization\n\nLanguage\nModeling\n---\nN-gram    Smoothing, Interpolation,\nLanguage  and Backoff\nModeling\n---\nThe problem of sparsity\n\nMaximum likelihood estimation has a problem\n\u2022 Sparsity! Our finite training corpus won't have\nsome perfectly fine sequences\n\u2022  Perhaps it has \"ruby\" and \"slippers\"\n\u2022  But happens not to have \"ruby slippers\"\n---\nThis sparsity can take many forms\n\nVerbs have direct objects; those can be sparse too!\n\nCount(w | denied the)\n\nCounts:              allegations\n 3 allegations       reports     outcome\n 2 reports                      requestattack    \u2026\n 1 claims            claims      man\n 1 request\n 0 attack\n 0 outcome\n 7 total\n---\nThe intuition of smoothing\n                               (Example modified from Dan Klein!)\nWhen we have sparse statistics:\nCount(w | denied the)          allegations\n3 allegations\n2 reports                      reports     outcome\n1 claims                                   attack  \u2026\n                                           request\n1 request                                  claims    man\n7 total\nSteal probability mass to generalize better\nCount(w | denied the)\n 2.5 allegations\n 1.5 reports                   allegations              outcome\n 0.5 claims                                          attack\n 0.5 request                   reports               man   \u2026\n 2 other                                   claims request\n 7 total\n---\n  lunch     3  1                 1               1                    1             2         1\n  spend     2  1            2          1              1                    1             1\n  spend     2  1            2          1              1                    1             1\n  Add-one estimation\n  spend        2  1              2               1                    1             1         1\nFigure 3.6  Add-one smoothed bigram counts for eight of the words (out of V\nFigure 3.6  Add-one smoothed bigram counts for eight of the words (out of V\n  Figure 3.6  Add-one smoothed bigram counts for eight of the words (out\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero count\n  the Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero co\n \u2022 Also called Laplace smoothing\n  Figure 3.7 shows the add-one smoothed probabilities for the bigrams\n  Figure 3.7 shows the add-one smoothed probabilities for the bigrams\n \u2022 Pretend we saw each word one more time than we did\nRecall that normal bigram probabilities are computed by normalizing e\nRecall that normal bigram probabilities are computed by normalizing e\n  Figure 3.7 shows the add-one smoothed probabilities for the bigra\ncounts by the unigram count:\ncounts by the unigram count:\n  Recall that normal bigram probabilities are computed by normalizin\n \u2022 Just add one to all the counts!\n  counts by the unigram count:                        C (w                 w )\n                                                      C (w                 w )\n                                                                           n\n                                                            n\u22121            n\n                     P(w |w            ) =                  n\u22121\n                     P(w |w             ) =\n                               n\n                               n       n\u22121\n                                       n\u22121            C (w                 )\n                                                      C (w                 )\n  MLE estimate:                                             C (w              w )\n                                                            n\u22121\n                     PMLE (wn |wn\u22121 ) =                               n\u22121 n\u22121 n\nFor add-one smoothed bigram counts,                                   C (w    )\nFor add-one smoothed bigram                 we need to augment the unigram co\n                               counts, we need to augment the unigram c\n                                                                            n\u22121\nnumber of total word types in the vocabulary V :\nnumber of total word types in the vocabulary V :\n  For add-one smoothed bigram counts, we need to augment the unigra\n  Add-1 estimate:\n  number of total word types in the vocabulary V :\n                                      C (w            w ) + 1                 C (w       w ) + 1\n                                       C (w           w ) + 1                 C (w       w ) + 1\n                                                      n\n                                                      n                                  n\n                                            n                                            n\n                                             n\n                                             \u2212\n                                                 \u2212\n                                                 1\n                                                      1                          n\n                                                                                    n\n                                                                                    \u2212\n                                                                                     \u2212\n                                                                                         1\n                                  P                                                      1\n         P        (w |w  ) =      P                                         =\n         P         (w |w  ) =                                               =\n            Laplace\n            Laplace  n\n                     n  n\n                          n\n                          \u2212\n                          \u2212\n                           1\n                           1           (C (w               w) + 1)             C (w       ) + V\n                                       (C (w               w) + 1)             C (w       ) + V\n                                                 n\n                                                      n\n                                                      \u2212\n                                                           \u2212\n                                                           1\n                                                            1                        n\n                                                                                     n\n                                                                                         \u2212\n                                                                                         \u2212\n                                                                                          1\n                                       w C (w               w ) + 1            C (w 1      w ) +\n                                       w         n\u22121             n                        n\u22121 n\n---\nMaximum Likelihood Estimates\n\nThe maximum likelihood estimate\n \u25e6     of some parameter of a model M from a training set T\n \u25e6     maximizes the likelihood of the training set T given the model M\nSuppose the word \u201cbagel\u201d occurs 400 times in a corpus of a million words\nWhat is the probability that a random word from some other text will be\n\u201cbagel\u201d?\nMLE estimate is 400/1,000,000 = .0004\nThis may be a bad estimate for some other corpus\n \u25e6     But it is the estimate that makes it most likely that \u201cbagel\u201d will occur 400 times\n       in a million word corpus.\n---\n                                                   c\u2217\n                                           d =     i\n ts,      Berkeley Restaurant Corpus: Laplace\n     we need to augment the unigram count by the\n          smoothed                           i     ci\n abulary             V : bigram counts\n   Now that we have the intuition for the unigram case, let\u2019s smooth our Berkeley\n   Restaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the\n  C(w          w ) + 1              C(w            w ) + 1\n   bigrams in Fig. 3.1.\n     (Cn\u22121     n                 =         n\u22121       n                        (3.24)\n  w         (wn\u22121iw) + 1)                 C(wn\u22121 ) + V\n                         want       to      eat           chinese     food     lunch     spend\n     i          6        828        1       10            1           1        1         3\n en in the previous section will need to be aug-\n     want       3        1          609     2             7           7        6         2\nthe smoothed bigram probabilities in Fig.                                   3.7.\n     to         3        1          5       687           3           1        7         212\n     eat        1        1          3       1             17          3        43        1\n uct the count matrix so we can see how much a\nhe chinese      2        1          1       1             1           83       2         1\n     original counts. These adjusted counts can be\n     food       16       1          16      1             2           5        1         1\n  shows the reconstructed counts.\n     lunch      3        1          1       1             1           2        1         1\n     spend      2        1          2       1             1           1        1         1\n  [C(w         w ) + 1] \u00d7 C(w                )\n   Figure 3.6  Add-one smoothed bigram counts for eight of the words (out of V = 1446) in\n            n\u22121 n                         n\u22121\n---\n                               P(w |w                   ) =\n                               P(w |w                    ) =                                                                 (3.2\n                                         n                                                                                   (3.2\n                                         n          n\u22121\n                                                    n\u22121              C (w    )\n                                                                     C (w    )\n                                                                           n\u22121\n         Laplace-                                                           n\u22121\n add-one                   smoothed bigrams\n             smoothed bigram counts, we need to augment the unigram count by th\nr add-one smoothed bigram counts, we need to augment the unigram count by t\n  ber of total word types in the vocabulary V :\nmber of total word types in the vocabulary V :\n                                                    C (w        w ) + 1               C (w               w ) + 1\n                                                    C (w            w ) + 1                C (w          w ) + 1\n                                                                     n                                   n\n                                                          n\u22121        n                              n\u22121  n\n                                         P                n\u22121                                        n\u22121\n    P                (w |w     ) =            P                                      =\n    P                (w |w            ) =                                             =                                      (3.2\n         Laplace                                                                                                             (3.2\n          Laplace       n\n                        n   n\u22121\n                            n\u22121                       (C (w               w) + 1)          C (w          ) + V\n                                                      (C (w               w) + 1)          C (w          ) + V\n                                                             n\u22121                                     n\u22121\n    16       C          3  \u2022   N-             L               n\u22121                                     n\u22121\n                                                    w        M\n                HAPTER             GRAM             w\n                                                   ANGUAGE          ODELS\n s, each of the unigram counts given in the previous section will need to be aug\nus, each of the unigram counts given in the previous section will need to be au\nnted by V =     i             want            to             eat           chinese         food          lunch       spend\n nted by V = 1446. The result is the smoothed bigram probabilities in Fig. 3.7.\n    i           1446. The result is the smoothed bigram probabilities in Fig. 3.7.\n                0.0015        0.21            0.00025        0.0025        0.00025         0.00025       0.00025     0.00075\n    want        0.0013        0.00042         0.26           0.00084       0.0029          0.0029        0.0025      0.00084\n It is often convenient to reconstruct the count matrix so we can see how much\n It is often convenient to reconstruct the count matrix so we can see how much\n    to          0.00078       0.00026         0.0013         0.18          0.00078         0.00026       0.0018      0.055\noothing algorithm has changed the original counts. These adjusted counts can b\noothing algorithm has changed the original counts. These adjusted counts can\n    eat         0.00046       0.00046         0.0014         0.00046       0.0078          0.0014        0.02        0.00046\n puted by Eq. 3.25. Figure 3.8 shows the reconstructed counts.\n  puted by Eq. 3.25. Figure 3.8 shows the reconstructed counts.\n    chinese     0.0012        0.00062         0.00062        0.00062       0.00062         0.052         0.0012      0.00062\n    food        0.0063        0.00039         0.0063         0.00039       0.00079         0.002         0.00039     0.00039\n    lunch       0.0017        0.00056         0.00056        0.00056       0.00056         0.0011        0.00056     0.00056\n                                                   [C (w        w ) + 1] \u00d7 C (w                      )\n                                                    [C (w           w ) + 1] \u00d7 C (w                   )\n                                                                     n\n                                                          n          n\n                                                           n\n                                                             \u2212\n                                                             \u2212\n                                                              1\n                                                              1                            n\n                                                                                           n\n                                                                                            \u2212\n                                                                                            \u2212\n                                                                                                    1\n                     \u2217                                                                              1\n                     c \u2217\n                     c (w      w ) =\n    spend                 (w       w ) =\n                0.0012        0.00058         0.0012         0.00058       0.00058         0.00058       0.00058     0.00058 (3.2\n                                      n                                                                                      (3.2\n                           n          n\n                              n\n                              \u2212\n                              \u2212\n                               1\n                               1                              C (w           ) + V\n    Figure 3.7       Add-one smoothed bigram                  C (w           ) + V\n                                                    probabilities for eight of the words (out of V = 1446) in the BeRP\n                                                                          n\n                                                                          n\n                                                                           \u2212\n                                                                           \u2212\n                                                                            1\n    corpus of 9332 sentences. Previously-zero probabilities are in           1\n Note that add-one smoothing has made a                                    gray.\n Note that add-one smoothing has made                                     very big change to the counts. Com\n                                                                          a very big change to the counts. Com\n---\noothed probability. This adjusted count is easier to compare direct\n056   0.00056       0.00056  0.00056  0.0011  0.00056  0\nLE counts. That is, the Laplace probability can equally be expresse\nE counts. That is, the Laplace probability can equally be expresse\n058   Visualization Technique: Reconstituted counts\nted   0.0012        0.00058  0.00058  0.00058  0.00058  0\n      count divided by the (non-smoothed) denominator from Eq. 3.25:\ned count divided by the (non-smoothed) denominator from Eq. 3.25:\ned bigram probabilities for eight of the words (out of V = 1446) in th\n                                      \u2217\n                             C(w  w ) + 1  C\u2217 (w       w )\nted by Eq.  3.26.            C(w  w ) + 1  C (w        w )\n               Previously-zero probabilities are in gray.\n                                  n\u22121 n                n\u22121 n\n      P       (w |w   ) =    n\u22121      n    =           n\u22121 n\n      P       (w |w   ) =                  =\n         Laplace      n  n\u22121\n      Laplace     n      n\u22121  C(w     ) + V   C(w          )\n                              C(w n\u22121) + V    C(w n\u22121)\n                                  n\u22121                  n\u22121\ng terms, we can solve for C \u2217 (wn\u22121 wn ) :\n\n      C \u2217 (wn\u22121 wn ) = [C (wn\u22121 wn ) + 1] \u00d7 C (wn\u22121 )\n                              C (wn\u22121 ) + V\nshows the reconstructed counts, computed by Eq. 3.27.\n i       want         to      eat        chinese  food      lunch\n---\nant          0.0013        0.00042       0.26         0.00084        0.0029           0.0029           0.0025      0.00084\n 0.00078                   0.00026       0.0013       0.18           0.00078          0.00026          0.0018      0.055\n             Reconstituted counts C*\n ting terms, we can solve for C \u2217 (w                                      w ) :\n             0.00046       0.00046       0.0014       0.00046        0.0078     n     0.0014           0.02        0.00046\n inese       0.0012        0.00062       0.00062      0.00062        n\u22121\n                                                                     0.00062          0.052            0.0012      0.00062\n od          0.0063        0.00039       0.0063     [ 0.00039        0.00079          0.002            0.00039     0.00039\n nch         0.0017        0.00056       0.00056    C (w             w ) + 1] \u00d7 C (w                             )\n                                                      0.00056        0.00056          0.0011           0.00056     0.00056\n end         0.0012C \u2217 (w               w ) =                 n\u22121        n                             n\u22121\n                           0.00058       0.0012       0.00058        0.00058          0.00058          0.00058     0.00058\ngure 3.7     Add-one         n\u22121         n                       C (wn\u22121 ) + V\n                         smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP\n pus of 9332 sentences. Previously-zero probabilities are in gray.\n.8 shows the reconstructed counts, computed by Eq. 3.27.\n                                 i         want      to          eat          chinese          food     lunch       spend\n                  i              3.8       527       0.64        6.4          0.64             0.64     0.64        1.9\n        i                want             to                  eat               chinese                  food           lunch\n                  want           1.2       0.39      238         0.78         2.7              2.7      2.3         0.78\n             C*:  to             1.9       0.63      3.1         430          1.9              0.63     4.4         133\n                  eat            0.34     0.64                6.4               0.64                     0.64           0.64\n                                           0.34      1           0.34         5.8              1        15          0.34\n                  chinese        0.2       0.098     0.098       0.098        0.098            8.2      0.2         0.098\n                         0.39             238                 0.78              2.7                      2.7            2.3\n                  food           6.9       0.43      6.9         0.43         0.86             2.2      0.43        0.43\n                  lunch          0.57      0.19      0.19        0.19         0.19             0.38     0.19        0.19\n                         0.63             3.1                 430               1.9                      0.63           4.4\n        0.34      spend          0.32      0.16      0.32        0.16         0.16             0.16     0.16        0.16\n                         0.34             1                   0.34              5.8                      1              15\n                 Figure 3.8      Add-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus\n---\n16       C          3     \u2022      N-                  L                   M\n            HAPTER normalized Berkeley Restaurant Project sentences.                                        Note that the majority of the\n                                     GRAM               ANGUAGE                ODELS\nCompare with raw bigram counts\n                    values are zero. In fact, we have chosen the sample words to cohere with each other;\n              i     a matrix selected from a random set of eight words would be even more sparse.\n                                 want                to               eat                chinese           food                 lunch      spend\n i            0.0015             0.21       i        0.00025          0.0025             0.00025           0.00025              0.00025      0.00075\n want         0.0013             0.00042                   want          to         eat       chinese              food          lunch      spend\n                          i                     5    0.26             0.00084            0.0029            0.0029               0.0025       0.00084\n to           0.00078            0.00026                   827           0          9         0                    0             0          2\n                          want                  2          0.0013     0.18               0.00078           0.00026              0.0018     0.055\n eat          0.00046            0.00046                     0           608        1         6                    6             5          1\n                          to                    2          0.0014     0.00046            0.0078            0.0014               0.02         0.00046\n chinese      0.0012             0.00062                     0           4          686       2                    0             6          211\n                          eat                   0    0.00062          0.00062            0.00062           0.052                0.0012       0.00062\n food         0.0063             0.00039                     0           2          0         16                   2             42         0\noriginal                  chinese               1          0.0063     0.00039            0.00079           0.002                0.00039      0.00039\n lunch        0.0017             0.00056                     0           0          0         0                    82            1          0\n                          food              15 0.00056                0.00056            0.00056           0.0011               0.00056      0.00056\n spend        0.0012             0.00058                     0           15         0         1                    4             0          0\n                          lunch                 2          0.0012     0.00058            0.00058           0.00058              0.00058      0.00058\nFigure 3.7                                                   0           0          0         0                    1             0          0\n                                                        Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP\ncorpus of 9332            spend                 1            0           1          0         0                    0             0          0\n                   sentences. Previously-zero probabilities are in gray.\n                    Figure 3.1                                      Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-\n                    rant               i                want        to               eat           chinese              food      lunch      spend\n                                Project corpus of 9332 sentences. Zero counts are in gray. Each cell shows the count of\n                    the column label word following the row label word. Thus the cell in row i and column want\n                     i                 3.8              527         0.64             6.4           0.64                 0.64      0.64       1.9\n                    means that want followed i 827 times in the corpus.\n                     want              1.2              0.39        238              0.78          2.7                  2.7       2.3        0.78\nadd-1                to Figure         1.9              0.63        3.1              430           1.9                  0.63      4.4        133\n                     eat             3.2 shows the bigram probabilities after normalization (dividing each cell\n                    in Fig. 3.1        0.34             0.34        1                0.34          5.8                  1         15         0.34\nsmoothed             chinese           by the appropriate unigram for its row, taken from the following set of\n                    unigram            0.2              0.098       0.098            0.098         0.098                8.2       0.2        0.098\n                     food        counts):\n                                       6.9              0.43        6.9              0.43          0.86                 2.2       0.43         0.43\n                     lunch                  i              want    to           eat      chinese    food      lunch            spend\n                                       0.57             0.19        0.19             0.19          0.19                 0.38      0.19         0.19\n                     spend             0.32             0.16        0.32             0.16          0.16                 0.16      0.16         0.16\n                                            2533           927     2417         746      158        1093      341              278\n---\nAdd-1 estimation is a blunt instrument\n\nSo add-1 isn\u2019t used for N-grams:\n \u25e6  Generally we use interpolation or backoff instead\nBut add-1 is used to smooth other NLP models\n \u25e6  Or we can use add-k where 0<k<1\n \u25e6  It's used in naive bayes text classification\n \u25e6  In domains where the number of zeros isn\u2019t so huge.\n---\nBackoff and Interpolation\nSometimes it helps to use a simpler model\n \u25e6  Condition on less context for contexts you know less about\nBackoff:\n \u2022  If enough evidence, use trigram P(w\u2099|w\u2099\u22122w\u2099\u22121)\n \u2022  If not, use bigram P(w\u2099|w\u2099\u22121)\n \u2022  Else unigram P(w\u2099)\n\nInterpolation:\n \u25e6  mix unigram, bigram, trigram\n\nInterpolation works better\n---\n                                             \u02c6\nunigram counts.                              P(wn |wn\u22122 wn\u22121 ) = l1 P(wn |wn\nLinear Interpolation\nIn simple linear interpolation, we combine different order N-grams by linearly+l2 P(wn |\ninterpolating all the models. Thus, we estimate the trigram probability P(wn |wn\u22122 wn\u22121 )\nby mixing together the unigram, bigram, and trigram probabilities, each weighted+l3 P(wn )\nby a l :\nSimple interpolation\n                   such that the l s sum to 1:                  X\n                   \u02c6\n                   P(wn |wn\u22122 wn\u22121 ) =  l1 P(wn |wn\u22122 wn\u22121 )     li = 1\n                                        +l2 P(wn |wn\u22121 )\n                                        +l3 P(wn )              i (4.24)\n\nsuch that the l s sum to 1:In a slightly more sophisticated version of linear i\nLambdas conditional on context:\n                    X\n                   computed in a more sophisticated way, by condition\n                     li = 1                                       (4.25)\n                    i\n                   if we have particularly accurate counts for a particul\nIn a                P(Wn|Wn\u22122Wn\u22121) = \u03bb1 (wn\u22122)P(wn|wn\u22122Wn\u22121)\n         slightly more sophisticated version of linear interpolation, each l weight is\n                   counts of the trigrams based on this bigram will be\ncomputed in a more sophisticated way, by     +\u03bb2(wn\u22122)P(wn|Wn\u22121)\n                                           conditioning on the context. This way,\nif we have particularly accurate counts for a particular bigram, we assume that the\n                   make the l s for those trigrams higher and thus give\ncounts of the trigrams based on this bigram  + \u03bb3(Wn\u22121 )P(Wn)\n                                             will be more trustworthy, so we can\nmake the l s for those trigrams higher and thus give that trigram more weight in\n---\nHow to set \u03bbs for interpolation?\n\nUse a held-out corpus\n\n   Training Data     Held-Out  Test\n                     Data      Data\n\nChoose \u03bbs to maximize probability of held-out data:\n\u25e6  Fix the N-gram probabilities (on the training data)\n\u25e6  Then search for \u03bbs that give largest probability to held-\n   out set\n---\n Backoff\n\nSuppose you want:\n        P(pancakes| delicious souffl\u00e9)\n If the trigram probability is 0, use the bigram\n        P(pancakes| souffl\u00e9)\n If the bigram probability is 0, use the unigram\n        P(pancakes)\nComplication: need to discount the higher-order ngram so\nprobabilities don't sum higher than 1 (e.g., Katz backoff)\n---\nStupid Backoff\n\nBackoff without discounting (not a true probability)\n\n                 \"    count(w\u2071           )\n                 $              i\u2212k+1         if  count(w\u2071  ) > 0\nS (w | wi\u2212\u00b9  ) = $    count(wi\u2212\u00b9         )        i\u2212k+1\ni    i\u2212k+1       #              i\u2212k+1\n                 $    0.4 S (w    | wi\u2212\u00b9          )  otherwise\n                 $\n                 %                i           i\u2212k+2\n\nS (wi ) = count(w\u2071 )\nN\n                    80\n---\n Summary: What to do if you never saw an n-gram\n in training\nSmoothing: Pretend you saw every n-gram one (or k) times\nmore than you did\n \u2022  A blunt instrument (replacing a lot of zeros) but sometimes useful\nBackoff: If you haven't seen the trigram, use the (weighted)\nbigram probability instead\n \u2022  Weighting is messy; \"stupid\" backoff works fine at web-scale\nInterpolation: (weighted) mix of trigram, bigram, unigram\n \u2022  Usually the best! We also use interpolation to combine multiple LLMs\n---\nN-gram    Smoothing, Interpolation,\nLanguage  and Backoff\nModeling\n\n",
        "summary": "Purpose of the Document:\nThe document provides an introduction to N-gram language models, focusing on their use in predicting upcoming words and evaluating their performance. It covers the basics of N-gram models, including their structure, estimation methods, and evaluation metrics like perplexity.\n\nMain Ideas:\n\u2022 N-gram models predict upcoming words based on previous words.\n\u2022 Language models assign probabilities to words or sentences.\n\u2022 N-gram models use the Markov assumption to simplify probability calculations.\n\u2022 Smoothing, interpolation, and backoff techniques handle sparsity in N-gram models.\n\u2022 Perplexity is used to evaluate the performance of language models.\n\nKey Concepts:\n\u2022 N-gram models\n\u2022 Language models\n\u2022 Markov assumption\n\u2022 Smoothing\n\u2022 Interpolation\n\u2022 Backoff\n\u2022 Perplexity\n\u2022 Training and test sets\n\u2022 Sampling\n\nKey Takeaways:\n\u2022 N-gram models predict words based on previous words in a sequence.\n\u2022 The Markov assumption simplifies the calculation of word probabilities.\n\u2022 Smoothing techniques help handle sparsity in N-gram models.\n\u2022 Interpolation and backoff are methods to improve the accuracy of N-gram models.\n\u2022 Perplexity is a metric used to evaluate the performance of language models.\n\u2022 Training and test sets are essential for evaluating model performance.\n\u2022 Sampling methods can be used to generate sentences from N-gram models.",
        "quiz": [
            {
                "question_text": "What is the primary goal of a language model?",
                "answers": [
                    {
                        "text": "To predict upcoming words",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that a language model is a machine learning model that predicts upcoming words."
                    },
                    {
                        "text": "To generate new sentences",
                        "is_correct": false,
                        "explanation": "While language models can generate text, their primary goal is to predict words, not to generate new sentences directly."
                    },
                    {
                        "text": "To correct grammar errors",
                        "is_correct": false,
                        "explanation": "Although word prediction can assist in grammar checking, it is not the primary goal of a language model."
                    },
                    {
                        "text": "To recognize speech patterns",
                        "is_correct": false,
                        "explanation": "Speech recognition is an application of word prediction, but it is not the primary goal of a language model."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the two main paradigms of language models mentioned in the content?",
                "answers": [
                    {
                        "text": "N-gram language models and large language models",
                        "is_correct": true,
                        "explanation": "The content explicitly mentions these as the two main paradigms of language models."
                    },
                    {
                        "text": "Rule-based models and statistical models",
                        "is_correct": false,
                        "explanation": "These terms are not mentioned in the provided content context."
                    },
                    {
                        "text": "Recurrent neural networks and transformers",
                        "is_correct": false,
                        "explanation": "These are specific types of models, not the main paradigms mentioned in the content."
                    },
                    {
                        "text": "Generative models and discriminative models",
                        "is_correct": false,
                        "explanation": "These are broader categories of models, not specifically mentioned as the two main paradigms in the content."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is one application of word prediction in language tasks?",
                "answers": [
                    {
                        "text": "Grammar or spell checking",
                        "is_correct": true,
                        "explanation": "The content explicitly mentions that word prediction is helpful in grammar or spell checking tasks."
                    },
                    {
                        "text": "Generating random sentences",
                        "is_correct": false,
                        "explanation": "The content does not mention generating random sentences as an application of word prediction."
                    },
                    {
                        "text": "Translating languages",
                        "is_correct": false,
                        "explanation": "The content does not mention translating languages as an application of word prediction."
                    },
                    {
                        "text": "Creating new words",
                        "is_correct": false,
                        "explanation": "The content does not mention creating new words as an application of word prediction."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the basic principle behind how large language models (LLMs) generate text?",
                "answers": [
                    {
                        "text": "By predicting the next word over and over again",
                        "is_correct": true,
                        "explanation": "The content explicitly states that LLMs generate text by predicting the next word repeatedly."
                    },
                    {
                        "text": "By using predefined templates for sentences",
                        "is_correct": false,
                        "explanation": "The content does not mention the use of predefined templates for text generation."
                    },
                    {
                        "text": "By randomly selecting words from a dictionary",
                        "is_correct": false,
                        "explanation": "The content does not suggest that LLMs use random word selection from a dictionary."
                    },
                    {
                        "text": "By analyzing the grammatical structure of sentences",
                        "is_correct": false,
                        "explanation": "While grammar is mentioned as a part of language tasks, it is not described as the principle behind text generation in LLMs."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What does the Chain Rule of Probability state?",
                "answers": [
                    {
                        "text": "The Chain Rule of Probability states that the joint probability of a sequence of words can be decomposed into the product of conditional probabilities of each word given all previous words.",
                        "is_correct": true,
                        "explanation": "This is the direct application of the Chain Rule of Probability as described in the content context."
                    },
                    {
                        "text": "The Chain Rule of Probability states that the probability of a word is independent of all previous words.",
                        "is_correct": false,
                        "explanation": "This contradicts the Chain Rule, which explicitly involves conditional probabilities given previous words."
                    },
                    {
                        "text": "The Chain Rule of Probability states that the probability of a sentence is the sum of the probabilities of each individual word.",
                        "is_correct": false,
                        "explanation": "The Chain Rule involves multiplication of conditional probabilities, not summation."
                    },
                    {
                        "text": "The Chain Rule of Probability states that the probability of a word is equal to the probability of the previous word.",
                        "is_correct": false,
                        "explanation": "The Chain Rule does not equate the probability of a word to the probability of the previous word but rather involves conditional probabilities."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the formula for the joint probability of a sequence of words according to the Chain Rule?",
                "answers": [
                    {
                        "text": "P(w\u2081)P(w\u2082|w\u2081)P(w\u2083|w\u2081,w\u2082)...P(w\u2099|w\u2081,...,w\u2099\u208b\u2081)",
                        "is_correct": true,
                        "explanation": "This is the correct formula for the joint probability of a sequence of words according to the Chain Rule as described in the content."
                    },
                    {
                        "text": "P(w\u2081) + P(w\u2082|w\u2081) + P(w\u2083|w\u2081,w\u2082) + ... + P(w\u2099|w\u2081,...,w\u2099\u208b\u2081)",
                        "is_correct": false,
                        "explanation": "This option incorrectly uses addition instead of multiplication, which is not the correct application of the Chain Rule."
                    },
                    {
                        "text": "P(w\u2081) \u00d7 P(w\u2082) \u00d7 P(w\u2083) \u00d7 ... \u00d7 P(w\u2099)",
                        "is_correct": false,
                        "explanation": "This option ignores the conditional probabilities and only considers the marginal probabilities of each word."
                    },
                    {
                        "text": "P(w\u2081|w\u2082) \u00d7 P(w\u2082|w\u2083) \u00d7 P(w\u2083|w\u2084) \u00d7 ... \u00d7 P(w\u2099\u208b\u2081|w\u2099)",
                        "is_correct": false,
                        "explanation": "This option reverses the order of the conditional probabilities, which is not consistent with the Chain Rule."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the main challenge in estimating probabilities of entire sentences directly from relative frequency counts?",
                "answers": [
                    {
                        "text": "There are too many possible sentences to count their frequencies accurately.",
                        "is_correct": true,
                        "explanation": "The content explicitly states that language is creative and new sentences are constantly invented, making it impossible to gather enough data to estimate probabilities of entire sentences directly from relative frequency counts."
                    },
                    {
                        "text": "The computational power required to process entire sentences is insufficient.",
                        "is_correct": false,
                        "explanation": "The content does not mention computational power as a limitation; it focuses on the vast number of possible sentences and the inability to gather sufficient data."
                    },
                    {
                        "text": "The probability distributions for entire sentences are too complex to model.",
                        "is_correct": false,
                        "explanation": "While complexity might be a factor, the content specifically highlights the issue of data insufficiency rather than the complexity of modeling probability distributions."
                    },
                    {
                        "text": "Existing language models are not designed to handle entire sentences.",
                        "is_correct": false,
                        "explanation": "The content does not state that language models are incapable of handling entire sentences; it discusses the challenge of estimating probabilities due to the vast number of possible sentences."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the primary purpose of an N-gram language model?",
                "answers": [
                    {
                        "text": "To predict the probability of a sequence of words or the next word in a sequence",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that a language model, including an N-gram model, predicts upcoming words and assigns probabilities to sequences or next words."
                    },
                    {
                        "text": "To generate random words from a predefined vocabulary",
                        "is_correct": false,
                        "explanation": "The concept description does not mention generating random words; it focuses on predicting words based on probabilities."
                    },
                    {
                        "text": "To correct grammar and spelling errors in text",
                        "is_correct": false,
                        "explanation": "While word prediction can assist in grammar and spell checking, the primary purpose of an N-gram model is to predict words, not directly correct errors."
                    },
                    {
                        "text": "To create new sentences from scratch without any input",
                        "is_correct": false,
                        "explanation": "The concept description explains that N-gram models predict words based on previous words, not create sentences from nothing."
                    }
                ],
                "topic": "Ngram-Lms",
                "subtopic": "Main Content",
                "concepts": [
                    "Ngram-Lms"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    },
    "data/raw/6CCSAHAI-HumanAI Interaction-Lect5.pdf": {
        "metadata": {
            "file_name": "6CCSAHAI-HumanAI Interaction-Lect5.pdf",
            "file_type": "pdf",
            "content_length": 14129,
            "language": "en",
            "extraction_timestamp": "2025-12-09T14:02:26.540439+00:00",
            "timezone": "utc"
        },
        "content": "6CCSAHAI\n\n   Human-centred AI as a\n   Design Process\n   Part 2: Prototyping human-centred AI\n   systems\n   Dr Georgia Panagiotidou\n\n   1\n---\nThis week we will:\n\n\u2022 understand the basic goal of prototyping\n\u2022 understand key conceptual ideas in prototyping\n\u2022 explore examples of AI specific prototyping techniques\n---\n    ENGAGEMENT\n    Connecting the dots and building relationships\n    between different citizens, stakeholders and partners\n\n                 DESIGN\n               PRINCIPLES\n         1. Be People Centered\n2. Communicate (Visually & Inclusively)\n       3. Collaborate & Co-Create\n      4 Iterate, Iterate, Iterate\n\n    CHALLENGE    OUTCOME\n\n    I Discover  Derine           Develop  Deiver\n                          METHODS\n         BANK\n\nExplore, Shape, Build\n\nCreating the conditions that allow innovation,\n including culture change, skills and mindset\n    LEADERSHIP\n              3\n---\n    Today\u2019s focus    ENGAGEMENT\n    Connecting the dots and building relationships\nbetween different citizens, stakeholders and partners\n\n                        DESIGN\n                      PRINCIPLES\n                1. Be People Centered\n       2. Communicate (Visually & Inclusively)\n              3. Collaborate & Co-Create\n             4 Iterate, Iterate, Iterate\n\n    - Prototype    CHALLENGE  I Discover  Derine    Develop  Deiver    OUTCOME\n\n       METHODS\n         BANK\n\nExplore, Shape, Build\n\nCreating the conditions that allow innovation,\n including culture change, skills and mindset\n    LEADERSHIP\n\n    Interviews                      Share ideas\n    Shadowing                       All ideas worthy\n    Seek to understand              \u00b7Diverge/Converge\n    Non-judgmental                  \"Yes and\" thinking\n                                    Prioritize        \u00b7Mockups\n                                                       Storyboards\n    EMPATHIZE             IDEATE                      Keep it simple\n                                                      \u2022Fail fast\n                                                      Iterate quickly    Phase 1  Phase 2    Phase 3  Phase 4    Phase 5  Phase 6\n\n                   DEFINE                     PROTOYPE     UNDERSTAND  DEFINE    SKETCH  DECIDE    PROTOTYPE  VALIDATE\n    Personas\n    Role objectives\n    \u2022Decisions                                             TesT\n    Challenges\n    \u2022 Pain Points                    Understand impediments\n                                     What works?\n    https://dschool.stanford.edu     Role play\n                                     Iterate quickly\n                                                                                                              4\n---\nWhat are prototypes?\n\n\u201cA prototype is one manifestation of a design that allows\nstakeholders to interact with it and to explore its suitability;\nit is limited in that a prototype will usually emphasize one set\nof product characteristics and de-emphasize others\u201d\n\nSharp, Preece, Rogers. \u201cInteraction Design.\u201d\n---\nStudent Infornation\nStulet Numbr:789-567-234                            Hep\nFirstNmei\n                                   Scot                                                         421 PM\nWhat are prototypes?    Middle:\u1d42\u1d50                                               4.21 PM        Settings\n                       SMameAmbler                           Search  4.21PM     Sarah\n                       Saltatim:r.                                                             ogmail.com\n                        Date first Ea Jume 1 203             Say hello         Sarah Green     Male      Female\n                        Seminars:  Term    12k Shitus        to your new       3.293       855 ations\n                        Semma      Fall 2003\u1d2c     Pessed     workspace         lana Lima\n                        Csc1001                   Paysed\n                        Csc 200 re to AMF2003     Enroled                      gan Fox\n                        CSC 203 Ahd AMSmm 204                FIND OUT MORE     Cruise          Facebook\n                                                                                Tug\n                                                                               ma Stone\n                                                             o R     *\n\n                        Ad.. Dof. Tanseriph Close\n\n.... can be anything from:\n\n\u2022     a paper-based storyboard through to a complex piece of software;\n\n\u2022     from a cardboard mock-up to a moulded or pressed piece of metal.\n\nThe purpose is to help users envision the important parts of interactions\n\nwith the newly designed systems\n---\nWhy prototype?\n\nPrototypes can:\n\u2022     help you think through and understand\n      the possibilities in your design space\n\n\u2022     help you test assumptions about users,\n      the experience, or viable technologies.\n---\nWhy prototype human-centred AI?\n\n\u2022  To align AI system behaviour with user expectations\n\u2022  To catch usability issues early\n\u2022  To explore ethical, trust, and explainability concerns before\n   implementation\n\nTraditional AI     Human-Centred AI\nModel-centric      User-centric\nData first         People first\nEvaluate by        Evaluate by usability, trust\naccuracy\n---\nTypes of prototypes  Type    Advantages                                               Disadvantages\n\nLow & High Fidelity                                                                    No error checking\n\n                                        Lower \"development\" cost                       Little/no detail\n\n                                        Easier to evaluate multiple concepts           Difficult / poorly suited for\n\n                       Low-fidelity     Capture contexts of use, market needs          usability assessment\n                                        Less constrained (conceptual models)           Navigation limitations\n\n                                        Concept communication tool                     Content may not fit layout\n\n                                        Explore high-level design & layout options     Can't test dynamic layout\n\n                  Partially functional\n\n                  Interactive                                          Time & resource intensive to\n                                                                       develop and change\n                  Contains content: effective preview of layout        Not effective for requirements\nHigh-fidelity     Usability test w/ target users, identify UX bugs     gathering or early-stage\n\n                  Look & feel resembles final product                  ideation\n\n                  \"Living specification\"                               May confuse stakeholders\n                                                                       prototype vs final product\n                  Use as marketing and sales tool\n\n                                            adapted from 11.2, Beyond Interaction Design\n---\n    Sketches, Wireframes &    Usability\n    Prototypes                Refinement\n                              Concept\n\n    2 Ideation  Validation                                              20\n      Iteration\n\n                                                                        M\n\n2 Sketch    Wireframe\n  Ideas,    Structure\n  possibilities to    overall flow\n  pursue\n\nPrototype            Design\nStructure, flow,     Final branded and\n and details         fully developed\n (without final      product\n visual design)\n\n    https://www.uxmatters.com/mt/archives/2010/05/sketches-and-wireframes-and-prototypes-oh-my-\n    creating-your-own-magical-wizard-experience.php\n---\nLow-fidelity Prototypes\n---\nWireframes  miro  Share 8\n\nLanding Page  Our Product  About  News\n\u2022  Wireframing is a process where  PRDUCTS ABOUT CONTACT  PRODUCTS ABOUT CONTACT  PROWUCTS ABOUT CONTACT  PROUCTS ABOUT CONTACT\ndesigners draw overviews of  SHORT STATEMENT\ninteractive products to establish  2  ABOUT THE TEAM\nthe structure and flow of possible  OUR TEAM  \u202200\nNAME  NAME\nSRAME  S SRAME\ndesign solutions.  PRODUCT #1  HIGHLIGHT#1  Pexition  Axition\nNAME  pAME\nCTA  SUURNAME  S AME\nCTA  HIGHLGHT#2  Paxition  Pauiition\n\u2022  Wireframes are primarily tools for  CTA  CTA  CTA\ncollaboration toward making better  PRODUCT DESCRIPTION  HIGHLIGHT #3  SHORT STATEMENT\nCTA  ABOUT THE COMPANY\nprototypes and products faster.  ABOUT VS  CTA  HISTORY  CTA  CTA\n\nDETAILS\nADDRE35\nADDRESS  D A D\nD E5\nADDRE3S  ADDRE3S\n5  RES\u2085\n\n  >>    144%\n\n  https://www.interaction-design.org/literature/topics/wireframing\n---\nDashboardMainHome                                                    Q  Search\n\nWiki                 IHistory I Preferences | Administration | Log Out\n                     Welcome Peldi\n\nPage Operations Main\nBrowse Space     Welcome to our Wiki!\n                 Added by Peldi, last edited by Peldi on Oct 17, 2013\nAdd Content      Welcome to our awesome wiki, a place for us to share knowledge yadda yadda yadda\n                 You can share images like the one below:\n\nHere's some more text lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do\neiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,\nquis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat\n10 ChildrenShow Chlldren I View in Hierarchy I Add Child Page\n\n                                                             https://balsamiq.com\n---\nPaper Prototyping\n\nWe provide research-based UX\n\u2022  Paper prototyping is the process of  guidance, by studying users\naround the world.\nsketching out potential concepts,\nflows or ideas on paper and testing  Recent Articles from NN\nthose sketches with users  Recents  ARTICLE\n9 Ways to Encourage\nEmployee Sharing and\nEngagement on an Intranet\nRecents  Date\nARTICLE\nRemote Usability Testing:\nStudy Guide\nDate\n\nARTICLE\n9 Ways to Encourage\nEmployee Sharing and\nEngagement on an Intranet\nDate\n\nNeos\nViACOs  Date\nw to Sell UX:\nranslating UX\nto Business\nValue\n\nhttps://www.nngroup.com/articles/paper-prototyping-cutout-kit/\n---\nPaper Prototyping  \u2461Recoiue conpirnmhou\nwalt cnt goe wodg is\nconfimedt yla email.\n\u2467Py Por your stag.\nwe accopt bouk trauper and\ncanit corcts you cou cncel\n\u2022  Paper prototyping is the process of  you boting up to 30\ndays prior to your arnival.\no more informaton, visil\nsketching out potential concepts,  mayrment ey  8 Tr 3\nflows or ideas on paper and testing  Ther o t\nyou  i\nS howi uho?  yo  o\nthose sketches with users  contacr the rowse cume  SaL\nAeouT youe siAY  k\nHeme  chec-in  weur o e wt\nHouse  nonday,  Choc-ot  page\n1atn Sopbeuta urnday  tay??\nLoca hion  25thpe  Heo at te is\n720  tnat  CACQ\nBie  eh\nCourbes  6nisnts  te\nopf esn  CoumsE\nNakes o s ed ow)  T  T\nTOPIC  Explee\nCr2s  o\nNa o d\nPy-men\nA  5\n219 Fk  ABut You\nNaue\n\n  Pno\n  https://martha-eierdanz.com/paper-prototyping-in-practice\n---\nClick-through\nPrototypes     Google Pixel    Google Pioxel 2    Google Pixel 3    Google Pixuel 4\ne.g., Figma    Main            Main               Main\n               First           Second             Third\n               Page            Page               Page              Main Page\n\nNext    Prev  Next  Prev  Next\n---\n                                B80\nT   +             H             T +                                                                Share    45%\n B                   V  V  Q           m                              Drafts / Getting Start...             >\n\n QLayers   Assets     Page 1                                                Design    Prototype Inspect\n\n # This                                                                     Interactions                     +\n #This or That                  This        Interaction details       \u00d7 Tap                         This\n           This or That                 This or That                  This\n    :: This Button\nFlow1                                                   On tap              Scroll behavior\n    That Button\n    That Image                                          \u2192Navigate to  This  Position               Scroll with parent\n    This Image                                          Animation            Show prototype settings\n\n  Smart animate\n  This  That    This    Ease out  \u2467300ms    A\n                        This F That    This  That    This    That\n---\nHigh-fidelity Prototypes\n---\nTypes of high-fidelity    Vertical prototyping    Horizontal prototyping\nprototypes                                        O\n\n                                                  0 0 0 0\n                                                  O\n\n A horizontal prototype is a prototype, where all the visual parts of    Fully implemented\n the user interface of a new computer system is implemented, i.e.\n screen dialogues and their interconnections can be demonstrated,\n but no data can be processed. In contrast a vertical prototype is a     Partially implemented\n prototype where a few selected functions are implemented in such\n detail that realistic data can be processed, i.e. a realistic work task\n can be performed with a vertical prototype. (Gr\u00f8nbak, 1989)             Not implemented\n---\nClick-through\n\nPrototypes\n\n               AAO\ne.g., Figma       Mee edoa mee    310000\n                                sob o bos.\n                                 ee o rr\n\na0O    teocnre    09\n\nA\n\nCe co osonM\n\nDesbmr    seubises    9s  6s\nRa         Raererr          eeeao    Lereao\n\nM aa\n---\nPhysical\nComputing\n         TIME:0:12:21\n         STOPWATCH\n---\nWizard of Oz Prototyping\n\ncn\n---\nWizard of Oz testing \u2014 The listening type writer IBM 1984\n\nDear\nDear Henry    Henry\n\n                      Speech\n                      Computer\n\nWhat the user sees            The wizard\n---\nAI Use Case            Prototype Strategy\n\nRecommender system     Mock interface with fake suggestions\n\nChatbot                Wizard-of-Oz with scripted responses\n\nClassifier UI          Confidence bars + explanation cards\n---\nRemember!\n\u2022  what assumptions are your prototypes testing?\n\u2022  what does this mean for how/why you prototype?\n---\n6CCSAHAI\n\n   Human-centred AI as a\n   Design Process\n   Part 2: Prototyping human-centred AI\n   systems\n   Dr Georgia Panagiotidou\n\n   26\n\n",
        "summary": "Purpose of the Document:\nThe document discusses the process of prototyping human-centered AI systems, focusing on key concepts, techniques, and design principles to create effective AI prototypes.\n\nMain Ideas:\n\u2022 Understanding the goals and key concepts of prototyping\n\u2022 Exploring AI-specific prototyping techniques\n\u2022 Emphasizing engagement, design principles, and innovation\n\u2022 Differentiating between traditional AI and human-centered AI approaches\n\nKey Concepts:\n\u2022 Prototyping\n\u2022 Human-centered AI\n\u2022 Design principles\n\u2022 Low-fidelity prototypes\n\u2022 High-fidelity prototypes\n\u2022 Wireframing\n\u2022 Paper prototyping\n\u2022 Click-through prototypes\n\u2022 Wizard of Oz prototyping\n\nKey Takeaways:\n\u2022 Prototypes help explore design possibilities and test assumptions about users and technologies.\n\u2022 Human-centered AI focuses on aligning system behavior with user expectations and addressing usability, trust, and explainability concerns.\n\u2022 Prototypes range from low-fidelity sketches to high-fidelity interactive models, each with specific advantages and disadvantages.\n\u2022 Design principles for human-centered AI include being people-centered, communicating visually and inclusively, collaborating and co-creating, and iterating continuously.\n\u2022 Prototyping techniques like wireframing, paper prototyping, and Wizard of Oz testing are essential for developing and refining AI systems.",
        "quiz": [
            {
                "question_text": "What is the basic goal of prototyping in the context of human-centred AI?",
                "answers": [
                    {
                        "text": "To align AI system behaviour with user expectations",
                        "is_correct": true,
                        "explanation": "The content explicitly states that one of the goals of prototyping human-centred AI is to align AI system behaviour with user expectations."
                    },
                    {
                        "text": "To finalize the design of the AI system",
                        "is_correct": false,
                        "explanation": "The content does not mention finalizing the design as a goal of prototyping. Prototyping is about exploration and alignment, not finalization."
                    },
                    {
                        "text": "To create a fully functional AI system",
                        "is_correct": false,
                        "explanation": "Prototyping is not about creating a fully functional system but about exploring and testing design possibilities."
                    },
                    {
                        "text": "To replace user feedback with automated testing",
                        "is_correct": false,
                        "explanation": "The content emphasizes the importance of user interaction and feedback, not replacing it with automated testing."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect5",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect5"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the four design principles mentioned in the lecture on human-centred AI?",
                "answers": [
                    {
                        "text": "Be People Centered, Communicate (Visually & Inclusively), Collaborate & Co-Create, Iterate, Iterate, Iterate",
                        "is_correct": true,
                        "explanation": "These are the four design principles explicitly mentioned in the lecture on human-centred AI."
                    },
                    {
                        "text": "Empathize, Ideate, Define, Prototype, Test",
                        "is_correct": false,
                        "explanation": "These are stages of the design process, not the design principles mentioned in the lecture."
                    },
                    {
                        "text": "Explore, Shape, Build",
                        "is_correct": false,
                        "explanation": "These are methods in the prototype bank, not the design principles mentioned in the lecture."
                    },
                    {
                        "text": "Discover, Define, Develop, Deliver",
                        "is_correct": false,
                        "explanation": "These are outcomes of the design process, not the design principles mentioned in the lecture."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect5",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect5"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the purpose of prototyping in the context of human-centred AI?",
                "answers": [
                    {
                        "text": "To help users envision the important parts of interactions with the newly designed systems",
                        "is_correct": true,
                        "explanation": "This is directly stated in the content context as the purpose of prototyping in human-centred AI."
                    },
                    {
                        "text": "To finalize the design of the AI system",
                        "is_correct": false,
                        "explanation": "The content context does not mention finalizing the design as a purpose of prototyping."
                    },
                    {
                        "text": "To create a fully functional AI system",
                        "is_correct": false,
                        "explanation": "Prototyping is about exploring and testing, not creating a fully functional system."
                    },
                    {
                        "text": "To replace user feedback with automated testing",
                        "is_correct": false,
                        "explanation": "Prototyping emphasizes user interaction and feedback, not replacing it with automated testing."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect5",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect5"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the definition of a prototype according to Sharp, Preece, and Rogers?",
                "answers": [
                    {
                        "text": "A prototype is one manifestation of a design that allows stakeholders to interact with it and to explore its suitability.",
                        "is_correct": true,
                        "explanation": "This is the exact definition provided by Sharp, Preece, and Rogers in the context."
                    },
                    {
                        "text": "A prototype is a final product that is ready for market release.",
                        "is_correct": false,
                        "explanation": "This contradicts the definition provided, as prototypes are not final products."
                    },
                    {
                        "text": "A prototype is a detailed plan of the system's architecture.",
                        "is_correct": false,
                        "explanation": "The definition focuses on interaction and suitability, not architectural plans."
                    },
                    {
                        "text": "A prototype is a theoretical concept that is never physically created.",
                        "is_correct": false,
                        "explanation": "The definition emphasizes the physical manifestation of a design for interaction."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect5",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect5"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are some examples of prototyping techniques mentioned in the lecture?",
                "answers": [
                    {
                        "text": "Mockups, Storyboards, Role play",
                        "is_correct": true,
                        "explanation": "These are examples of prototyping techniques mentioned in the lecture content."
                    },
                    {
                        "text": "Surveys, Questionnaires, Interviews",
                        "is_correct": false,
                        "explanation": "While these are research methods, they are not specifically mentioned as prototyping techniques in the lecture content."
                    },
                    {
                        "text": "Focus Groups, User Testing, A/B Testing",
                        "is_correct": false,
                        "explanation": "These are user research and testing methods, not prototyping techniques mentioned in the lecture."
                    },
                    {
                        "text": "Wireframing, Flowcharts, User Personas",
                        "is_correct": false,
                        "explanation": "These are design and research tools, but they are not specifically highlighted as prototyping techniques in the lecture content."
                    }
                ],
                "topic": "6Ccsahai-Humanai Interaction-Lect5",
                "subtopic": "Main Content",
                "concepts": [
                    "6Ccsahai-Humanai Interaction-Lect5"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ]
    }
}