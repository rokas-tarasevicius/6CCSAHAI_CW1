{
    "data/raw/Week9 - LGT.pdf": {
        "metadata": {
            "file_name": "Week9 - LGT.pdf",
            "file_type": "pdf",
            "content_length": 54823,
            "language": "en",
            "extraction_timestamp": "2025-11-26T00:26:07.659218+00:00",
            "timezone": "utc"
        },
        "content": "                    KING'S\n   Applications of  .\n                    College\n   LLM \u2013 Part I:    LONDON\n   Retrieval\n   Augmented\n   Generation\n   (RAG)\n\n   Week 9 - LGT     BVSH HOVSE\n\nG  Dr Lin Gui\n   Lin.1.gui@kcl.ac.uk\n---\nLearning outcomes\n\n\u26ab By the end of this topic, you will be able to:\n\n  \u26ab  Understand the core concepts of Retrieval-Augmented Generation and how it\n     differs from standard LLM approaches.\n\n  \u26ab  Build and configure a basic RAG pipeline using embeddings, retrievers, and\n     generators.\n\n  \u26ab  Evaluate and optimize RAG performance through effective data preparation,\n     chunking, and retrieval strategies.\n\n2\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n3\n---\n                                                        RAG overview\n\n                                                                               \u26ab                                  When answering questions\n                                                                                                                  or generating text, it first\n                                                                                                                  retrieves relevant\n                                                                                                                  information from a large\n                                                                                                                  number of documents, and\n                                                                                                                  then LLMs generates\n                                                                                                                  answers based on this\n                                                                                                                  information.\n\n                                                                               \u26ab                                  By attaching an external\n                                                                                                                  knowledge base, there is\n                                                                                                                  no need to retrain the\n                                                                                                                  entire large model for each\n                                                                                                                  specific task.\n\n                                                                               \u26ab                                  The RAG model is\n                                                                                                                  especially suitable for\n\n                                           Input        Query                       Indexing                      knowledge-intensive tasks.\nUser                                        How do you evaluate the fact       Documents\n                                            that OpenAI's CEO, Sam Altman,              Chunks Vectors\n\n        Output                              by the board in just three days,\n                                            and then was rehired by the                 embeddings\n                                            company, resembling a real-life\n                                            version of \"Game of Thrones\" in             Retrieval\n\nwithout RAG                                                                    Relevant Documents\n\nfuture events. Currently, I do not have     LLM         Generation\n\nand rehiring of OpenAI's CEO ..            Question:                           Chunk 1: \"Sam Altman Returns to                                4\nwith RAG\n\nthe company's future direction and        based on the following information:     Chunk 2: \"The Drama Concludes? Sam\n                                                                                  Altman to Return as CEO of OpenAl,\nand turns reflect power struggles and     Chunk 2:                                Board to Undergo Restructuring\"\n                                          Chunk 3 :\nOpenAl...                                 Combine Context                         OpenAl Comes to an End: Who Won\n             Answer                       and Prompts                             and Who Lost?\"\n---\n   Symbolic Knowledge or Parametric Knowledge\n\n    \u26ab Ways to optimize LLMs.\n\n    \u26ab Prompt Engineering    This week\n\n    \u26ab Instruct / Fine-tuning\n\n    \u26ab  Retrieval-Augmented\n       Generation\n\n    Week 7    Week 8\n\nExternal Knowledge\n     Required\n    High     Modular RAG\n\n             multiple modules    Retriever Fine-tuning\nAdvanced RAG                     Collaborative Fine-tuning\n\noptimization                     All of the above\n Naive RAG                       RAG             Generator Fine-tuning\n\n   XoT Prompt     Prompt Engineering  Fine-tuning          5\n e.g. CoT, ToT\nFew-shot Prompt\n    Low           Standard Prompt                      Model Adaptation\n           Low                                       High  Required\n---\nRAG vs Fine-tuning\n\n Data Processing\n                ddling.    datasets, and limited datasets may not result\n\n 6\n\n higher latency.    retrieval, resulting in lower latency.\n---\nRAG Application\n\n\u26ab Scenarios where RAG is applicable:\n\n  \u26ab  Long-tail distribution of data\n\n  \u26ab  Frequent knowledge updates\n\n  \u26ab  Answers requiring verification and traceability\n\n  \u26ab  Specialized domain knowledge\n\n  \u26ab  Data privacy preservation\n\nQuestion Answering     Fact checking    Dialog systems    Summarisation\n\nMachine translation    Code generation    Sentiment Analysis    Commonsense\n                                                                reasoning\n\n                                                                           7\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  Foundation of information Retrieval\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 8\n---\n   Foundation of information Retrieval\n\n   \u26ab What is information Retrieval?\n\n     \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n        returns those items to the user, typically in list form sorted per computed\n        relevance#\n\n   \u26ab Three main questions in information retrieval:\n\n     \u26ab  How to map the text into features (Embedding method)\n\n     \u26ab  How to measure the similarity between features (IR Modelling)\n\n     \u26ab  How to do it efficiently (Indexing)\n\n[#] Qiaozhu Mei and Dragomir Radev, \u201cInformation Retrieval,\u201d The Oxford Handbook of Computational Linguistics,\n2\u207f\u1d48 edition, Oxford University Press, 2016.    9\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n10\n---\nDiscrete representation\n\n\u26ab  In discrete representation, for both query and document, we assign each word a\n   specific dimension. If a word appears query/document, then value of the\n   corresponding dimension is:\n\n    \u26ab  In Binary representation: 1\n\n    \u26ab  In TF (term frequency) based representation: t (how many times this word\n       appears within the query/documents)\n\n    \u26ab  In TF-IDF (inverse document frequency) based representation: tlog(n/x)\n\n       \u26ab  Here, t is term frequency, n is number of documents, x is the number of\n          documents which contains this term.\n\n11\n---\nDiscrete representation (example)\n\n\u26ab We have the following documents:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n\u26ab After pre-processing:\n\n  \u26ab  D1 = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d.\n\n  \u26ab  D2 = \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n  \u26ab  D3 = \u201cshipment\u201d, \u201cgold\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n                                                  12\n---\nDiscrete representation (example)\n\n \u26ab Building vocabulary:\n\n   \u26ab V = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d, \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n \u26ab  Detect the feature for each document. If the feature occurs, the corresponding\n    value is \u20181\u2019, otherwise \u20180\u2019 (binary feature):\n\n           shipment  gold  damage  fire          delivery  silver  arrive  truck\n     D1     1        1     1       1             0         0       0       0\n     D2     0        0     0       0             1         1       1       1\n     D3     1        1     0       0             0         0       1       1\n\n 13\n---\nDiscrete representation (example)\n\n\u26ab  Definition \u2013 term frequency (TF):\n\n    \u26ab  \ud835\udc61 - how many times the term appears in the document\n\n\u26ab  Example:\n\n    \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n    \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n    \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n              shipment  gold  damage  fire  delivery  silver    arrive  truck\n        D1     1        1     1       1     0              0     0      0\n        D2     0        0     0       0     1              2     1      1\n        D3     1        1     0       0     0              0     1      1\n\n                                                                             14\n---\nDiscrete representation (example)\n\n\u26ab Definition \u2013 inverse document frequency (IDF):\n\n  \u26ab  \ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc5b/\ud835\udc65) \u2013 n is number of documents, x is the number of documents which\n     contains this term\n\n\u26ab Example:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n          shipment     gold  damage  fire  delivery  silver     arrive  truck\n          0.176     0.176    0.477   0.477  0.477    0.477      0.176   0.176\n                       Inverse document frequency vector\n\n                                                                             15\n---\nDiscrete representation (example)\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1      1         1        1      1         0        0           0         0\n D2      0         0        0      0         1        2           1         1\n D3      1         1        0      0         0        0           1         1\n                           Term frequency matrix\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n        0.176     0.176    0.477   0.477    0.477     0.477      0.176     0.176\n                           Inverse document frequency vector\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1     0.176     0.176    0.477   0.477     0        0           0         0\n D2      0        0         0      0        0.477     0.954      0.176     0.176\n D3     0.176     0.176     0      0         0        0          0.176     0.176\n\n                            TF-IDF Matrix\n                                                                                16\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n17\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n18\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n19\n---\n   Dense Passage Retrieval\n\n   \u26ab  Encode questions and text passages into continuous vectors (embeddings) and\n      retrieve passages using vector similarity instead of keyword overlapping.\n\n   \u26ab  Train directly on question\u2013passage pairs, using in-batch negatives to improve\n      efficiency.\n\n                 Question q    Passage p                                In each batch, there are multiple\n\n                 BERTQ         BERTp                Passage encoder     question\u2013answer pairs, both\n      Question encoder                                                  matched and unmatched. Matched\n                                                                        pairs should have similar\n\n                          OOOOOOOO  0OOOOOOO                            representations, while unmatched\n                                                                        pairs should have representations\n                          $h_q$                                         that are far apart.\n                                                Training phase\n\n      Similarity score: dot product  (q, =  Fine-tune two encoders\nhttps://aclanthology.org/2020.emnlp-main.550.pdf                                                 20\n---\n   ReContriever\n\n   \u26ab  What if we don\u2019t have annotated data (Matched and unmatched QA-pair).\n\n   \u26ab  Using pseudo-examples: For each passage/document p, create an augmented\n      version p\u2032. Then treat (p, p\u2032) as a positive pair:\n\n       \u26ab  Masking words (random word masking)\n\n       \u26ab  Span deletion\n\n       \u26ab  Back-translation Sentence\n\n       \u26ab  Reordering Adding noise\n\n       \u26ab  Perturbations Cropping (taking a subset of sentences)\n\nhttps://aclanthology.org/2023.findings-acl.695.pdf    21\n---\n  Using API\n\n   \u26ab  There are many APIs could do this job, for example, Mistral AI:\n\n                       YMISTRAL EMBED API\n\n                       8OPEN IN COLAB\n\n   \u26ab  Example: link    How to Generate Embeddings\n                       To generate text embeddings using Mistral Al's embeddings APl, we can make a request to the APl endpoint and specify the\n                       embedding model mistra1-embed , along with providing a list of input texts. The APl will then return the corresponding\n                       embeddings as numerical vectors, which can be used for further analysis or processing in NLP applications.\n\n   \u26ab Some other options:    PYTHON TYPESCRIPT      CURL                                                           OUTPUT\n                            import os\n                            from mistralai import Mistral\n          Sentence Bert     api_key = os.environ[\"MISTRAL_API_KEY\"]\n     \u26ab                      model = \"mistral-embed\"\n                            client = Mistral(api_key=api_key)\n\n     \u26ab    SimCSE            embeddings_batch_response = client.embeddings.create(\n                            model=model,\n                            inputs=[\"Embed this sentence.\", \"As well as this one.\"],\n\n     \u26ab    \u2026\u2026                )\n                            The output is an embedding object with the embeddings and the token usage information.\n\n                            Let's take a look at the length of the first embedding:\n\n                            PYTHON TYPESCRIPT CURL\n                            len(embeddings batch response.data[0].embedding)\n\nhttps://docs.mistral.ai/capabilities/embeddings    22\n---\nIR Modelling\n\n\u26ab What is information Retrieval?\n\n  \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n     returns those items to the user, typically in list form sorted per computed\n     relevance\n\n\u26ab Three main questions in information retrieval:\n\n  \u26ab  How to map the text into features (Embedding method)\n\n  \u26ab  How to measure the similarity between features (IR Modelling)\n\n  \u26ab  How to do it efficiently (Indexing)\n\n23\n---\nIR Modelling\n\n\u26ab  In IR modelling, we use different metric to measure the similarity/distance\n   between the query and given documents. The target is to find the top-k relevant\n   documents based on the given query.\n\n    \u26ab  Cosine similarity (for both Discrete & Continuous representation)\n\n    \u26ab  Jaccard distance (for Discrete representation only)\n\n    \u26ab  BM25 (for Discrete representation only)\n\n24\n---\nCosine similarity\n\n\u26ab Cosine similarity\n                           \u03c3\ud835\udc5b \ud835\udc65\ud835\udc56 \ud835\udc66\ud835\udc56\n                   \ud835\udc36\ud835\udc5c\ud835\udc60 \ud835\udc65, \ud835\udc66 = \u03c3\ud835\udc5b  \ud835\udc56=1 \u03c3\ud835\udc5b\n                                   \ud835\udc56=1(\ud835\udc65\ud835\udc56 )2  \ud835\udc56=1(\ud835\udc66\ud835\udc56 )2\n\n\u26ab Considering\n  \u2212  D1 = [1,1,1,1,0,0,0,0]\n  \u2212  D3 = [1,1,0,0,0,0,1,1]\n                                 \ud835\udc36\ud835\udc5c\ud835\udc60(D1,D3)=1/2\n\n25\n---\nJaccard similarity\n\n\u26ab Only considering if there is over lapping or not. We don\u2019t care about the value.\n\n\u26ab For example:            C1    sim(cl,c2)    C1  C2\n\n  \u26ab  \ud835\udc45\ud835\udc65 = [2,0,3,3]     C2\n\n  \u26ab  \ud835\udc45\ud835\udc66 = [1,1,0,5]     C3                    JACCARD SIMILARITY\n                                                  2    0.5\n                                              4  2+1+1\n\n\u26ab Jaccard similarity: \ud835\udc60\ud835\udc56\ud835\udc5a \ud835\udc65, \ud835\udc66 = \ud835\udc79\ud835\udc99\u2229\ud835\udc79\ud835\udc9a\n                                    \ud835\udc79\ud835\udc99\u222a\ud835\udc79\ud835\udc9a\n\n                                               26\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              hyperparameters         Average length of all docs\n                                                                                  27\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b      \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1      \ud835\udc56               1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n     Maybe\u2026a bit confusing                            Average length of all docs\n     Can you speak in English?  hyperparameters\n                                                                                  28\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b             \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1             \ud835\udc56        1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n Relax\u2026it is pretty simple actually    hyperparameters    Average length of all docs\n\n                                                                                  29\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:  IDF term in Q (is it an important word?)\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| )\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              The \u2018percentage\u2019 of querying words in D\n\n                                                                                  30\n---\nIndexing\n\n\u26ab Next question, how to do it efficiently (Indexing)\n\n\u26ab  Suppose we have 1k queries, and there are 1 billion documents in knowledge\n   based, how many times of comparison we need?\n\n\u26ab  1k x 1b\n    It is a really huge number.\n    In real world scenario, it could be even larger\n    If there is only one important task in information retrieval, it must be\n    \u201cindexing\u201d\n\n31\n---\nIndexing - Discrete representation\n\n\u26ab  Inverted index\n\n\u26ab  Since the discrete representation is sparse (most dims are zero), we can build\n   inverted index. For each word, we build a link list to store all the documents\n   contain this word.\n\n\u26ab  For the given query, the complexity is now only related to the #unique words in\n   the query. (In most queries, the size is just few words)\n   doclD                          geo-scopelD              geo-scopelD   docID\n     1                            Europe                   Europe        1 2 7\n     2                                Europe               France        3\n     3                                France               Portugal      5\n     4                                England              England       4\n     5                                Portugal             Quebec        6\n     6                                Quebec               Spain         8\n     7                                Europe\n     8                                Spain\n                     Forward Index                         Inverted Index\n                                                                                 32\n---\n   Indexing - Continuous representation\n\n   \u26ab  In continuous representation, it might be a bit complex. There is no sparse\n      representation anymore.\n\n   \u26ab  We can use the following method to speed up the searching.\n\n       \u26ab  Vector compression \u2013 reduce the size of vectors\n\n       \u26ab  Hierarchical clustering \u2013 in each layer only search the nearest cluster\n                                          Clustering the documents first, and then,\n                                          Only consider the nearest centroid during the searching\n          voronoi cells  xq  Centroids                        voronoi cells  xq    Centroids\n                         o\n                             o                                                     o\n                  e\n                         o\n                  o          \u00a9    o                           9                  o\n                                  -\n                       o                                           o\n         Pause (k)\n\nhttps://www.pinecone.io/learn/series/faiss/faiss-tutorial/                                       33\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n34\n---\nNaive RAG\n\n\u26ab  Step 1 \u2013 indexing\n\n    \u26ab  Divide the document into even chunks, each chunk being a piece of the\n       original text.\n\n    \u26ab  Using the encoding model to generate an embedding for each chunk.\n\n    \u26ab  Store the Embedding of each block in the vector database.\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  Retrieve the k most relevant documents using vector similarity search.\n\n\u26ab  Step 3 \u2013 Generation\n\n    \u26ab  The original query and the retrieved text are combined and input into a LLM\n       to get the final answer\n                                                                             35\n---\n  Naive RAG\n\n    \u26ab Step 1 \u2013 indexing\n\n    \u26ab Step 2 \u2013 Retrieval\n\n    \u26ab Step 3 \u2013 Generation\n\n                                    Offline\n\nDocuments  Document Chunks  Vector Database\n    8                                      36\n   User  Query    Related DocumentChunks\n                            Frozen\n    Augmented Prompt        LLM\n---\nAdvanced RAG\n\n  \u26ab Step 1 \u2013 indexing\n\n  \u26ab + index optimization\n\n  \u26ab + pre-retrieval process\n\n  \u26ab Step 2 \u2013 Retrieval\n\n  \u26ab +post-retrieval process\n\n  \u26ab Step 3 \u2013 Generation\n\n  URLS  PDFs  Database\n     Documents               Document Chunks       Vector Database\n                             Fine-grained Data Cleaning\n                             Sliding Window /Small2Big\n                             Add File Structure\n                             Query Rewrite/Clarifcation\n  User    Query              Retriever Router                          37\n                              Pre-retrieval    Related Document Chunks\n\n  Prompt              LLM                        Rerank  Filter  Prompt Compression\n                                                         Post-retrieval\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Sliding windows\n\n    \u26ab  + index optimization       Fine-grained segmentation\n\n    \u26ab  + pre-retrieval process    Adding metadata\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n38\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Sliding windows\n\n      \u26ab  + index optimization       Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process    Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n                                    Document\n\n         0                200 100   300 200  400 300  500\n\n  Split the doc into chunks, and ensure there is over lapping between chunks (WHY?)\n                                                                                   39\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                                      Sliding windows\n\n      \u26ab  + index optimization          Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                            Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n\n                          Document     Section 1             Paragraph 1.1\n\n                                       Section 2             Paragraph 1.2\n\n                          Searching on fine-grained text     Paragraph 1.3\n                                                                           40\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                               Sliding windows\n\n      \u26ab  + index optimization                        Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                     Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation                             Web page    Publishing date\n\n                                                                    Title\n     The metadata is the aspects of each chunk.\n     It will help both retriever and generator to                Parents node\n     improve the performance.\n\n  41\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process    Summarization\n\n\u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n    \u26ab  +post-retrieval process    Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n\n42\n---\nAdvanced RAG\n\n   \u26ab  Step 1 \u2013 indexing                    Retrieve routes\n\n       \u26ab  + index optimization\n\n       \u26ab  + pre-retrieval process          Summarization\n\n   \u26ab  Step 2 \u2013 Retrieval                   Rewriting\n\n       \u26ab  +post-retrieval process          Confidence judgment\n\n   \u26ab  Step 3 \u2013 Generation                  Instead of one flat \u201cretrieve chunks by embeddings\u201d step, you can:\n\n                                           Searching doc first\n\n  Retrieve routes = multiple retrieval     Query    Document    Chunk\n  paths that a RAG system can choose                            Searching chunks\n  from, depending on query intent, data                         within the doc\n  type, or document structure.\n                                                                                                             43\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing                       Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process             Summarization\n\n\u26ab  Step 2 \u2013 Retrieval                      Rewriting\n\n    \u26ab  +post-retrieval process             Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n                                           Document\n                                                           Summarise first\n                                  Query    Summarisation\n\n                                  Searching in smmarisation\n                                  instead of full documents\n                                                                          44\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing               Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process     Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval              Rewriting\n\n      \u26ab  +post-retrieval process     Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n        Benefits:                    Query\n        a more explicit query             Rewrite the query first\n        a more keyword-rich query    Rewrite the\n        a more structured query      query      Searching\n        multiple diverse sub-queries\n                                     Searching by re-written query\n                                                                  45\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process    Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n      \u26ab  +post-retrieval process    Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n  Query     Document             LLM            Confidence checking    Output\n\n            Confirm the Confidence before output\n             By similarity scores\n             By LLM Confidence scores                                        46\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing           Re-order\n\n    \u26ab  + index optimization    Filter content retrieval\n\n    \u26ab  + pre-retrieval process\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n47\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing           Re-order\n\n      \u26ab  + index optimization    Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation         Evidence#1  Evidence#2  Evidence#3  Question\n\n  LLMs is sensitive with the input order\n  The early input chunks has higher weights\n  How to organize the searched evidence for final output is\n  important\n\n  48\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Re-order\n\n      \u26ab  + index optimization       Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process    Evidence#1  Evidence#2  Evidence#3  Question\n\n  \u26ab  Step 3 \u2013 Generation\n\n                                                           Evidence#1  Evidence#3  Question\n\n  To avoid possible hallucination, filtering the irrelevant\n  evidences.\n\n                                                                                           49\n---\nModular RAG\n\n   Na\u00ef\n\u26ab  ve RAG\n\n      Read           Retrieve    Generate\n\n\u26ab  DSP\n\n   Demonstrate       Search      Predict    Generate\n\n\u26ab  Rewrite-Retrieve-Read\n\n      Rewrite        Retrieve    Read\n\n\u26ab  Retrieve-then-read\n\n   Retrieve          Read        Generate\n\n50\n---\n   Different RAG Paradigms\n\n    Modules\n    8 C 8 E2    Search\nUser Query  Documents    User Query  Documents    Routing    Predict\n\n    Indexing    Query Routing    Indexing    Rewrite  RAG  Rerank\n\n  Read\n                               Fusion\n Memory\n\n    \\Post-Retrieval    Patterns\n \u2192|l|+                               51\nSummary                        Retrieve\n\n    Output    Output\n\n    Naive RAG    Advanced RAG    Modular RAG\n---\nKey problems in RAG\n\n \u26ab  How to retrieve\n\n \u26ab  When to retrieve\n\n \u26ab  How to use the retrieved information\n\n 52\n---\nHow to retrieve\n\n \u26ab By using the information on different structuration levels\n\n \u26ab  Token level         It excels in handling long-tail and cross-domain issues with high\n                        computational efficiency, but it requires significant storage.\n\n \u26ab  Phrase level\n\n \u26ab  Chunk level         The search is broad, recalling a large amount of information, but with\n                        low accuracy, high coverage but includes much redundant information.\n\n \u26ab  Entity level\n\n \u26ab  Knowledge level     Richer semantic and structured information, but the retrieval efficiency\n                        is lower and is limited by the quality of KG.\n\n 53\n---\n   When to retrieve\n\n    \u26ab Two questions:\n\n    \u26ab When we need to retrieve information to support the QA\n\n    \u26ab How many times we need to retrieve the information\n\n    \u26ab Solution#1: Conducting once search during the reasoning process.\n\n    High efficiency, but low relevance of the\n    retrieved documents\n\n    Retrieved document d.\n              Jobs cofounded     Jobs was raisedd;    Jobs is thex    apple\n  Retriever    Apple in his      by adopted...        CEO of        pearnot\n             parents' garage\n Document    Input                 Steve Jobs         Jobs is the     apple    apple\nRetrieval    Reformulation       passed away...       CEO of        pearnot    pearnot    54\nTest Context X    Black-box      Jobs cofoundedJobs is the            apple\n Jobs is the    LM                  Apple...          CEO of           pear\n   CEO of _                                           Ensemble          not\n    Apple\n---\n  When to retrieve\n\n   \u26ab  Two questions:\n\n   \u26ab  When we need to retrieve information to support the QA\n\n   \u26ab  How many times we need to retrieve the information\n\n   \u26ab Solution#2: Adaptively conduct the search.\n\n   Balancing efficiency and information\n   might not yield the optimal solution\n\n Search results:Dx               Retriever\n [1]:Search results:Dq2\n [2]:[1]:Search results:Dq3\n [2]:[1]: ...\nP     [2]: ..                             x\n     x Generate a summary about Joe Biden.\nAy1 Joe Biden attended           q2                55\n Q2[Search(Joe Biden University)]\n y2tthe University of Pennsylvania, where he earned\n q3[Search(Joe Biden degree)]    q3\n y3 a law degree.\n---\n When to retrieve\n\n  \u26ab  Two questions:\n\n  \u26ab  When we need to retrieve information to support the QA\n\n  \u26ab  How many times we need to retrieve the information\n\n  \u26ab Solution#3: Retrieve once for every N tokens generated.\n\n  A large amount of information with low\n  efficiency and redundant information.\n\n    Masked Language Modelling:\n    Bermuda Triangle is in the    western part\n  <MASK> of the Atlantic Ocean.\nPretraining    Atlas\nFew-shot\n          Fact checking:\nBermuda Triangle is in the western                                False    56\n      part of the Himalayas.            The Bermuda\n                                    Triangle is an urban\n                                    legend focused on a\n                                      loosely-defined\n       Question answering:             region in the       Western part of the\n  Where is the Bermuda Triangle?    western part of the    North Atlantic Ocean\n                                       North Atlantic\n                                           Ocean.\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 57\n---\nKey Technologies\n\n\u26ab  Data indexing optimization\n\n\u26ab  Structured Corpus\n\n\u26ab  Retrieval Source Optimization\n\n\u26ab  KG as a Retrieval Data Source\n\n\u26ab  Query Optimization\n\n\u26ab  Embedding Optimization\n\n\u26ab  Fine-tuning on RAG\n\n58\n---\n Data indexing optimization\n\n  \u26ab Chunk optimization\n\n  \u26ab Small-2-big: Embedding at sentence level expand the window during generation\n\n  process.\n\n                   Embed Sentence \u2192 Link to Expanded Window\n\n                              Continuous observation of the Atlantic meridional\n                              overturning circulation (AMOC) has improved the\n                              understanding of its variability (Frajka-Williams et al.,    What the LLM Sees\n                              2019), but there is low confidence in the quantification\n                              of AMOC changes in the 20th century because of low\n                   Embeddingagreement in quantitative reconstructed and simulated\n                              trends. Direct observational records since the\n                   Lookup     mid-2000s remain too short to determine the relative\nQuestion:                     contributions of internal variability, natural                                59\n                              forcing and anthropogenic forcing to AMOC change\nWhat are the                  (high confidence). Over the 21st century, AMOC wil\nconcerns                      very likely decline for all SSP scenarios but will not\n                              involve an abrupt collapse before 2100. 3.2.2.4 Sea Ice\nsurrounding the               Changes\nAMOC?                         Sea ice is a key driver of polar marine life, hosting        What the LLM Sees\n                              unique ecosystems and affecting diverse marine\n                              organisms and food webs through its impact on light\n                              penetration and supplies of nutrients and organic\n                              matter (Arrigo, 2014)\n---\nData indexing optimization\n\n\u26ab  Chunk optimization\n\n\u26ab  Sliding window: sliding chunk covers the entire text, avoiding semantic ambiguity.\n\n                          Maintain overlap for\n                          contextual continuity\n\nLoaded large\ndocument  Dividing into  Merging units into\n          compact units  larger chunks\n\n60\n---\nData indexing optimization\n\n \u26ab  Chunk optimization\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 61\n---\nStructured Corpus\n\n \u26ab  Adding meta-data: adding meta-data in the query searching to improve retrieval\n    accuracy, provide context during chunking, and enables filtering\n\n                             Indexed documents\n                                                                        Filtered subset of\n                                                                        documents    Most relevant\n    Did we implement any new                                                           documents\n       policies in 2021?     year: 2020, content: ...\n\n                             year: 2020, content:                       year: 2021, content.:..\n    year = 2021              year: 2021, content: .    Select relevant  year: 2021, content...    Vector similarity    year: 2021, content:.\n                                                       documents                                  search\n                             year: 2021, content:..                     year: 2021, content...                       year: 2021, content:...\n    Metadata filter          year: 2021, content: .\n\n Filter the irrelevant docs\n\n                           Ensure each chunk contains the metadata\n\n                                                                  62\n---\nRetrieval Source Optimization\n\n \u26ab  Adding meta-data\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 63\n---\nKG as a Retrieval Data Source\n\n\u26ab  Extract entities from the user's input query, then construct a subgraph to form\n   context, and finally feed it into the large model for generation.\n\n    \u26ab  Use LLM (or other models) to extract key entities from the question.\n\n    \u26ab  Retrieve subgraphs based on entities, delving to a certain depth, such as 2\n       hops or even more.\n\n    \u26ab  Utilize the obtained context to generate answers through LLM.Two-stage\n       method: Retrieve documents through summaries, then retrieve text blocks\n       from the documents.\n\n64\n---\n  KG as a Retrieval Data Source\n\n  \u26ab  Extract entities from the user's input query, then construct a subgraph to form\n     context, and finally feed it into the large model for generation.\n     P Meta Summary Entities        x k    Layer[i+1]\n                                    Summary Entities    Summarization by LLM  Layer[i]\n                                    Normal Entities     GMM Clustering        Layer[i-1]\n     Documents                      Hilndex: Indexing with Hierarchical Knowledge\n\n                                    6Communities        Global                                            Community Report\n                  Query             Key Entity          Bridge\n                                    Reasoning Paths                              Reasoning Paths                          Generation by LLM\n                                                        xk\n                                    Hatten KG             Local    Key Entity\n                                                                  Descriptions\n                                                        HiRetrieval: Retrieval with Hierarchical Knowledge\n\nhttps://arxiv.org/pdf/2503.10150                                                                                                           65\n---\n   Query Optimization\n\n    \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n       the Query can yield better retrieval results.\n\n\u26ab  Rewrite query:\n                 Input     Input\n                           Black-box LLM\n\n                 Retriever    Rewriter\n\nInput    Example\nSmall PrLM  Input:\n            What profession does Nicholas Ray and\nRewriter            Elia Kazan have in common?\n\n                                  Query             Quuery              Query: Nicholas Ray profession\n                     Documents    Web Search        Web Search            Query: Elia Kazan profession\n                                  Retriever         Retriever           Elia Kazan was an American film and\n                                                                         theatre director, producer,\n       Black-box LLM                                Documents            screenwriter and actor, described\n           Reader                 Documents                               Nicholas Ray American author and\n                                                                          director, original name Raymond\n                                                                          Nicholas Kienzle, born August 7,\n                     Output       Black-box LLM     Black-box LLM         1911, Galesville, Wisconsin, U.S.\n                                  Reader            Reader               Correct (reader                   director\n                                  Output            Reward Output        Hit (retriever\n                     (a) Retrieve-then-read (b)Rewrite-retrieve-read    (c) Trainable rewrite-retrieve-read\n\n https://arxiv.org/pdf/2305.14283    66\n---\n Query Optimization\n\n  \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n     the Query can yield better retrieval results.\n\n  \u26ab Clarify the query:                                        Ambiguous Question (AQ)\n                                                    \"What country has the most medals         o  M\n                                                             in Olympic history?\"             88\n\n                                                      Tree of Clarifications\n                                                      Question                       Pruned   Information\n                                                   Clarification                              Retrieval\n                                                                       *\n                                                   DQ1            DQ2   DQ3\n                                                                       \"What country has\n                                                                       the most total medals\n                                                                       in Olympic history?\"\n                                                   Question       Question\n                                                   Clarification  Clarification\n                                                   *                   *                      Passages\n                                                   DQ11 DQ12 DQ13 DQ21 DQ22 DQ23 DQ24\n                                                   \"What country has the  \"What country has\n                                                   most medals in winter  the most gold medals\n                                                     Olympic history?\"  in Olympic history?\"\n\n                                                                                     Long Form Answer\n\n                                                     Answer                          \"The United States has the\n                                                   Generation                        most total medals. .\n                                                                                        Norway has won most\n\nhttps://aclanthology.org/2023.emnlp-main.63.pdf                                      medals in winter Olympic.\"    67\n---\n Embedding Optimization\n\n  \u26ab Better embedding always indicate a better retrieval results:\n\n             \u26ab    Selecting a more suitable embedding method\n                                                                0Retriever &  HotpotQA      Dataset\n                  Fine-tuning the embedding model               Framework      EM F1        2Wiki      NQ   WebQ\n             \u26ab                                                  [BM25                       EM F1 EM F1 EM F1\n                                                                               |25.4, 37.2[16.6 21.1[26.0 32.8 [22.2 31.2\n                                                                2+SuRe      38.8 53.523.8.31.036.6 47.934 4 48.5\n                                                                 +EmbQA (ours) 42.0 55.8|27.4 36.642.2 54.438.2 52.1\n                                                                DPR            20.6 21.7[10.8 13.5[25.0 34.2[23.8 34.4\n                                                                 +SuRe         25.0 31.9 14.2 16.038.8 52.336.0 49.6\n                                                                 +EmbQA (ours) |29.8 36.3 16.8 21.0    38.0 52.0\n                                                                                                   43.0 54.4\n                                                                 Contriever   [22.6 35.4\n                                                                                     [16.6 20.7[25.8 32.8\n                                                                                                             25.2 34.2\n                                                                 +SuRe          33.8 50.6 21.0 29.3 39.0 52.834.4 48.5\n                                                                 +EmbQA (ours) 36.6 52.7 26.4 34.2 42.2 53.6\n                  Try different embedding methods in the RAG    [BM25         [21.2 29.2               36.0 49.6\n                                                                20+SuRe        32.2 46.1 [13.8 21.7 18.8 25.319.0 26.1\n                                                                                            17.8 30.1\n                                                                                                    35.2 45.131.6 45.7\n                                                                  +EmbQA (ours) 34.8 44.3 18.6 30.5 35.8 46.035.8 48.1\n                                                                [DPR               7.8 11.0 3.8 4.5[22.2,26.718.8 27.7\n                                                                +Sure          15.0 21.8 6.4 8.540.0 51.8\n                                                                 +EmbQA (ours) 16.2 23.3 7.6 9.6             32.6 47.7\n                                                                                                   |40.2 49.433.4 46.0\n                                                                Contriever     19.4 28.6[13.6 20.7[21.8 27.4117.8,244\n                                                                 +SuRe         28.0 41.6 17.2 25.4 39.8 51.630.2 45.0\n                                                                 +EmbQA (ours)29.8 42.3 17.4 26.2 40.6 51.8 31.6 43.0\n                                                                [BM25          [28.6 37.1[20.2 24.1 [24.0 29.4 [22.6 31.4\n                                                                20+Sure        43.6 54.7 28.4 34.1 41.6 49.0 36.6 47.3\n                                                                 +EmbQA (ours) 44.6 55.628.8 33.8 42.4 49.2 38.2 48.7\n                                                                [DPR           8.8 9.8 5.6 7.1\n                                                                                                  [29.2 32.6[25.6 31.1\n                                                                 +Sure         21.8 27.3 12.2 16.1\n                                                                                                   45.4 54.6\n                                                                                                             38.4 49.6\n                                                                 +EmbQA (ours) 22.6 29.1 13.8 17.345.8 54.7\n  AD                                                                                                      38.6 50.1\n  Facts                                                         Contriever     27.0 34.0[17.6 20.0 26.6 31.9 21.0 29.1\n\n0                                                                +Sure         38.8 50.323.8 30.4 44.0 52.9 36.4 48.1\n                                                                +EmbQA (ours)39.0 50.2\n            General-Purpose                                                              24.4 30.9 45.2 50.5 37.0 48.6\n     C-Pack  Text Embedding                                                                                              68\nhttps://arxiv.org/pdf/2503.01606\n\n  C-MTEB  C-MTP  C-TEM  Recipe\n---\nEmbedding Optimization\n\n \u26ab Better embedding always indicate a better retrieval results:\n\n   \u26ab  Selecting a more suitable embedding method\n\n   \u26ab  Fine-tuning the embedding model\n\n      An in-context learning based method to generate prompt\n\n                                                                                       generate Query-Doc pair    Fine-tuning with pseudo data\n\n 1           A few query and\n             relevant document\n             examples\n             for each doc     You are an award\n             in documents     winning relevance                                 GPT-x\n                              expert. Suggest          Large Language Model     BARD\n                              relevant queries for                              Flan-T5\nDocuments                     this article $article                                                               69\n                              queries:                 Synthetic queries for documents\n             LLM Query Generation Prompt\n                                                       \"Labeled data\"                  9\n---\nFine-tuning on RAG\n\n\u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n  \u26ab  Retriever fine-tuning\n\n  \u26ab  Generator fine-tuning\n\n70\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n      \u26ab  Retriever fine-tuning\n\n      \u26ab  Generator fine-tuning\n\n                              A small LM\n\n    Using the attention scores\n    annotate which documents\n    the LM \u201cprefers\u201d.\n\n                                                DecSource LM\n                                                    Fusion-in-Decoder\n                                                Enc Enc. Enc\n    Source Task                                     Q+D1 Q+D2Q+DN\n                                                    Retrieve\n                                                      N Docs\n  Positives         Negatives                   Pre-Trained Retriever    71\nGround Truth U    ANCE Sampling\n    -Top-K FiDAtt                               Target LMs Target Tasks\n    https://aclanthology.org/2023.acl-long.136.pdf\n                                     Generic                GCMETRY\n                                     Plug-In                  WAKT\n   Augmentation-Adapted Retriever                           RISTORY\n                                                            LITERATRE\n                                                            SCIENCE\n                                                              MATH\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n    \u26ab Retriever fine-tuning\n\n    \u26ab Generator fine-tuning\n\n    Product Search                 Premium hiking bag.            Unstructured Data                                             Structured Data (Negative)\n    Black large capacity hiking    Size: Max Color: Black                                     Structured Data (Positive)\n    bag, made of canvas.           Material: canvas               These erasable mood pencils     [MASK1] changing [MASK2]       Tire Specifications: Material:\n\nCode Search\nGiven two numbers, the\n\nlargest number is returned\nQuery\n]\nPretrained Language Model\n                               are made of quality wood       [MASK3] with [MASK4]            Rubber Tire Size: 16X6.50-8\n                               and color temperature          Function: removing wrong        Tire Type: Tubeless Rim\ndef compare(a, b):             coating, have non-fading        writing Material: [MASK2]      Width: 5.375\" Tread Depth:\nreturn max(a, b)          colors.                              Changing Size: 18 x 0.5 cm     7.1mm\" Pattern: P332\nStructured Data (Positive)\n Structured Data (Negative)                                               T5\n                               Structured Data Alignment (Loss: C sDA)      Masked Entity Prediction (Loss: LP)\n\n    Training    Push Away                           Prediction                                          erasers\n                                                    [MASK1] Color\n                Ground Truth                        [MASK2] mood\n                                Align               [MASK3] pencils,\n    .                                               [MASK4]\n\n    Embedding Space    Optimized Embedding Space    Add an entity prediction loss in the fine-tuning           72\n---\nK\nING'S\nCollege\nLONDON\n\nThank you\n\nDr Lin Gui\nLin.1.Gui@kcl.ac.uk\nwww.kcl.ac.uk/people/lin-gui\n\n"
    },
    "data/raw/vector25aug-v2.pdf": {
        "metadata": {
            "file_name": "vector25aug-v2.pdf",
            "file_type": "pdf",
            "content_length": 68177,
            "language": "en",
            "extraction_timestamp": "2025-11-26T00:26:48.850082+00:00",
            "timezone": "utc"
        },
        "content": "Vector     Word Meaning\nSemantics &\nEmbeddings\n---\nWhat do words mean?\n\nN-gram or text classification methods we've seen so far\n\u25e6  Words are just strings (or indices wi in a vocabulary list)\n\u25e6  That's not very satisfactory!\nIntroductory logic classes:\n\u25e6  The meaning of \"dog\" is DOG; cat is CAT\n   \u2200x DOG(x) \u27f6 MAMMAL(x)\nOld linguistics joke by Barbara Partee in 1967:\n\u25e6  Q: What's the meaning of life?\n\u25e6  A: LIFE\nThat seems hardly better!\n---\nDesiderata\n\nWhat should a theory of word meaning do for us?\nLet's look at some desiderata\nFrom lexical semantics, the linguistic study of word\nmeaning\n---\n   Lemmas and senses\n          lemma\n          mouse (N)\n\nsense     1. any of numerous small rodents...\n          2. a hand-operated device that controls\n          a cursor...    Modified from the online thesaurus WordNet\n\n  A sense or \u201cconcept \u201d is the meaning component of a word\n  Lemmas can be polysemous (have multiple senses)\n---\nRelations between senses: Synonymy\n\nSynonyms have the same meaning in some or all\ncontexts.\n\u25e6  filbert / hazelnut\n\u25e6  couch / sofa\n\u25e6  big / large\n\u25e6  automobile / car\n\u25e6  vomit / throw up\n\u25e6  water / H\u20820\n---\nRelations between senses: Synonymy\n\nNote that there are probably no examples of perfect\nsynonymy.\n\u25e6  Even if many aspects of meaning are identical\n\u25e6  Still may differ based on politeness, slang, register, genre,\n   etc.\n---\nRelation: Synonymy?\n\nwater/H\u20820\n\"H\u20820\" in a surfing guide?\nbig/large\nmy big sister != my large sister\n---\nThe Linguistic Principle of Contrast\n\nDifference in form \u00e0 difference in meaning\n---\nAbb\u00e9 Gabriel Girard 1718                       LA' JUSTESSE\n                                               DE LA\nRe: \"exact\" synonyms                           LANGUE FRANCOISE.\n                                               o v\n\"jc ne crois pas qu'il y ait de                LES DIFFERENTES SIGNIFICATIONS\n                                               DES MOTS QUI PASSENT\nmot fynonime dans aucune                       POU.R\n \"                                             SYNONIMES\nLangue. Je le dis par con-                     PAr M.PAbb\u00c9 GIRARD C.D.M.D.D.B.\n [I do not believe that there      SPIRAT\n is a synonymous word in any\n language]                         A PARIS,\n                                   CheZ I.AURENT D'HOURY, IMprimeur-\n Lbraire, au bas de la rue de la Harpe,vis-\n d vis la rue S. Severin, au Saint Efprit.\n                                               M DCC.XVIII.\n                                   Avce Approbason & Frivilegs dus Roy.\n                        Thanks to Mark Aronoff!\n---\nRelation: Similarity\n\nWords with similar meanings. Not synonyms, but sharing\nsome element of meaning\n\ncar,  bicycle\ncow,  horse\n---\nAsk humans how similar 2 words are\n\nword1      word2          similarity\nvanish     disappear      9.8\nbehave     obey           7.3\nbelief     impression     5.95\nmuscle     bone           3.65\nmodest     flexible       0.98\nhole       agreement      0.3\n\n          SimLex-999 dataset (Hill et al., 2015)\n---\n    Relation: Word relatedness\n\nAlso called \"word association\"\nWords can be related in any way, perhaps via a semantic\nframe or field\n\n \u25e6  coffee, tea: similar\n \u25e6  coffee, cup: related, not similar\n---\nSemantic field\n\nWords that\n\u25e6  cover a particular semantic domain\n\u25e6  bear structured relations with each other.\n\n hospitals\n   surgeon, scalpel, nurse, anaesthetic, hospital\n restaurants\n   waiter, menu, plate, food, menu, chef\n houses\n   door, roof, kitchen, family, bed\n---\nRelation: Antonymy\n\nSenses that are opposites with respect to only one\nfeature of meaning\nOtherwise, they are very similar!\n    dark/light      short/long fast/slow  rise/fall\n    hot/cold        up/down      in/out\nMore formally: antonyms can\n \u25e6    define a binary opposition or be at opposite ends of a scale\n   \u25e6  long/short, fast/slow\n \u25e6    Be reversives:\n   \u25e6  rise/fall, up/down\n---\nConnotation (sentiment)\n\n\u2022 Words have affective meanings\n  \u2022     Positive connotations (happy)\n  \u2022     Negative connotations (sad)\n\u2022 Connotations can be subtle:\n  \u2022     Positive connotation: copy, replica, reproduction\n  \u2022     Negative connotation: fake, knockoff, forgery\n\u2022 Evaluation (sentiment!)\n  \u2022     Positive evaluation (great, love)\n  \u2022     Negative evaluation (terrible, hate)\n---\nConnotation\n                                          Osgood et al. (1957)\nWords seem to vary along 3 affective dimensions:\n\u25e6  valence: the pleasantness of the stimulus\n\u25e6  arousal: the intensity of emotion provoked by the stimulus\n\u25e6  dominance: the degree of control exerted by the stimulus\n\n                  Word          Score      Word         Score\n    Valence       love           1.000     toxic              0.008\n                  happy          1.000     nightmare          0.005\n    Arousal       elated         0.960     mellow             0.069\n                  frenzy         0.965     napping            0.046\n    Dominance     powerful       0.991     weak               0.045\n                  leadership     0.983     empty              0.081\n\n                                           Values from NRC VAD Lexicon (Mohammad 2018)\n---\nSo far\n\nConcepts or word senses\n\u25e6  Have a complex many-to-many association with words (homonymy,\n   multiple senses)\nHave relations with each other\n\u25e6  Synonymy\n\u25e6  Antonymy\n\u25e6  Similarity\n\u25e6  Relatedness\n\u25e6  Connotation\n---\nVector     Word Meaning\nSemantics &\nEmbeddings\n---\nVector     Vector Semantics\nSemantics &\nEmbeddings\n---\nComputational models of word meaning\n\nCan we build a theory of how to represent word\nmeaning, that accounts for at least some of the\ndesiderata?\nWe'll introduce vector semantics\n The standard model in language processing!\n Handles many of our goals!\n---\nLudwig Wittgenstein\n\nPI #43:\n\"The meaning of a word is its use in the language\"\n---\nLet's define words by their usages\n\nOne way to define \"usage\":\nwords are defined by their environments (the words around them)\n\nZellig Harris (1954):\nIf A and B have almost identical environments we say that they\nare synonyms.\n---\nWhat does recent English borrowing ongchoi mean?\n\nSuppose you see these sentences:\n    \u2022 Ong choi is delicious saut\u00e9ed with garlic.\n    \u2022 Ong choi is superb over rice\n    \u2022 Ong choi leaves with salty sauces\nAnd you've also seen these:\n    \u2022  \u2026spinach saut\u00e9ed with garlic over rice\n    \u2022  Chard stems and leaves are delicious\n    \u2022  Collard greens and other salty leafy greens\nConclusion:\n\u25e6 Ongchoi is a leafy green like spinach, chard, or collard greens\n  \u25e6 We could conclude this based on words like \"leaves\" and \"delicious\" and \"sauteed\"\n---\nOngchoi: Ipomoea aquatica \"Water Spinach\"\n\n  \u7a7a\u5fc3\u83dc\n  kangkong\n  rau mu\u1ed1ng\n  \u2026\n\n  Yamaguchi, Wikimedia Commons, public domain\n---\nIdea 1: Defining meaning by linguistic distribution\n\nLet's define the meaning of a word by its\ndistribution in language use, meaning its\nneighboring words or grammatical environments.\n---\nIdea 2: Meaning as a point in space (Osgood et al. 1957)\n3 affective dimensions for a word\n\u25e6    valence: pleasantness\n\u25e6    arousal: intensity of emotion\n\u25e6    dominance: the degree of control exerted\n              Word          Score      Word         Score\nValence       love           1.000     toxic             0.008\n              happy          1.000     nightmare         0.005\nArousal       elated         0.960     mellow            0.069  NRC VAD Lexicon\n              frenzy         0.965     napping           0.046  (Mohammad 2018)\nDominance     powerful       0.991     weak              0.045\n\u25e6             leadership     0.983     empty             0.081\nHence the connotation of a word is a vector in 3-space\n---\nIdea 1: Defining meaning by linguistic distribution\n\nIdea 2: Meaning as a point in multidimensional space\n---\nDefining meaning as a point in space based on distribution\n?\nEach word = a vector (not just \"good\" or \"w\u2084\u2085\")\nSimilar words are \"nearby in semantic space\"   drinks\nWe build this space automatically by           alcoholic\n                                         seeing which words are\nnearby in text       candy chocolate           cider\n\n                     cream\n                                         juice\n                     0     honey                    wine\n\n                     0  corn  rice\n\n \u2022                                       beef  fried  soup drink\n                                         potatoes\n                              wheat      foods   pork   cooking\n\n                                         vegetablesbread\n---\nWe define meaning of a word as a vector\n\nCalled an \"embedding\" because it's embedded into a\nspace (see textbook)\nThe standard way to represent meaning in NLP\n Every modern NLP algorithm uses embeddings as\n the representation of word meaning\nFine-grained model of meaning for similarity\n---\nIntuition: why vectors?\n\nConsider sentiment analysis:\n\u25e6   With words, a feature is a word identity\n  \u25e6  Feature 5: 'The previous word was \"terrible\"'\n  \u25e6  requires exact same word to be in training and test\n\u25e6   With embeddings:\n  \u25e6  Feature is a word vector\n  \u25e6  'The previous word was vector [35,22,17\u2026]\n  \u25e6  Now in the test set we might see a similar vector [34,21,14]\n  \u25e6  We can generalize to similar but unseen words!!!\n---\nWe'll discuss 2 kinds of embeddings\n\nSimple count embeddings\n\u25e6  Sparse vectors\n\u25e6  Words are represented by the counts of nearby words\n\nWord2vec\n\u25e6  Dense vectors\n\u25e6  Representation is created by training a classifier to predict whether a\n   word is likely to appear nearby\n\u25e6  Later we'll discuss extensions called contextual embeddings\n---\n    From now on:\n    Computing with meaning representations\n    Vector Semantics and\n    instead of string representations\n    Embeddings\n\n   C\u8005@\u00c2(|\uff0c\u00f3|\u800c\u00ffC Nets are for fish;\n                Once you get the fish, you can forget the net.\n   \u8a00\u8005@\u00c2(\u270f\uff0c\u00f3\u270f\u800c\u00ff\u8a00 Words are for meaning;\n                Once you get the meaning, you can forget the words\n                                      \u00d1P(Zhuangzi), Chapter 26\n\n         The asphalt that Los Angeles is famous for occurs mainly on its freeways. But\n in the middle of the city is another patch of asphalt, the La Brea tar pits, and this\nasphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-\n---\nVector     Vector Semantics\nSemantics &\nEmbeddings\n---\nVector     Count-based embeddings\nSemantics &\nEmbeddings\n---\n Remember the intuition: words with similar\n neighborhoods have similar meanings\nHow to measure a word's neighborhood?\nWord-context matrix (a kind of co-occurrence matrix)\n  \u25e6  each row represents a word in the vocabulary\n  \u25e6  each column represents how often each other word in the\n     vocabulary appears nearby\n---\nWord-Context Matrix\n\n            aardvark\n            abacus    zydeco\n            adept\n                    affect\n                      agate     How often does\n\naardvark                  \u2026     agate occur\nabacus                          near abacus?\nadept\naffect\nagate\n\u2026\n\nzydeco\n---\nWord-Context Matrix\nWhat does \"nearby\" mean?\nFor right now let's say \"within 4 words\"\n---\n most common, however, to use smaller contexts, generally a window around the\nThe word-context matrix\n word, for example of 4 words to the left and 4 words to the right, in which case\n the cell represents the number of times (in some training corpus) the column word\nOne set of 4-word contexts\n occurs in such a \u00b14 word window around the row word. For example here is one\n example each of some words in their windows:\n\n is traditionally followed by       cherry       pie, a traditional dessert\n often mixed, such as               strawberry   rhubarb pie. Apple pie\n computer peripherals and personal  digital      assistants. These devices usually\n a computer. This includes          information  available on the internet\n If we then take every occurrence of each word (say strawberry) and count the con-\n text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n simplified subset of the word-word co-occurrence matrix for these four words com-\n puted from the Wikipedia corpus (Davies, 2015).\n Note in Fig. 6.5 that the two words cherry and strawberry are more similar to\n each other (both pie and sugar tend to occur in their window) than they are to other\n words like digital; conversely, digital and information are more similar to each other\n than, say, to strawberry. Fig. 6.6 shows a spatial visualization.\n---\nLet's consider a mini-matrix of 3 words.\n most common, however, to use smaller contexts, generally a window around the\n word, for example of 4 words to the left and 4 words to the right, in which case\nHow often do \"a\", \"computer\", and \"pie\n the cell represents the number of times (in some training corpus) the column word\noccur in the context of \"cherry\"?\n occurs in such a \u00b14 word window around the row word. For example here is one\n example each of some words in their windows:\n\n is traditionally followed by       cherry       pie, a traditional dessert\n often mixed, such as               strawberry   rhubarb pie. Apple pie\n computer peripherals and personal  digital      assistants. These devices usually\n a computer. This includes          information  available on the internet\n If we then take every occurrence of each word (say strawberry) and count the con-\n text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n simplified subset of the word-word co-occurrence matrix for these four words com-\n puted from the Wikipedia corpus (Davies, 2015).\n Note in Fig. 6.5 that the two words cherry and strawberry are more similar to\n each other (both pie and sugar tend to occur in their window) than they are to other\n words like digital; conversely, digital and information are more similar to each other\n than, say, to strawberry. Fig. 6.6 shows a spatial visualization.\n---\n  most common, however, to use smaller contexts, generally a window around the\n The word-          5.3             \u2022    S IMPLE COUNT- BASED EMBEDDINGS              7\n               context mini-matrix for just 4 words\n  word, for example of 4 words to the left and 4 words to the right, in which case\n  the cell represents the number of times (in some training corpus) the column word\n and 3 contexts\ncontext co-occurrence matrix is very large, because for each word in the vocabulary\n  occurs in such a \u00b14 word window around the row word. For example here is one\n(since |V |) we have to count how often it occurs with every other word in the vo-\n  example each of some words in their windows:\ncabulary, hence dimensionality |V | \u21e5 |V |. Let\u2019s therefore instead sketch the process\n               is traditionally followed by  cherry         pie, a traditional dessert\non a smaller scale. Imagine that we are going to look at only the 4 words, and only\nconsider       often mixed, such as          strawberry     rhubarb pie. Apple pie\n             the following 3 context words:     a, computer, and pie.       Furthermore let\u2019s\nassume computer peripherals and personal     digital        assistants. These devices usually\n we only count occurrences in the mini-corpus above.\n So before     a computer. This includes     information    available on the internet\n  If           looking at Fig. 5.2, compute by hand the counts for these 3 context\nwords for we then take every occurrence of each word (say strawberry) and count the con-\n             the four words cherry, strawberry, digital, and information.\n  text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n  simplified subset of the word-word co-occurrence matrix for these four words com-\n  puted from   a                                computer                    pie\n cherry        the Wikipedia corpus (Davies, 2015).\n               Note in Fig. 6.51that the two words     0                    1\nstrawberry     0                                      cherry and strawberry are more similar to\n  each other (both pie and sugar                       0                    2\n digital       0                    tend to occur in their window) than they are to other\n  words like digital; conversely, digital and          1                    0\ninformation    1                                      information are more similar to each other\n  than, say, to strawberry. Fig. 6.6 shows a 1                              0\nFigure 5.2     Co-occurrence vectors for four         spatial visualization.\n                                                words with counts from the 4 windows above,\n---\n         consider the following 3 context words: a, computer, and pie. Furthermore let\u2019s\n         assume we only count occurrences in the mini-corpus above.\n        The word-context mini-matrix for just 4 words\n          So before looking at Fig. 5.2, compute by hand the counts for these 3 context\n        and 3 contexts\n         words for the four words cherry, strawberry, digital, and information.\n\n                         a                       computer              pie\n         cherry          1                          0                  1\n         strawberry      0                          0                  2\n          digital        0                          1                  0\n         information     1                          1                  0\n\n        \u2022 Figure 5.2    Co-occurrence vectors for four words with counts from the 4 windows above,\n          This 4x3 matrix is a subset of full |V| x |V| matrix\n         showing just 3 of the potential context word dimensions. The vector for cherry is outlined in\n\n        \u2022 red. Note that a real vector would have vastly more dimensions and thus be even sparser.\n          Each word is represented by a row vector with\n          Hopefully your count matches what is shown in Fig. 5.2, so that each cell repre-\n          dimensionality [1 x |V|]\n         sents the number of times a particular word (defined by the row) occurs in a partic-\n        \u2022 ular context (defined by the word column).\n          With co-occurrence counts with each other word\n          Each row, then, is a vector representing a word.   To review some basic linear\nctor     algebra, a vector is, at heart, just a list or array of numbers. So cherry is represented\n---\n    most common, however, to use smaller contexts, generally a window around the\nhere is one example each of some words in their windows:\n    word, for example of 4 words to the left and 4 words to the right, in which case\n      is traditionally followed by      cherry           pie, a traditional dessert\n    the cell represents the number of times (in some training corpus) the column word\nA                  often mixed, such as strawberry       rhubarb pie. Apple pie\nselection from a larger word-context matrix\n    occurs in such a \u00b14 word window around the row word. For example here is one\ncomputer peripherals and personal       digital          assistants. These devices usually\n    example each of some words in their windows:\n            a computer. This includes   information      available on the internet\n      If we  is traditionally followed by cherry         pie, a traditional dessert\n            then take every occurrence of each word (say strawberry) and count the\ncontext            often mixed, such as   strawberry     rhubarb pie. Apple pie\n            words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a\n      computer peripherals and personal   digital        assistants. These devices usually\nsimplified subset of the word-word co-occurrence matrix for these four words com-\n             a computer. This includes    information    available on the internet\nputed from the Wikipedia corpus (Davies, 2015).\n    If we then take every occurrence of each word (say strawberry) and count the con-\n    text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n    simplified     aardvark     ...    computer    data  result        pie    sugar     ...\n       cherry     subset of the word-word co-occurrence matrix for these four words com-\n    puted from    0             ...       2         8     9            442     25       ...\n     strawberry    the Wikipedia corpus (Davies, 2015).\n     Note in Fig. 0             ...       0         0     1            60      19       ...\n      digital      6.5 that the two words cherry and strawberry are more similar to\n    each other (both 0          ...     1670       1683  85            5       4        ...\n    information    pie and sugar tend to occur in their window) than they are to other\n    words like    0             ...     3325       3982  378           5       13       ...\nFigure 6.6        digital; conversely, digital and information are more similar to each other\n    than,    Co-occurrence vectors for four words in the Wikipedia corpus, showing six of\nthe         say, to strawberry. Fig. 6.6 shows a spatial visualization.\n      dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\nred. Note that a real vector would have vastly more dimensions and thus be much sparser.\n---\ncomputer\n\n4000\n        information\n3000    [3982,3325]\n        digital\n2000    [1683,1670]\n\n1000\n\n1000 2000 3000 4000\ndata\n---\nThe word-context matrix\n\nWord context matrix is |V| x |V|\nThis could be 50,000 x 50,000\nMost of these numbers are zero!\nSo these are sparse vectors\nThere are efficient algorithms for storing and\ncomputing with sparse matrices\n---\nVector     Count-based embeddings\nSemantics &\nEmbeddings\n---\n           Cosine for computing word similarity\nVector\nSemantics &\nEmbeddings\n---\nhence of length |V |, or both with documents as dimensions as documents, of length\n|D|) and gives a measure of their similarity. By far the most common similarity\n Computing word similarity: Dot product and cosine\nmetric is the cosine of the angle between the vectors.\n The cosine\u2014like most measures for vector similarity used in NLP\u2014is based on\nthe dot product operator from linear algebra, also called the inner product:\n The dot product between two vectors is a scalar:\n N\n dot product(v, w) = v \u00b7 w = X vi wi = v1 w1 + v2 w2 + ... + vN wN          (6.7)\n i=1\n The dot product tends to be high when the two\nAs we will see, most metrics for similarity between vectors are based on the dot\n vectors have large values in the same dimensions\nproduct. The dot product acts as a similarity metric because it will tend to be high\njust when the two vectors have large values in the same dimensions. Alternatively,\n Dot product can thus be a useful similarity metric\nvectors that have zeros in different dimensions\u2014orthogonal vectors\u2014will have a\n between vectors\ndot product of 0, representing their strong dissimilarity.\n This raw dot product, however, has a problem as a similarity metric: it favors\nlong vectors. The vector length is defined as\n---\n  ill see, most metrics for similarity between vectors are based on the dot\nt. The dot product acts as a similarity metric because it will tend to be high\n           Problem with raw dot-product\n  n the two vectors have large values in the same dimensions. Alternatively,\n  that have zeros in different dimensions\u2014orthogonal vectors\u2014will have a\n  uct of Dot product favors long vectors\n           0, representing their strong dissimilarity.\n  s raw dot product, however, has a problem as a similarity metric: it favors\n           Dot product is higher if a vector is longer (has higher\n  ctors. The vector length is defined as\n           values in many dimension)\n           Vector length:        v\n                                 u N\n                                 uX\n                             |v| = t    v2                        (6.8)\n                                        i\n                                     i=1\n           Frequent words (of, the, you) have long vectors (since\n product is higher if a vector is longer, with higher values in each dimension.\n           they occur many times with other words).\n equent words have longer vectors, since they tend to co-occur with more\n  nd have higher co-occurrence values with each of them. The raw dot product\n  l be     So dot product overly favors frequent words\n           higher for frequent words. But this is a problem; we\u2019d like a similarity\n---\n             This raw dot product, however, has a problem as a similarity metric: it favors\nctor length  long vectors. The vector length is defined as\n  The cosine similarity metric between two vectors ~\n                                                                      v and ~\n    Alternative: cosine for                                            w thus can be computed\n                                                  computing word similarity\n :                                                          v\n                                                            u N\n                                                            uX\n                                                  |v| = t             v2            (6.8)\n                                                                      iN\n\n             The dot product is higher if                    i=1 X vi wi\n                                              a vector is longer, with higher values in each dimension.\n                                              ~\n                                              v \u00b7 ~\n               cosine(                            w          v        i=1 v\n                                ~\n                                v, ~\n                                 w) =                  =                            (6.10)\n             More frequent words have longer vectors, since they tend to co-occur with more\n                                              |              u              u\n                                              ~\n                                                   v||~\n             words and have                        w|           N            N\n                                higher co-occurrence values with each of them. The raw dot product\n                                                             uX uX\n             thus will be higher for frequent                t              2 t  2\n                                                   words. But this is a problem; we\u2019d like a similarity\n             metric that tells us how similar two words are           vi         wi\n                                                                regardless of their frequency.\n             We modify the dot                                  i=1              i=1\n                                   product to normalize for the vector length by dividing the\n      For some applications we pre-normalize each vector, by dividing it by its length,\n             dot product by the lengths of each of the two vectors. This normalized dot product\neating a     turns out to be the same as the cosine of the angle between the two vectors, following\n             unit vector of length 1. Thus we could compute a unit vector from ~\n               Based on the definition of the dot product between two vectors a                        a by\n             from the definition of the dot product between two vectors a and b:              and b\nviding it by |~\n             a|. For unit vectors, the dot product is the same as the cosine.\n      The cosine value ranges from 1 for vectors pointing in the same direction, through\n                                                   a \u00b7 b = |a||b| cos q\n or vectors that are orthogonal, to -1 for vectors pointing in opposite directions.\n                                                   |a \u00b7 b  = cos q                            (6.9)\nut raw frequency values are                        a||b|\n                                   non-negative, so the cosine for these vectors ranges\n---\nCosine as a similarity metric\n\n                                         1\n-1: vectors point in opposite directions 0.5\n+1: vectors point in same directions        50  400  150  200  250  300  350\n0: vectors are orthogonal                -0.5\n\n                                         -1\n\nBut since raw frequency values are non-negative, the\n50\ncosine for term-term matrix vectors ranges from 0\u20131\n---\n        0 for vectors that are orthogonal, to -1 for vectors pointing in opposite direction\nraw frequency values are non-negative, so the cosine for these vectors ranges\n rs        Let\u2019s see how the cosine computes which of the words cherry or digital is c\n        that are orthogonal, to -1 for vectors pointing in opposite directions.\n        But raw frequency values are non-negative, so the cosine for these vectors rang\n  0\u20131.  Cosine examples\n        in meaning to information, just using raw counts from the following shortened t\nequency values are non-negative, so the cosine for these vectors ranges\n        from 0\u20131.\n Let\u2019s see how the cosine computes which of the words cherry or digital is closer\n           Let\u2019s see how the cosine computes which of the words cherry or digital is clos\n eaning to information, just                                          pie  data  computer\n        in meaning to                   using raw counts from the following shortened table:\nsee how the cosine information, just using raw counts from the following shortened tabl\n                             computes which of the words cherry or digital is closer\n                                                  cherry              442     8   pie     data     computer\ng to       v \u2022 w  v      w       \u2211 N v wpie                     data       computer  2\n        information, just using raw counts from the following shortened table:\n                                                  i  i\n        cos(v, w) =  =   \u2022  =                i=1  digital             5    1683   1670\n                                                                   piecherry      442     8        2\n                     v w  v  w  \u2211 N               \u2211 N                     data   computer\n                                cherry               442           8       2\n                                             information              5    3982   3325\n                                     v 2                 w 2\n                                                  cherry           442     8      2\n                                             i             i          digital     5       1683     1670\n                                 i=1pie              data       computer\n                                digital              i=1\n                                                      5         1683       1670\n                                                  digital        5       1683    1670\n                          cherry     442              8               information 5       3982     3325\n                                                                4422\n                             information              5         3982 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                     information                 5         3325\n                                                         p               3982    3325\n        cos(cherry, information) =                                               p                      = .017\n                             digital         5       1683             21670\n                          information        5             442        + 82 + 22       52 + 39822 + 33252\n                                                     3982              3325\n                                                  442 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                                                442 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                        p                          5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\ns(cherry, information) =                             p                p          p                = .017\n        cos(cherry, information) =                       p                            p                = .017\n        cos(digital, information) =                                                                          = .\n                                         4422 + 82 + 22                    52 + 39822 + 33252\n                                                           4422 + 82 + 22        52 + 39822 + 33252\n                                        442 \u21e4 5 + 8 5\u00b2 + 1683\u00b2 + 1670\u00b2                   52 + 39822 + 33252\n                                                           \u21e4 3982 + 2 \u21e4 3325\n                                                  5 \u21e4 5 + 5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\n                             p                                  1683 \u21e4 3982 + 1670 \u21e4 3325\nrry, information) =                                  p          p                     p   = .017\n        cos(digital, information) =                                                                        = .996\ns(digital, information) =               p                                     p                        = .996\n           The model decides that information is way closer to digital than it is to cher\n                                4422 + 82 + 22                  52 + 39822 + 33252\n                                             2             52 + 16832 + 16702         52 + 39822 + 33252\n        result that seems                5        + 16832 + 16702                52 + 39822 + 33252\n                                sensible. Fig. 6.7 shows a visualization.\n                             51         5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\n tal, information) =         p                                        p                           = .996\n           The model decides that information is way closer to digital than it is to cherry,\n The model decides that information is way closer to digital than it is to cherry, a\n        result that seems          52 + 16832 + 16702                    52 + 39822 + 33252\n lt                                sensible. Fig. 6.7 shows a visualization.\n        that seems sensible. Fig. 6.7 shows a visualization.\n---\n   V Visualizing cosines\n    S                   E\n    (well, angles)\n    ECTOR EMANTICS AND   MBEDDINGS\n\n        \u2019\n        pie\n        \u2018\n        1:  500\n        Dimension  cherry\n                         digital    information\n\n                   500   1000  1500  2000  2500  3000\n\n                         Dimension 2: \u2018computer \u2019\nre 6.7   A (rough) graphical demonstration of cosine similarity, showing vec\n---\nVector       Cosine for computing word\nSemantics &  similarity\nEmbeddings\n---\nVector     TF-IDF\nSemantics &\nEmbeddings\n---\nBut raw frequency is a bad representation\n\n\u2022  The co-occurrence matrices we have seen represent each\n   cell by word frequencies.\n\u2022  Frequency is clearly useful; if sugar appears a lot near\n   apricot, that's useful information.\n\u2022  But overly frequent words like the, it, or they are not very\n   informative about the context\n\u2022  It's a paradox! How can we balance these two conflicting\n   constraints?\n---\n    fool                36     0.012\n    Two good            37     0\n    common solutions for word weighting\n    sweet               37     0\n ighting of the value for word t in document d , wt ,d thus combines\nth idf: tf-idf:  tf-idf value for word t in document d:\n\n                 wt ,d = tft ,d \u21e5 idft                     (6.13)\n\n idf weighting to the Shakespeare term-document matrix in Fig. 6.2.\n        Words like \"the\" or \"it\" have very low idf\nvalues for the dimension corresponding to the word         good have\n ince   PMI: (Pointwise mutual information)\n       this word appears in every document, the tf-idf algorithm\nd in any comparison                       #(% ,% )\n                        of the plays. Similarly, the word  fool, which\n        \u25e6 PMI !! , !\"   = $%& # % !#( \"\n the 37 plays, has a much lower           !  %\" )\n                                          weight.\n hting is by far the dominant way of weighting co-occurrence ma-\n          See if words like \"good\" appear more often with \"great\" than\n n retrieval, but also plays a role in many other aspects of natural\n          we would expect by chance\n akespeare\u2019s favorite adjectives, a fact probably related to the increased use of\n---\n     Term frequency (tf) in the tf-idf algorithm\n                       tft , d  = count(t , d )\n\n commonly we squash the raw frequency a bit, by using the lo\n cy  We could imagine using raw count:\n     instead. The intuition is that a word appearing 100 times\nn\u2019t make that word 100 times more likely to be relevant to the\n     tf\u209c,d = count(t,d)\n ment. We also need to do something special with counts of 0,\n he log of 0. 2\n     But instead of using raw count, we usually squash a bit:\n               (\n     tft , d   =  1 + log10 count(t , d )     if count(t , d ) > 0\n                  0                           otherwise\n\nuse log weighting, terms which occur 0 times in a document wou\n---\n      for discriminating those documents from the rest of the collection; terms that occur\nt     frequently across the entire collection aren\u2019t as helpful. The document frequency\n      df  Document frequency (df)\n      t   of a term t is the number of documents it occurs in. Document frequency is\n      not the same as the collection frequency of a term, which is the total number of\n      times the word appears in the whole collection in any document. Consider in the\n          df is the number of documents t occurs in.\n      collection of Shakespeare\u2019s 37 plays the two words Romeo and action. The words\n             t\n      have identical collection frequencies (they both occur 113 times in all the plays) but\n          (note this is not collection frequency: total count across\n      very different document frequencies, since Romeo only occurs in a single play. If\n          all documents)\n      our goal is to find documents about the romantic tribulations of Romeo, the word\n          \"Romeo\" is very distinctive for one Shakespeare play:\n      Romeo should be highly weighted, but not action:\n                         Collection Frequency  Document Frequency\n              Romeo      113                   1\n              action     113                   31\n          We emphasize discriminative words like Romeo via the inverse document fre-\nf     quency or idf term weight (Sparck Jones, 1972). The idf is defined using the frac-\n      tion N /df , where N is the total number of documents in the collection, and df is\n---\ner of documents in many collections, this measure\n      common as to be completely non-discriminative since they o\n      Inverse document frequency (idf)\n      good or sweet.3\ng function. The resulting definition for inverse\nus                                           Word         df     idf\n            \u2713        \u25c6                       Romeo        1      1.57\n      idf  = log  N                          salad        2      1.27\n                                             Falstaff     4         (6.13)\n      t           10  dft                    forest       12     0.967\n                                                                 0.489\n                                             battle       21     0.246\n e words in the Shakespeare corpus, ranging from\n      N is the total number of documents     wit          34     0.037\n hich occur in only one play like fool                    36     0.012\n      in the collection                      Romeo, to those that\n alstaff, to those which are very            good         37     0\n                                            common like fool or so\n                                             sweet        37     0\nn-discriminative since they occur in all 37 plays like\n---\nWhat is a document?\n\nCould be a play or a Wikipedia article\nBut for the purposes of tf-idf, documents can be\nanything; we often call each paragraph a document!\n---\n defined either by Eq. 6.11 or by Eq. 6.12) with id\n            Final tf-idf weighted value for a word\n                            wt , d           = tft , d \u21e5 idft\n           Raw counts:                           6.3            \u2022    W ORDS AND VECTORS  7\n            As You Like It   Twelfth Night    Julius Caesar    Henry V\n f-idf weighting to the Shakespeare term-documen\n            battle          1      0                7             13\n             good           114    80               62            89\n on          fool           36     58               1             4\n    Eq.                 6.12. Note that the tf-idf values for the\n             wit            20     15               2             3\n            Figure 6.2       The term-document matrix for four words in four Shakespeare plays. Each cell\n ord 6      \u2022  V            S      E\n           tf good have now all become 0; since this wor\n HAPTER     -idf: ECTOR          EMANTICS AND    MBEDDINGS\n            contains the number of times the (row) word occurs in the (column) document.\n idf algorithm leads it to be ignored. Similarly, the\n                        As You Like It  Twelfth Night           Julius Caesar  Henry V\n            represented as a count vector, a column in Fig. 6.3.\n            battle      0.246           0                       0.454          0.520\n vector     To review some basic linear algebra, a vector is, at heart, just a list or array of\n  f the 37 plays, has a much lower weight.\n            good        0               0                       0              0\n            numbers. So As You Like It is represented as the list [1,114,36,20] (the first column\n            fool        0.030           0.033                   0.0012         0.0019\n            vector in Fig. 6.3) and Julius Caesar is represented as the list [7,62,1,2] (the third\n            wit         0.085           0.081                   0.048          0.054\nr space     column vector).      A vector space is a collection of vectors, characterized by their\n            Figure 6.9       A tf-idf weighted term-document matrix for four words in four Shakespeare\n---\nVector     TF-IDF\nSemantics &\nEmbeddings\n---\nVector     Word2vec\nSemantics &\nEmbeddings\n---\nSparse versus dense vectors\n\nCount vectors (even if weighted by tf-idf)\n\u25e6  long (length |V|= 20,000 to 50,000)\n\u25e6  sparse (most elements are zero)\nAlternative: learn vectors which are\n\u25e6  short (length 50-1000)\n\u25e6  dense (most elements are non-zero)\n---\nSparse versus dense vectors\n\nWhy dense vectors?\n\u25e6   Short vectors may be easier to use as features in machine\n    learning (fewer weights to tune)\n\u25e6   Dense vectors may generalize better than explicit counts\n\u25e6   Dense vectors may do better at capturing synonymy:\n  \u25e6 car and automobile are synonyms; but are distinct dimensions\n    \u25e6  a word with car as a neighbor and a word with automobile as a\n       neighbor should be similar, but aren't\n\u25e6   In practice, they work better\n       56\n---\nCommon methods for getting short dense vectors\n\n\u201cNeural Language Model\u201d-inspired models\n\u25e6  Word2vec (skipgram, CBOW), GloVe\nSingular Value Decomposition (SVD)\n\u25e6  A special case of this is called LSA \u2013 Latent Semantic\n   Analysis\nAlternative to these \"static embeddings\":\n  \u2022     Contextual Embeddings (ELMo, BERT)\n  \u2022     Compute distinct embeddings for a word in its context\n  \u2022     Separate embeddings for each token of a word\n---\nSimple static embeddings you can download!\n\nWord2vec (Mikolov et al)\nhttps://code.google.com/archive/p/word2vec/\n\nGloVe (Pennington, Socher, Manning)\nhttp://nlp.stanford.edu/projects/glove/\n---\nWord2vec\nPopular embedding method\n\u2022     Very fast to train\n\u2022     Code available on the web\nIdea: predict rather than count\nWord2vec provides various options. We'll do:\nskip-gram with negative sampling (SGNS)\n---\n  Word2vec\nInstead of counting how often each word w occurs near \"apricot\"\n\u25e6   Train a classifier on a binary prediction task:\n  \u25e6  Is w likely to show up near \"apricot\"?\nWe don\u2019t actually care about this task\n  \u25e6  But we'll take the learned classifier weights as the word embeddings\nBig idea: self-supervision:\n  \u25e6  A word c that occurs near apricot in the corpus cats as the gold \"correct\n     answer\" for supervised learning\n  \u25e6  No need for human labels\n  \u25e6  Bengio et al. (2003); Collobert et al. (2011)\n---\nApproach: predict if candidate word c is a \"neighbor\"\n\n1.  Treat the target word t and a neighboring context word c\n    as positive examples.\n2.  Randomly sample other words in the lexicon to get\n    negative examples\n3.  Use logistic regression to train a classifier to distinguish\n    those two cases\n4.  Use the learned weights as the embeddings\n---\nSkip-Gram Training Data\n\nAssume a +/- 2 word window, given training sentence:\n\n\u2026lemon, a [tablespoon of apricot jam, a] pinch\u2026\nc1    c2 [target]      c3             c4\n---\nSkip-Gram Classifier\n\n(assuming a +/- 2 word window)\n\n\u2026lemon, a [tablespoon of apricot jam, a] pinch\u2026\n c1            c2 [target]          c3  c4\nGoal: train a classifier that is given a candidate (word, context) pair\n (apricot, jam)\n (apricot, aardvark)\n\u2026\nAnd assigns each pair a probability:\nP(+|w, c)\nP(\u2212|w, c) = 1 \u2212 P(+|w, c)\n---\nSimilarity is computed from dot product\n\nRemember: two vectors are similar if they have a high\ndot product\n\u25e6 Cosine is just a normalized dot product\nSo:\n\u25e6 Similarity(w,c) \u221d w \u00b7 c\nWe\u2019ll need to normalize to get a probability\n\u25e6 (cosine isn't a probability either)\n           64\n---\ndel   Turning dot products into probabilities\n      the probability that word  c is a real context word for target word w\n\n      Sim(    P(+|w, c) =       s (c \u00b7 w) =    1\n              w,c) \u2248 w \u00b7 c                     1 + exp (\u2212c \u00b7 w)\n                                                6.8  \u2022         W ORD 2 VEC\n      To turn this into a probability\nmoid function returns a number between 0 and 1, but to make it a prob\n del the probability that word   c is a real context word for target word w\n      We'll use the sigmoid from logistic regression:\n so need the total probability of the two possible events ( c is a context\nsn\u2019t a context word) to sum to 1. We thus estimate the probability that\neal context   P(+|w, c) =       s (c \u00b7 w) =    1\n      word for          w as:                  1 + exp (\u2212c \u00b7 w)\n\n      moid function returns a number between 0 and 1, but to make it a proba\n              P(\u2212|w, c) =         1 \u2212 P(+|w, c)\n lso need the total probability of the two possible events ( c is a context\nisn\u2019t a context word)   = s (\u2212c \u00b7 w) =         1\n                        to sum to 1. We thus estimate the probability that w\n                                               1 + exp (c \u00b7 w)\n---\n    How  P(\u2212|w, c) =                    1 \u2212 P(+|w, c)           6.8  \u2022  W ORD 2 VEC  19\n         Skip-Gram Classifier computes P(+|w, c)\n    We model the probability that word c is a real context           1\n                        = s (\u2212c \u00b7 w) =                         word for target word w as:     (6.29)\n                                                      1 + exp (c \u00b7 w)\n                     P(+|w, c) =        s (c \u00b7 w) = 1 + exp1                         (6.28)\nation 6.28 gives us                                            (\u2212c \u00b7 w)\n    This is             the probability for one word, but there are many context\n             for one context word, but we have lots of context words.\nds  The sigmoid function returns a number between 0 and 1, but to make it a probability\n    in the window. Skip-gram makes the simplifying assumption that all context\n    We'll assume independence and just multiply them:\n    we\u2019ll also need the total probability of the two possible events (c is a context word,\nds are independent, allowing us to just multiply their probabilities:\n    and c isn\u2019t a context word) to sum to 1. We thus estimate the probability that word c\n    is not a real context word for w as:\n                                                   L\n                     P(\u2212|w, c) =        1 \u2212 P(+|w, Y\n                        P(+|w, c1:L ) = c)            s (ci \u00b7 w)                              (6.30)\n                        = s (\u2212c \u00b7 w) = i=1                     1                     (6.29)\n                                                   1 + exp (c \u00b7 w)\n                                                  L\n    Equation 6.28 gives us the probability for    X\n                     log P(+|w, c                      ) one word, but there are many context\n    words in the window. Skip-gram makes   =          log s (c          \u00b7 w)                  (6.31)\n                                                  the simplifying assumption that all context\n                                        1:L                             i\n    words are independent, allowing us to just multiply their probabilities:\n                                                   i=1\n                    L\nmmary, skip-gram    Y\n                     trains a probabilistic classifier that, given a test target word\n---\nSkip-gram classifier: summary\n\nA probabilistic classifier, given\n\u2022     a test target word w\n\u2022     its context window of L words c1:L\nEstimates probability that w occurs in this window based\non similarity of w (embeddings) to c1:L (embeddings).\n\nTo compute this, we just need embeddings for all the\nwords.\n---\nThese embeddings we'll need: a set for w, a set for c\n\n1..d\naardvark  1\napricot\n\n\u2026         \u2026  W target words\n\n& =  zebra  |V|\n     aardvark  |V|+1\n     apricot\n                    C  context & noise\n     \u2026         \u2026       words\n\n     zebra     2V\n---\nVector     Word2vec\nSemantics &\nEmbeddings\n---\nVector       Word2vec: Learning the\nSemantics &  embeddings\nEmbeddings\n---\nWord2vec learns embeddings by starting with an initial set of embedding vecto\nSkip-Gram Training data\nand then iteratively shifting the embedding of each word w to be more like the em\nbeddings of words that occur nearby in texts, and less like the embeddings of word\nthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n... lemon,     a [tablespoon of apricot jam,      a] pinch ...\n           \u2026lemon, a [tablespoon of apricot jam,     a] pinch\u2026\n                c1 c1    c2  t       c3           c4\nThis example has a target word c2 [target]  c3       c4\n                             t (apricot), and 4 context words in the L = \u00b1\nwindow, resulting in 4 positive training instances (on the left below):\n positive examples +                        negative examples -\n t         c                    t           c           t        c\n apricot   tablespoon           apricot     aardvark    apricot  seven\n apricot   of                   apricot     my          apricot  forever\n                                apricot     where       apricot  dear\n apricot   jam                                                         71\n apricot   a                    apricot     coaxial     apricot  if\n---\nWord2vec learns embeddings by starting with an initial set of embedding vecto\nSkip-Gram Training data\nand then iteratively shifting the embedding of each word w to be more like the em\nbeddings of words that occur nearby in texts, and less like the embeddings of word\nthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n... lemon,     a [tablespoon of apricot jam,        a] pinch ...\n           \u2026lemon, a [tablespoon of apricot jam,       a] pinch\u2026\n                c1 c1    c2    t        c3          c4\nThis example has a target word c2 [target]    c3       c4\n                             t (apricot), and 4 context words in the L = \u00b1\nwindow, resulting in 4 positive training instances (on the left below):\n positive examples +         For              negative examples -\n t         c                       each positive\n                                   t          c           t        c\n                             example we'll grab k\n apricot   tablespoon              apricot    aardvark    apricot  seven\n                             negative examples,\n apricot   of                      apricot    my          apricot  forever\n                             sampling by frequency\n                                   apricot    where       apricot  dear\n apricot   jam                                                         72\n apricot   a                       apricot    coaxial     apricot  if\n---\n ord2vec learns embeddings by starting with an initial set of embedding vectors\n    Word2vec learns embeddings by starting with an initial set of embedding vecto\n d then Skip-Gram Training data\n         iteratively shifting the embedding of each word w to be more like the em-\n    and then iteratively shifting the embedding of each word w to be more like the em\n ddings of words that occur nearby in texts, and less like the embeddings of words\n    beddings of words that occur nearby in texts, and less like the embeddings of word\nat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n    that don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n . lemon, a [tablespoon of apricot jam,                   a] pinch ...\n    ... lemon,          a [tablespoon of apricot jam,                     a] pinch ...\n                    \u2026lemon, a [tablespoon of apricot jam,                 a] pinch\u2026\n                c1       c1  c2     t        c3           c4\n                             c1     c2       t           c3               c4\n  This example has a target word t           c2 [target]       c3         c4\n         This example has a         (apricot), and 4 context words in the L = \u00b12\n                              target word t (apricot), and 4 context words in the L = \u00b1\nindow, resulting in 4 positive training instances (on the left below):\n    window, resulting in 4 positive training instances (on the left below):\n          positive examples +\n    positive examples +                                        negative examples -\n    t     tc        c                            negative examples -\n                                        t        t  c          c     t      t  c     c\n                                        apricotapricot         aardvark     apricot  seven\n          apricot   tablespoon                      aardvark         apricot seven\n    apricot tablespoon                  apricotapricot         my           apricot  forever\n          apricot   of                              my               apricot forever\n    apricot of                                   apricot       where        apricot  dear\n          apricot   jam                 apricot     where            apricot dear    73\n    apricot jam                                  apricot       coaxial      apricot  if\n          apricot   a                   apricot     coaxial          apricot if\n    apricot a\n---\n        Word2vec: how to learn vectors\n\nGiven the set of positive and negative training instances,\nand an initial set of embedding vectors\nThe goal of learning is to adjust those word vectors such\nthat we:\n \u25e6  Maximize the similarity of the target word, context word pairs\n    (w , c\u209a\u2092\u209b) drawn from the positive data\n \u25e6  Minimize the similarity of the (w , cneg) pairs drawn from the\n    negative data.\n\n    8/24/25                                74\n---\n \u2022 Minimize the similarity of the (w, cneg ) pairs from the negative examples.\n If we consider one word/context pair (w, c  ) with its k noise words c  ...c  ,\nLoss function for one w with c               , c                  ...c\n                   pos                       pos  neg1               neg1     negk\n we can express these two goals as the following loss function L         negk\n                                                                  to be minimized\n (hence the \u2212); here the first term expresses that we want the classifier to assign the\nMaximize the similarity of the target with the actual context words,\n real context word cpos a high probability of being a neighbor, and the second term\nand minimize the similarity of the target with the k negative sampled\n expresses that we want to assign each of the noise words cnegi a high probability of\nnon-neighbor words.\n being a non-neighbor, all multiplied because we assume independence:\n                   [    k                         ]\n   LCE  = \u2212 log    P(+|w, cpos ) \u220f P(\u2212|w, cnegi )\n        [               i=1 k                          ]\n        = \u2212 log P(+|w, cpos ) + \u2211 log P(\u2212|w, cnegi )\n        [                    i=1                                  ]\n                             k\n        = \u2212 log P(+|w, cpos ) + \u2211 log \u23081 \u2212 P(+|w, cnegi )\u2309\n        [               ki=1                           ]\n        = \u2212 log s (cpos \u00b7 w) + \u2211 log s (\u2212cnegi \u00b7 w)                        (6.34)\n                        i=1\n---\nLearning the classifier\n\nHow to learn?\n\u25e6  Stochastic gradient descent!\n\nWe\u2019ll adjust the word weights to\n\u25e6  make the positive pairs more likely\n\u25e6  and the negative pairs less likely,\n\u25e6  over the entire training set.\n---\nIntuition of one step of gradient descent\n\naardvark\n                    move apricot and jam closer,\napricot              w  increasing c\u209a\u2092\u209b z w\nW\n\n                              \u201c\u2026apricot jam\u2026\u201d\n!           zebra\n         aardvark             move apricot and matrix apart\n\n   jam               c\u209a\u2092\u209b     decreasing cneg1 z w\n   C k=2 matrix      cneg1    move apricot and Tolstoy apart\n\n          Tolstoy    cneg2    decreasing cneg2 z w\n            zebra\n---\n   Reminder: gradient descent\n\n   \u2022 At each step\n     \u2022      Direction: We move in the reverse direction from the\nISTIC R     gradient of the loss function\n     EGRESSION\n     \u2022      Magnitude: we move the value of this gradient\n            ! ! (# $ ; & , ( ) weighted by a learning rate \u03b7\n\n     \u2022      !\"\n            Higher learning rate means move w faster\n\n              wt +1 = wt \u2212 h d L( f (x; w), y)\n                             dw\n---\n    pos                                         k\nesses that we want to assign each of        \u2211       \u2308    \u2309\n    = \u2212 log P(+|w, c                        the noise words c  a high probability o\n                      ) +                           log 1 \u2212 P(+|w, c  )\n    The derivatives of the loss                                neg\n           pos                                           function\ng a non-neighbor, all                                          i  negi\n \u2022  V  S                    multiplied because we assume independence:\n       ECTOR EMANTICS AND E MBEDDINGS\n             [            [                 ki=1         ]     ]\n   k\n   \u220f\n       L  = \u2212 log         P(+|w, c         )\u2211 P(\u2212|w, c   )\n          = \u2212 log s (c         \u00b7 w) +           log s (\u2212c  \u00b7 w)        (6.34\n       CE                      pos  pos                  neg\nof as an exercise at the end of the chapter):              neg\n                                                           i  i\n                                           i=1\n             [                             i=1 k                   ]\ns, we want to maximize the dot                  \u2211\n             \u2202 LCE                  product of the word with the actual contex\n             = \u2212 log P(+|w, c              ) +      log P(\u2212|w, c   )\n , and minimize the = [s (c         \u00b7 w) \u2212 1]w\n                       dot products of the word with the k negative sampled non\nbor words.   \u2202 cpos [          pos  pos         i=1               negi    ]\n             \u2202 L                                k\n e minimize                                     \u2211\n                this loss function using stochastic gradient descent.      Fig. 6.1\n                = CE  = [s (cneg \u00b7 w)]w                 \u2308                 \u2309\ns the        \u2202 c    \u2212 log P(+|w, c         ) +         log 1 \u2212 P(+|w, c   )\n          intuition of one step of learning.\n                neg                 pos                                   negi\n                       [                        i=1          k     ]\n             \u2202 L                         k                   X\n                 CE    = [s (c      \u00b7 w) \u2211\n             aardvark          pos         \u2212 1]cpos +             [s (cneg \u00b7 w)]cneg\n                = \u2212 log s (c        \u00b7 w) +       log s (\u2212c        \u00b7 w)\n                \u2202 w            pos  move apricot and neg                   i    i    (6.3\n                                                                  jam closer,\n                                            i=1              i=1  i\n                apricot        w            increasing c          z w\n---\n               \u2202 w   = [s (cpos \u00b7 w) \u2212 1]cpos +  [s (cnegi \u00b7 w)]cnegi\n    Update equation in SGD                       i=1\n he update equations going from time step t to t + 1 in stochastic gradient de\nre thus:\n    Start with randomly initialized C and W matrices, then incrementally do updates\n\n        ct +1  = ct      \u2212 h [s (ct  \u00b7 wt ) \u2212 1]wt\n        pos       pos    pos\n        ct +1  = ct      \u2212 h [s (ct  \u00b7 wt )]wt\n        neg       neg    \"  neg                     k                              #\n\n        wt +1  = wt \u2212 h  [s (cpos \u00b7 wt ) \u2212 1]cpos + X[s (cnegi \u00b7 wt )]cnegi\n                                                  i=1\nust as in logistic regression, then, the learning algorithm starts with randoml\n alized W and C matrices, and then walks through the training corpus using gra\nescent to move W and C so as to maximize the objective in Eq. 6.34 by makin\n---\nTwo sets of embeddings\n\nSGNS learns two sets of embeddings\nTarget embeddings matrix W\nContext embedding matrix C\nIt's common to just add them together,\nrepresenting word i as the vector wi + ci\n---\n Summary: How to learn word2vec (skip-gram)\n embeddings\nStart with V random d-dimensional vectors as initial\nembeddings\nTrain a classifier based on embedding similarity\n  \u25e6 Take a corpus and take pairs of words that co-occur as positive\n  examples\n  \u25e6 Take pairs of words that don't co-occur as negative examples\n  \u25e6 Train the classifier to distinguish these by slowly adjusting all\n  the embeddings to improve the classifier performance\n  \u25e6 Throw away the classifier code and keep the embeddings.\n---\nVector       Word2vec: Learning the\nSemantics &  embeddings\nEmbeddings\n---\nVector     Properties of Embeddings\nSemantics &\nEmbeddings\n---\nThe kinds of neighbors depend on window size\n\nSmall windows (C= +/- 2) : nearest words are syntactically\nsimilar words in same taxonomy\n\u25e6Hogwarts nearest neighbors are other fictional schools\n\u25e6Sunnydale, Evernight, Blandings\nLarge windows (C= +/- 5) : nearest words are related\nwords in same semantic field\n\u25e6Hogwarts nearest neighbors are Harry Potter world:\n\u25e6Dumbledore, half-blood, Malfoy\n---\nability to capture relational meanings. In an important early vector space model of\n   Analogical relations\ncognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model\nfor solving simple analogy problems of the form a is to b as a* is to what?. In such\n   The classic parallelogram model of analogical reasoning\nproblems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as\ngrape is to  , and must fill in the word vine. In the parallelogram model, illus-\n   (Rumelhart and Abrahamson 1973)    #  \u00bb  # \u00bb\ntrated in Fig. 6.15, the vector from the word apple to the word tree (= apple \u2212 tree)\n   To solve: \"apple         #  \u00bb\nis added to the vector for  is to tree as grape is to _____\"\n                            grape (grape); the nearest word to that point is returned.\n   Add tree \u2013 apple to grape to get vine\n                                      tree\n             apple\n\n   vine\n   grape\n---\n llelogram method received more modern attention because of\n       Analogical relations via parallelogram\n rd2vec or GloVe vectors ( Mikolov et al. 2013b, Levy and Gold\n gton et al. 2014). For example, the result       #\n                 # of the expression (kin\n       The parallelogram                        \u00bb  #  \u00bb  #  \u00bb\n                 #           method can solve analogies with\n is a vector close to        \u00bb\n                 #  \u00bb  queen. Similarly, Paris \u2212 France + Italy)\n       both sparse and dense embeddings (Turney and\n hat is close to Rome. The embedding model thus seems to be ex\n       Littman 2005, Mikolov et al. 2013b)\n tions of relations like MALE - FEMALE , or CAPITAL - CITY- OF, or\n       /    king \u2013 man + woman is close to queen\nIVE SUPERLATIVE, as shown in Fig. 6.16 from GloVe.\nr a         Paris \u2013 France + Italy is close to Rome\n       a:b::a*:b* problem, meaning the algorithm is given a, b, and\n , the parallelogram method is thus:\n            For a problem a:a*::b:b*, the parallelogram method is:\n                 \u02c6 \u2217                     \u2217\n                 b      = argmax distance(x, a  \u2212 a + b)\n                        x\n---\nStructure in GloVE Embedding space\n0.5                               heiress\n\n0.4\n            niece                        countess\n     0.3    aunt                         duchess\n            $ister\n     0.2                                 empress\n\n     0.1                          madam\n\n0           nephew    heir\n\n-0.1        uncle     ;woman             ue earl,\n                                         queen\n-0.2         brother                     /duke\n\n-0.3                                     emperor\n\n-0.4                    man     sir       king\n\n-0.5\n\n            -0.5  -0.4  -0.3   0.2  -0.1  0  0.1  0.2  0.3  0.4  0.5\n---\nCaveats with the parallelogram method\n\nIt only seems to work for frequent words, small\ndistances and certain relations (relating countries to\ncapitals, or parts of speech), but not others. (Linzen\n2016, Gladkova et al. 2016, Ethayarajh et al. 2019a)\n\nUnderstanding analogy is an open area of research\n(Peterson et al. 2020)\n---\nEmbeddings as a window onto historical semantics\n\nTrain embeddings on different decades of historical text to see meanings shift\n\n                      ~30 million books, 1850-1990, Google Books data\na daft gay (1900s)                 b spread                          C  solemn\nflaunting       sweet                                                   awful (1850s)\ntasteful              cheerful       broadcast (1850s) soW              awe majestic\n                       pleasant                seed                     dread       pensive\n            frolicsom\u2091             circulated                       SOWS         gloomy\n              witty gay (1950s)                  scatter\n                bright               broadcast (1900s)                      horrible\ngays        bisexual                 newspapers                             appalling terrible\n                homosexual           television                             awful (1900s)     wonderful\n gay (1990s)                         radio                                       awful (1990s)\n     lesbian                       bbcbroadcast (1990s)                          aulweird\n\n            William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic Word Embeddings Reveal\n            Statistical Laws of Semantic Change. Proceedings of ACL.\n---\nEmbeddings reflect cultural bias!\n\n   Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. \"Man is to computer\n   programmer as woman is to homemaker? debiasing word embeddings.\" In NeurIPS, pp. 4349-4357. 2016.\n\n Ask \u201cParis : France :: Tokyo : x\u201d\n \u25e6 x = Japan\n Ask \u201cfather : doctor :: mother : x\u201d\n \u25e6 x = nurse\n Ask \u201cman : computer programmer :: woman : x\u201d\n \u25e6 x = homemaker\nAlgorithms that use embeddings as part of e.g., hiring searches for\nprogrammers, might lead to bias in hiring\n---\nHistorical embedding as a tool to study cultural biases\n             Garg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes.\n             Proceedings of the National Academy of Sciences 115(16), E3635\u2013E3644.\n\n\u2022     Compute a gender or ethnic bias for each adjective: e.g., how\n      much closer the adjective is to \"woman\" synonyms than\n      \"man\" synonyms, or names of particular ethnicities\n      \u2022     Embeddings for competence adjective (smart, wise,\n            brilliant, resourceful, thoughtful, logical) are biased toward\n            men, a bias slowly decreasing 1960-1990\n      \u2022     Embeddings for dehumanizing adjectives (barbaric,\n            monstrous, bizarre) were biased toward Asians in the\n            1930s, bias decreasing over the 20\u1d57\u02b0 century.\n\u2022     These match the results of old surveys done in the 1930s\n---\nVector     Properties of Embeddings\nSemantics &\nEmbeddings\n\n"
    }
}