{
    "data/raw/Week8 - LGT-slides.pdf": {
        "metadata": {
            "file_name": "Week8 - LGT-slides.pdf",
            "file_type": "pdf",
            "content_length": 54955,
            "language": "en",
            "extraction_timestamp": "2025-11-26T00:14:05.797478+00:00",
            "timezone": "utc"
        },
        "content": "              77    KING'S\n                    College\n                    O    LONDON\n\nLarge Language\nmodels -\ntraining\n\n Week 8 - LGT    BVSH HOVSE\n\nG  Dr Lin Gui\n   Lin.1.gui@kcl.ac.uk\n---\nLearning outcomes\n\n\u25cf  By the end of this topic, you will be about to:\n\n   \u25cf     Describe the principles of supervised fine-tuning and reinforcement learning\n\n   \u25cf     Describe the difference between PPO, DPO, and GRPO.\n\n2\n---\nBefore we start..\n\n \u25cf     More instances (formatting does matter!)                   What format?\n\n \u25cf     Example: sentiment analysis, which aims to predict the sentiment label\n       (positive/negative) for the give sentence\n Give some instances\n\n      love this phone - battery lasts all day; \ud83d\ude0a\n      The update ruined everything. Apps keep crashing.\n      \u2026\u2026\n      Yeah, fantastic job\u2026 three delays in a row. \ud83d\ude44              What does the format mean?\n\n                    FEW-SHOT EXAMPLES\n\n        \"role\": \"user\"\n        \"content\": \"TExT: I love this phone-battery lasts all day!\n\n        \"role\": \"assistant\"\n        \"content\": \"{\\\"label\\\":\\\"positive\\\",\\\"confidence\\\":0.93,\\\"evidence\\\":\\\"love this phone; lasts all day; \\\"}\"    3\n---\n   Before we start..\n\n   \u25cf     There is no specific formatting in prompt designing\n\n   \u25cf     Language is unstructured data, we need to use specific symbols to make the\n         input to be structured, which be easily understood by LLM.\n\n   \u25cf     This learning ability might come from the training of coding task\n\n   \u25cf     It doesn\u2019t have to be a real format like JSON or HTML, define whatever your like\n         and just ensure that it looks structured.\n\n   \u25cf     Another possible reason is that the symbols, especially the symbols frequently\n         used in programming data, is able to draw a higher attention scores in the model\n         and help the model to find the latent patterns.\n\nhttps://colab.research.google.com/drive/1P2z4IGhOso8sYesOgvHskFHHJf8IqpqX?usp=sharing    4\n---\nBefore we start..\n\n\u25cf     What is supervised learning?\n\n       \u25cf     A machine learning method where a model is trained on a labelled dataset\n\n       \u25cf     Three main components:\n\n              \u25cf     Loss function: the learning target\n\n              \u25cf     Optimiser: how to find the target\n\n              \u25cf     Labelled data: where to train the model based on loss function and\n                    optimiser.\n\n\u25cf     Do you think the LLM training is a supervised learning task?\n\n\u25cf     Why or why not?\n\n                                                                                      5\n---\nBefore we start..\n\n\u25cf  The main challenges in LLM training:\n\n   \u25cf     Loss function: how to define the loss function? Since the generation of\n         language is not a simple prediction task. How to define the label? Frequency?\n         Or Correctness? Or some other metric?\n\n   \u25cf     Optimiser: even the loss cannot be clearly defined, how to optimise?\n\n   \u25cf     Labelled data: it seems impossible to label all the knowledge created by\n         human beings manually.\n\n\u25cf     The training of LLM is a systematic task, there is no simple solution.\n\n\u25cf     Do you remember the magic prompt we used in the last week?\n\n\u25cf     Let\u2019s do it step-by-step                                              6\n---\nLLM Training, a step-by-step to do list\n\n\u25cf     Step#1 \u2013 Pre-training (Speak like human)\n\n\u25cf     Step#2 \u2013 Instruction fine-tuning (Understand the instruction)\n\n\u25cf     Step#3 \u2013 Learning from human feedback (Learning from interaction)\n                                                                   r HAB6N\n                                                                 Iho Sic         0249\n                                                                Blos\n                                                                   igken Wirtorba\n                                                                     \"?          su\n                                                                   E\n\n7\n---\nLLM Training, a step-by-step to do list\n\n\u25cf     Step#1 \u2013 Pre-training (Speak like human)\n\n\u25cf     Step#2 \u2013 Instruction fine-tuning (Understand the instruction)\n\n\u25cf     Step#3 \u2013 Learning from human feedback (Learning from interaction)\n                                                                   r HAB6N\n                                                                 Iho Sic         0249\n                                                                Blos\n                                                                   igken Wirtorba\n                                                                     \"?          su\n                                                                   E\n\n8\n---\nPre-training\n\n\u25cf     What is pre-training?\n\n\u25cf     The goal is to train the model to predict the next token for the give text.\n\n\u25cf     It literally simplify the three challenges:\n\n       \u25cf     Loss function: easy to define, based on the predictive probability.\n\n       \u25cf     Optimiser: just directly optimise the cross-entropy loss\n\n       \u25cf     Labelled data: plain text, no need of annotation!!\n\n\u25cf  But\u2026How?\n\n9\n---\nPre-training\n\n\u25cf     The goal is to predict the next word. Suppose we have a sentence:\n\n                 May the force be with you\n\n\u25cf     We feed the words into a LLM to predict the next word. We hope the prediction\n      is correct:\n\n                 May the force             LLM     be\n\n                 May the force be          LLM     with\n\n                 May the force be with     LLM     you\n\n\u25cf     We update the parameters to guide the LLM to produce the correct prediction\n\n                                                                       10\n---\nPre-training (example)\n\n\u25cf  The goal is to predict the next word. Suppose we have a sentence:\n\nGiven context    Candidates of Prediction     P(w)\n\nMay  the    force    be    with               0.51\n\n                           on                 0.21\n\n                           in                 0.13\n\nLoss function:\n\nWhat if the predicted word is incorrect?\n\n11\n---\nPre-training (example)\n\n\u25cf  The goal is to predict the next word. Suppose we have a sentence:\n\nGiven context    Candidates of Prediction     P(w)\n\nMay  the    force    be    with               0.21\n\n                           on                 0.51\n\n                           in                 0.13\n\nLoss function:\n\nThe loss becomes small or large?\n\n12\n---\nPre-training (example)\n\n\u25cf  The goal is to predict the next word. Suppose we have a sentence:\n\n          Given context    Candidates of Prediction            P(w)\n\nMay       the    force     be                    with          0.21\n\n                                                  on           0.51\n\n     7           Graph of -log(x)                 on           0.13\n     6\n     5\n     4                                           If the correct prediction has higher\n     3                                           probability, the loss will be smaller,\n     2                                           which means there are less updates in\n     1                                           the parameters\n     0\n          0.0  0.2  0.4 X 0.6        0.8  1.0                                          13\n---\nPre-training (example)\n\n\u25cf  What if the prediction in previous round is incorrect?\n\nMay the force            LLM     be\n\nMay the force be         LLM     on\n\nMay the force be ???     LLM     ???\n\n14\n---\nPre-training (example)\n\n\u25cf  What if the prediction in previous round is incorrect?\n\n   May the force             LLM     be\n\n   May the force be          LLM     on\n\n   May the force be with     LLM     ???\n\n\u25cf     Ignore the incorrect prediction and move to the next round by using the correct\n      word.\n\n15\n---\nPre-training \u2013 Discussion\n\n\u25cf  What is the strength of Pre-training?\n\n   \u25cf     Loss function & optimizer: simply and easy to implement\n\n   \u25cf     Dataset:\n\n          \u25cf     we do not need any annotated data.\n\n          \u25cf     Plain text can be used for pretraining.\n\n          \u25cf     We can scan all the text created by human in the past \u2013 if we want to do\n                so\n\n   \u25cf     But\u2026 what is the weakness?\n\n16\n---\nPre-training \u2013 Discussion\n\n\u25cf     A classic trade-off in machine learning: model complexity and generalizability\n\n\u25cf     Complex neural network is able to approximate complex function\n\n\u25cf     Complexity of neural networks is related to:\n\n      \u25cf  Depth of a neural net \u2191, number of parameters \u2191, number of labels \u2191\n\n\u25cf     LLM is complex!\n\n\u25cf     The potential risk of a complex learning structure:\n\n       \u25cf     Overfitting\n\n       \u25cf     Memorization rather than Learning\n\n\u25cf     Q: how do we know the Pre-trained LMs truly understand the pattern in the training\n      data, or it just memorise the instances what has been learned?\n\n\u25cf  Emm, this concept maybe a bit confusing\u2026let\u2019s move to an example.    17\n---\nExample: demonstration of a neural network\n\n \u25cf  https://playground.tensorflow.org/\n\n One more question: Solving a task == Understanding of a task?\n\n 18\n---\nExample: Human vs. Neural Networks\n\n \u25cf     You will be shown an image of a character from either the Pok\u00e9mon or Digimon\n       series. Your goal is to predict the correct label: Pok\u00e9mon or Digimon.\n\n Vsr\n PoKeMON  BIGITAN\n          DIGIMON    19\n          MONSTERS\n---\n    Example: Human vs. Neural Networks\n\n    \u25cf     Dataset:\n\n    \u25cf     Pok\u00e9mon images: https://www.Kaggle.com/kvpratama/pokemon-images-dataset/data\n\n    \u25cf     Digimon images: https://github.com/DeathReaper0965/Digimon-Generator-GAN\n\n    Pok\u00e9mon    Digimon\n\nTesting\nImages:\n       20\n---\nExample: Human vs. Neural Networks\n\n\u25cf     Model: 6-layer CNN + 1 linear layer classification\n                   model = Sequential()\n                   model.add(Conv2D(32, (3, 3), padding='same', input_shape=(120,120,3)))\n                   model.add(Activation('relu'))\n                   model.add(Conv2D(32, (3, 3)))\n                   model.add(Activation('relu'))\n                   model.add(MaxPooling2D(pool_size=(2, 2)))\n                   model.add(Conv2D(64,(3, 3), padding='same'))\n                   model.add(Activation('relu'))\n                   mode1.add(Conv2D(64,(3,3)))\n                   model.add(Activation('relu'))\n                   model.add(MaxPooling2D(pool_size=(2,2)))\n                   model.add(Conv2D(256, (3, 3), padding='same'))\n                   model.add(Activation('relu'))\n                   model.add(Conv2D(256, (3, 3)))\n                   model.add(Activation('relu'))\n                   model.add(MaxPooling2D(pool_size=(2, 2)))\n                   model.add(Flatten())\n                   model.add(Dense(1024))\n                   model.add(Activation('relu'))\n                   model.add(Dense(2))\n      Training     model.add(Activation('softmax'))\n\u25cf                  accuracy: 98.9% (using training data to test the performance)\n\n\u25cf     Testing accuracy: 98.4%\n---\n Example: Human vs. Neural Networks\n\n  \u25cf  Model: 6-layer CNN + 1 linear layer classification\n\n0  0\n\n 20  20  2I  20\n\n 4I  40  4I  4I\n\n E  6  6  6\n\n 8  8  8  81\n\n10D  10D  18D  10D\n\n2 41 E8 1D  2 4I  64 a 10D  2 4I E 8 10D  2 41 E a 10D\n\n0  0\n\n2  20  2I  20\n\n4I  4I  4I  4I\n\n621  6  64  6\n\n8  8\n\n18D  10D  10D  10D\n\n2 4I  E a 1D  2 41  64 a 10D  2 40  6 8 10D  2 40 64 8 10D\n---\n Example: Human vs. Neural Networks\n\n  \u25cf  Model: 6-layer CNN + 1 linear layer classification\n\nI  0  0\n\n 20  20  20  20\n\n 4I  4I  4I  4I\n\nE  E  6\n\n8  8  88  87\n\n10D  10D  10D  10D\n\n2 4I 6 8 10D  2 41 F 8 10D  2 41 6 81 10D  2 4I F a 10D\n\nI\n\n 20  20  20  20\n\n 4l  4I  40  4I\n\n E  E  6  6\n\n8  81  8  8\n\n10D  10D  10D  10D\n\n2 41 E a 1D  2 4I 6 81 10D  2 4I  62 8 10D  2 41 6 a 1D\n---\nExample: Human vs. Neural Networks\n\n\u25cf     Model: 6-layer CNN + 1 linear layer classification\n\n\u25cf     All the images of Pok\u00e9mon are PNG, while most images of Digimon are JPEG.\n\n\u25cf     Machine discriminates Pok\u00e9mon and Digimon based on the background colours.\n\nloading the files\n\npng files have transparent     transparent background\nbackground                     becomes black\n---\nExample: Human v.s. Neural Networks\n\n \u25cf  PASCAL VOC 2007 data set\n\n This slide is from: GCPR 2017 Tutorial \u2014 W. Samek & K.-R. M\u00fcller\n\n \u25cf  What\u2019s the connection to LLMs?\n---\nLLM Training, a step-by-step to do list\n\n\u25cf     Step#1 \u2013 Pre-training (Speak like human)\n\n\u25cf     Step#2 \u2013 Instruction fine-tuning (Understand the instruction)\n\n\u25cf     Step#3 \u2013 Learning from human feedback (Learning from interaction)\n                                                                   r HAB6N\n                                                                 Iho Sic         0249\n                                                                Blos\n                                                                   igken Wirtorba\n                                                                     \"?          su\n                                                                   E\n\n26\n---\nBefore we start..\n\n\u25cf     What we have now?\n\n       \u25cf     A sequence model which is able to predict the next word\n\n\u25cf     The unsolved issue\n\n       \u25cf     It might not understand the language\n\n       \u25cf     Just simply memorise the pattern in human language\n\n       \u25cf     Predict the most likely next word\n\n27\n---\nInstruction fine-tuning\n\n\u25cf     What is instruction fine-tuning?\n\n       \u25cf     Teach a model to follow human-like instructions.\n\n\u25cf     But\u2026how?\n\n       \u25cf     Collect the data with human instruction\n\n       \u25cf     Initialise the model with pre-trained parameter\n\n       \u25cf     Fine-tune the parameters with the instructions\n\n28\n---\nInstruction fine-tuning (example)\n\n\u25cf  Pre-training: train the model to predict the next word iteratively\n\nMay the force             LLM     be\n\nMay the force be          LLM     with\n\nMay the force be with     LLM     you\n\n29\n---\nInstruction fine-tuning (example)\n\n\u25cf     Instruction fine-tuning: add the instruction as a prompt, only fine-tune on the\n      feedback text\n      Pre-trained                Initialising\n         model\n\nWhat\u2019s the iconic expression from the star wars movie?             LLM     May\n\nWhat\u2019s the iconic expression from the star wars movie? May         LLM     the\n\nWhat\u2019s the iconic expression from the star wars movie? May the     LLM     force\n\nInstruction\n\n30\n---\nInstruction fine-tuning (strength)\n\n\u25cf     With the instruction fine-tuning, the model can understand the task.\n\n\u25cf     Evidence:\n\n       \u25cf     Pretrain the model on multiple language, but only fine-tuning on a Chinese\n             task.\n\n       \u25cf     But the model is able to do this task on English.\n\n             Model  Pre-train      Fine-tune            Testing     EM       F1\n             QANet  none           Chinese QA                       66.1     78.1\n                    Chinese        Chinese QA                       82.0     89.1\n                                   Chinese QA           Chinese     81.2     88.7\n             BERT   104 languages  English QA           QA          63.3     78.8\n\n                                   Chinese + English                82.6     90.1\n                                   F1 score of Human performance is 93.30%       31\n---\nTwo possible path to AI\n\n\u25cf     #1 Pre-train base model, and develop different task focused model with\n      instruction fine-tuning       SFT on Task#1\n\n      Raw corpus    Pre-trained     SFT on Task#2\n                    model\n\n                                    SFT on Task#3\n\n\u25cf     #2 Pre-train first, and keep instruction fine-tuning on the same base model\n\nRaw corpus  Pre-trained  SFT on Task#1  SFT on Task#2  SFT on Task#3\n            model\n\n                                                                    32\n---\nExample\n\n  \u25cf     #1 Pre-train base model, and develop different task focused model with\n        instruction fine-tuning    SFT on translation\n\n  Raw corpus    Pre-trained    SFT on summarisation\n                model\n\n                               SFT on Math\n\n  New task    Identify the type of task\n\n  33\n---\n  Example\n\n    \u25cf  #2 Pre-train first, and keep instruction fine-tuning on the same base model\n\n       Raw corpus\n\n    Pre-trained model     New task\n\n                                  No need to consider\n\n    SFT on translation            the type of task.\n                                  Because the model is\n                                  able to solve all the\n    SFT on                        tasks has been learned\nsummarisation\n\n    SFT on Math\n\n    34\n---\nAGI is more complex than you think\n\n\u25cf  Possible issues\n\n   \u25cf     Catastrophic Forgetting - The model\u2019s learning on one task can overwrite or\n         interfere with what it learned on another.\n\n   \u25cf     Instruction Ambiguity - If some tasks expect \u201cshort bullet points\u201d and others\n         \u201clong-form answers,\u201d the model might produce inconsistent outputs.\n\n   \u25cf     Data Imbalance Across Tasks - The model overfits to tasks with more data,\n         ignoring smaller tasks.\n\n   \u25cf     Domain Shift or Vocabulary Clash - Tasks from different domains (medical text\n         vs. casual dialogue) can confuse the model.\n\n   \u25cf     \u2026\u2026\n                                                                           35\n---\n  AGI is more complex than you think\n\n   \u25cf  Most training details are not released to public\n\n        Natural lanquage inference  Commonsense   Sentiment        Paraphrase    Closed-book QA        Struct to text              Translation\n               (7 datasets)         (4 datasets)  (4 datasets)     (4 datasets)  (3 datasets)                    (4 datasets)      (8 datasets)\n   ANLI (R1-R3)  RTE                CoPA              IMDB              MRPC     ARC (easy/chal.)         CommonGen              ParaCrawl EN/DE\n        CB    SNLI                  HellaSwag     Sent140               QQP      NQ                              DART            ParaCrawl EN/ES\n       MNLI   WNLI                  PiQA             SST-2              PAWS     TQA                             E2ENLG          ParaCrawl EN/FR\n       QNLI                         StoryCloze        Yelp              STS-B                                    WEBNLG            WMT-16 EN/CS\n                                                                                                                                 WMT-16 EN/DE\n        Reading comp.    Read. comp. w/     Coreference        Misc.                          Summarization                        WMT-16 EN/FI\n         (5 datasets)     commonsense       (3 datasets)     (7 datasets)                     (11 datasets)                        WMT-16 EN/RO\n   BoolQ      OBQA        (2 datasets)            DPR     CoQA     TREC          AESLC        Multi-News         SamSum\n    DROP      SQuAD         CosmosQA        Winogrande    QuAC     CoLA          AG News      Newsroom           Wiki Lingua EN    WMT-16 EN/RU\n   MultiRC                   ReCoRD         WSC273        WIC      Math          CNN-DM       Opin-Abs: iDebate  XSum            WMT-16 EN/TR\n                                                          Fix Punctuation (NLG)  Gigaword     Opin-Abs: Movie\n\n   Figure 3: Datasets and task clusters used in this paper (NLU tasks in blue; NLG tasks in teal).\n   In the 2023 research paper, we can see detailed information about the data they used, but the latest technical\n   report provides very little discussion of those details.\n\nhttps://arxiv.org/pdf/2109.01652                                                                                                                36\n---\n  AGI is more complex than you think\n\n   \u25cf  More dataset/tasks, the better over all performance. Even on unseen tasks.\n\n      Finetune on many tasks (\"instruction-tuning\")\n      Input (Commonsense Reasoning)           Input (Translation)             Inference on unseen task type\n      Here is a goal: Get a cool sleep on     Translate this sentence to\n      summer days.                            Spanish:                         Input (Natural Language Inference)\n      How would you accomplish this goal?     The new office building          Premise: At my age you will probably\n      OPTIONS:                                was built in less than three     have learnt one lesson.\n      -Keep stack of pillow cases in fridge.  months.                           Hypothesis: It's not certain how many\n      -Keep stack of pillow cases in oven.    Target                            lessons you'll learn by your thirties.\n      Target                                  El nuevo edificio de oficinas     Does the premise entail the hypothesis?\n      keep stack of pillow cases in fridge     se construy\u00f3 en tres meses.      OPTIONS:\n                      Sentiment analysis tasks                                 -yes -it is not possible to tell  -no\n                      Coreference resolution tasks                                      FLAN Response\n                                                                                        It is not possible to tell\n\n                                    GPT-3 175B zero shot  GPT-3 175B few-shot  FLAN 137B zero-shot\n\n   Performance                      53.2 56.2             63.7 72.6 77.4       49.8 55.7 56.6\n   on unseen                        42.9\n   task types\n\n                                    Natural language inference  Reading Comprehension  Closed-Book QA\n\nhttps://arxiv.org/pdf/2109.01652                                                                     37\n---\n   AGI is more complex than you think\n\n    \u25cf  More parameters, the better over all performance.\n\n    60                                       60    540B model\n    20                                       20\n    20 40                                    20 40    62B model\n          20    -1,836 tasks                   20     8B model\n                                    282 tasks\n                                    89 tasks\n                                    9 tasks\n          0     No finetuning                  0\n                8B  62B                    540B    0 9  89  282 682 1,836\n                Model size (# parameters)             Number of finetuning tasks\n\nhttps://arxiv.org/abs/2210.11416                                                38\n---\n  AGI is more complex than you think\n\n   \u25cf     Data quality is important.\n\n   \u25cf     LLaMA2 from meta:\n\n               Quality Is All You Need. Third-party SFT data is available from many different sources, but we found that\n          many of these have insufficient diversity and quality \u2014 in particular for aligning LLMs towards dialogue-style\n          instructions. As a result, we focused first on collecting several thousand examples of high-quality SFT data,\n          as illustrated in Table 5. By setting aside millions of examples from third-party datasets and using fewer but\n          higher-quality examples from our own vendor-based annotation efforts, our results notably improved. These\n          findings are similar in spirit to Zhou et al. (2023), which also finds that a limited set of clean instruction-tuning\n              data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of\n          thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of\n          27,540 annotations. Note that we do not include any Meta user data.\n\n   \u25cf  LIMA: Less Is More for Alignment\n\n      \u25cf  1k training examples - \u201cresponses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases\u201d\n\nhttps://arxiv.org/abs/2307.09288\nhttps://arxiv.org/abs/2305.11206      39\n---\nQuestion: where to find the data?\n\n \u25cf  Manually annotation is expensive.\n\n ASK\n CHATGPT\n NEED    FOR DATA\n MORE\n DATA\n         NEED\n         MORE\n         DATA\n\n \u25cf  Solution: Maybe we can ask ChatGPT!\n\n                                       40\n---\n   Self-instruct\n\n175 seed tasks with    Task Pool    Step 1: Instruction Generation    Step 2: Classification\n 1 instruction and                                                    Task Identification\n1 instance per task                 Task\n                                    LM  Instruction : Give me a quote from a    LM\n                                        famous person on this topic.\n\n                           Step 3: Instance Generation\n                           Task                                                                              Yes\n\n    Step 4: Filtering     Instruction : Find out if the given text is in favor of or against abortion.\n                          Class Label: Pro-abortion\n                          Input: Text: I believe that women should have the right to choose whether or not   Output-first  LM\n                          they want to have an abortion.\n\n                           Task                                                                              No\n                           Instruction : Give me a quote from a famous person on this topic.\n                           Input: Topic: The importance of being honest\n                           Output: \"Honesty is the first chapter in the book of wisdom.\" - Thomas Jefferson  Input-first\n\n https://arxiv.org/pdf/2212.10560    41\n---\nPossible legal issue: Open AI\u2019s Terms of Use\n\n \u25cf  https://openai.com/policies/terms-of-use\n\n     (c) Restrictions. You may not (i) use the Services in a way that infringes,\n     misappropriates or violates any person's rights; (ii) reverse assemble,\n    reverse compile, decompile, translate or otherwise attempt to discover the\n     source code or underlying components of models, algorithms, and\n     systems of the Services (except to the extent such restrictions are contrary\n    to applicable law); (ii) use output from the Services to develop models that\n     compete with OpenAl; (iv) except as permitted through the APl, use any\n    automated or programmatic method to extract data or output from the\n    Services, including scraping, web harvesting, or web data extraction; (v)\n    represent that output from the Services was human-generated when it is\n     not or otherwise violate our Usage Policies; (vii) buy, sell, or transfer API\n    keys without our prior consent; or (vii), send us any personal information of\n     children under 13 or the applicable age of digital consent. You will comply\n     with any rate limits and other requirements in our documentation. You may\n    use Services only in geographies currently supported by OpenAl.\n\n \u25cf  Alternative solution: open source project\n\n                                                                                  42\n---\n  Possible legal issue: Open AI\u2019s Terms of Use\n\n   \u25cf     LLaMA from Meta        Continue pre-training                                 LLaMA    Parameter-efficient fine-tuning\n                                Model inheritance        Instruction\n\n   \u25cf     An open-source LLM     Data inheritance         tuning                       chinese data    + chat data    Full parameter fine-tuning\n\n         project               Open-Chinese-LLaMA                          Chinese                    +synthetic data\n\n         Pretrained from       Linly-Chinese-LLaMA       Chinese Panda     Vicuna                     Alpaca  Alpaca  RLHF task data    Vicuna  Yulan-Chat\n   \u25cf                            + chat data              LLaMA                       BiLLa            Lora            PKU-Beaver        Goat\n\n         GPT-3 and PaLM        Cornucopia            Alpaca data                      chat data                                               synthetic dato\n                                se Lawyer\n                                LLaMA                                                 1/BELLE                                       OpenFlamingo  LLaVA  MiniGPT-4\n                                + chat data                                Baize                      Ziya            task data\n                                QiZhenGPT     Chinese                                                                                             + task data\n\n                                + task data    Alpaca                                       + task data               Guanaco\n                                             TaoLi                                          Koala             + task data      VisionLLM          InstructBLIP    Chatbridge\n\n                                                                          ChatMed                             LLaMA\n                                BenTsao               t12LAWGPT                                               Adapter               Multimodal models             PandaGPT\n\n   Math Finance  Medicine Law Bilingualism  Education\n\nhttps://arxiv.org/abs/2307.09288\nhttps://arxiv.org/abs/2302.13971    43\nhttps://arxiv.org/abs/2303.18223\n---\nLLM Training, a step-by-step to do list\n\n\u25cf     Step#1 \u2013 Pre-training (Speak like human)\n\n\u25cf     Step#2 \u2013 Instruction fine-tuning (Understand the instruction)\n\n\u25cf     Step#3 \u2013 Learning from human feedback (Learning from interaction)\n                                                                   r HAB6N\n                                                                 Iho Sic         0249\n                                                                Blos\n                                                                   igken Wirtorba\n                                                                     \"?          su\n                                                                   E\n\n44\n---\nThe main issue for instruction fine-tuning\n\n \u25cf     No enough training data\n\n \u25cf     Cost of human annotation (annotator need to write the answer)\n\n       Pre-trained            Initialising\n          model\n\n What\u2019s the iconic expression from the star wars movie?             LLM     May\n\n What\u2019s the iconic expression from the star wars movie? May         LLM     the\n\n What\u2019s the iconic expression from the star wars movie? May the     LLM     force\n\n 45\n---\nReinforcement Learning with Human Feedback (RLHF)\n\n \u25cf  We do not need the word level annotation\n\n Increase the generative probability\n\n                                    May the force be with you  HELLO!\n\n What\u2019s the iconic expression    LLM\n from the star wars movie?\n\n                                    If necessary, please learn to let it go\n\n Decrease the generative probability\n\n Annotator just needs to pick one from the generation results\n\n                                                             46\n---\nReinforcement Learning with Human Feedback (RLHF)\n\n\u25cf  Pros\n\n   \u25cf     The RLHF doesn\u2019t not require large number of labelling\n\n   \u25cf     The instruction fine-tuning focus on word level generation, but ignore the\n         overall quality. The RLHF doesn\u2019t.\n\n\u25cf  Cons\n\n   \u25cf     It might be difficult for human to identify the high-quality generation result\n         for a well-trained model.\n\n47\n---\nReward Model\n\n \u25cf     It might be difficult for human to identify the high-quality generation result for a\n       well-trained model.\n\n \u25cf     Train a reward model to simulate annotation\n\n                                May the force be with you\nWhat\u2019s the iconic expression    LLM                      Reward\nfrom the star wars movie?              If necessary, please learn to let it go  model  x\n\n\u25cf     Pros: no more human annotator needed\n\n\u25cf     Cons: ???\n\n                                          48\n---\nReward Model \u2013 cons\n\n \u25cf  It might be harmful if the model learn from Reward model\n\n                                                       Overoptimized policy\n                                                     28yo dude stubbornly postponees start pursuing\n    1.0                                               gymnastics hobby citing logistics reasons despite\n    20                     RM prediction              obvious interest??? negatively effecting long term\n       0.8                                             fitness progress both personally and academically\n                                                      thoght wise? want change this dumbass shitty ass\n       0.6                                            policy pls\n                                                      employee stubbornly postponees replacement cit-\n       0.4                                            ing personal reasons despite tried reasonable com-\n                                                       promise offer??? negatively effecting productivity\n       0.2                                             both personally and company effort thoghtwise?\n                   Actual preference                  want change this dumbass shitty ass policy at work\n          0   2  5 10  25  75                250     now pls halp\n              KL from supervised baseline            people insistently inquire about old self-harm scars\n                                                        despite tried compromise measures??? negatively\n                                                        effecting forward progress socially and academi-\n                                                       cally thoghtwise? want change this dumbass shitty\n                                                      ass behavior of mine please help pls halp          49\n---\nRLHF \u2013 more technical details\n\n \u25cf     We said that we want to increase the generative probability of human preferred\n       answer, but how?\n\n \u25cf  Some principles in RLHF:\n\n    \u25cf     If an answer got a high/low score from reward model, we should\n          increase/decrease the generative probability\n\n    \u25cf     During the updating parameters, we should not move too far from old\n          parameters (to stabilize the training)\n\n 50\n---\n   RLHF \u2013 more technical details (example)\n\n    \u25cf     We said that we want to increase the generative probability of human preferred\n          answer, but how?          Training on current version by using reward model to update the parameters\n                                                                    P=0.3                           Advantage\n                                           May the force be with you                                2.0\n\n    What\u2019s the iconic expression    LLM                                               Reward\n    from the star wars movie?                                       P=0.2             model\n\n    Current version                        If necessary, please learn to let it go                  0.8\n\n                         P=0.2             Iteratively learning\n\n      LLM\n                                 Replace the old version\n Previous version     P=0.1\nOld version for reference, we want to know if the new version\nis able to generate good answer with high probability\nUpdating the LLM in a buffer for\n         many rounds learning\n Always have two version\n in memory!\n Need a lot of GPU cards!!      51\n---\n RLHF \u2013 more technical details (example)\n\n  \u25cf  New question:\n\n  ASK     WHERE\n  CHATGPT    IS THE\n  FOR DATA    FUNDING?\n\nNEED          NEED\nMORE          MORE\n  DATA        GPUS\n\n  52\n---\n  RLHF \u2013 Train with Lora\n\n \u25cf  Who is LoRA?\n\n   LORA: LoW-RaNK AdAPtATION OF LArGE LAN-\n   GUAGE MODELS\n\n   Edward Hu  Yelong Shen  Phillip Wallis  Zeyuan Allen-Zhu\n   Yuanzhi Li  Shean Wang  Lu Wang  Weizhu Chen\n   Microsoft Corporation\n   {edwardhu, yeshe, phwallis, zeyuana,\n   yuanzhil, swang, luw, wzchen}@microsoft.com\n   yuanzhil@andrew.cmu.edu\n   (Version 2)\n\nhttps://arxiv.org/pdf/2106.09685    53\n---\nRLHF \u2013 Train with Lora\n\n\u25cf  The basic operations in neural networks:\n\n   \u25cf  Linear mapping                          Memory costs are mainly from here\n                                              Using Matrix decomposition to reduce the size!\n      \u25cf     Parameter size M x N\n\n      \u25cf     Where M is the input dimension and N is the output dimension\n\n   \u25cf  Non-linear projection\n\n      \u25cf     Parameter size\n\n      \u25cf     Depends on the projection function\n\n54\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n       Linear mapping                      Non-linear projection\n   0.1\n\n   0.6\n\n   0.5\n\n   0.7\n\n   -0.3\n\n                                                                55\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n          Linear mapping                   Non-linear projection\n\n   0.1    -0.13\n\n          -0.07                 -0.085\n   0.6    0.39\n\n          -0.18\n   0.5\n          0.33\n\n   0.7\n\n-0.3\n\n    56\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n          Linear mapping                   Non-linear projection\n\n   0.1\n          -0.24                 -0.085\n\n   0.6    -0.15\n          0.18                  0.138\n\n   0.5    0.27\n\n0.7    0.09\n\n-0.3\n\n    57\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n          Linear mapping                   Non-linear projection\n\n   0.1\n                                -0.085\n\n   0.6    0.05\n\n          0.04                  0.138\n   0.5    0.19\n\n          -0.31                 -0.153\n   0.7\n          0.20\n\n   -0.3\n\n                                                                58\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n           Linear mapping                  Non-linear projection\n\n   0.1\n                                -0.085\n\n   0.6\n           0.38                 0.138\n\n   0.5     0.23\n           -0.48                -0.153\n\n   0.7     -0.21\n                                -0. 112\n\n   -0.3    -0.33\n\n                                                                59\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf  The basic operations in neural networks:\n\n       Linear mapping                      Non-linear projection (SoftMax normalization, t=1)\n\n   0.1\n                                -0.085      0.24\n\n   0.6\n                                0.138       0.30\n\n   0.5\n                                -0.153      0.22\n\n   0.7\n                                -0. 112     0.23\n\n   -0.3\n\n                                                60\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf       The basic operations in neural networks (Matrix version):\n\n                                        A\u2081       A\u2082              A\u2083   A\u2084         A\u2085\n\n0.1                             1       0.1  0.6                 0.5  0.7    -0.3\n        -0.085                               B\u2081                       B\u2082             B\u2083        B\u2084\n\n0.6                                1         -0.13               -0.24           0.05          0.38\n        0.138                      2         -0.07               -0.15           0.04          0.23\n\n0.5                                3         0.39                    0.18        0.19      -0.48\n        -0.153                     4         -0.18                   0.27    -0.31         -0.21\n                                   5         0.33                    0.09        0.2       -0.33\n0.7     -0. 112                    1         C\u2081                  C\u2082          C\u2083          C\u2084\n\n                                        -0.085        0.138                  -0.153      -0.112\n-0.3                                                             A x B = C\n\n                                                                                                 61\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf     The basic operations in neural networks (Matrix version):\n\n                                           A\u2081       A\u2082         A\u2083   A\u2084         A\u2085      Input\n\n\u25cf     How many parameters here?    1       0.1  0.6            0.5  0.7    -0.3              Para.\n\n\u25cf     20 (4*8)                                  B\u2081                  B\u2082             B\u2083        B\u2084\n                                      1         -0.13          -0.24           0.05          0.38\n                                      2         -0.07          -0.15           0.04          0.23\n                                      3         0.39               0.18        0.19      -0.48\n                                      4         -0.18              0.27    -0.31         -0.21\n                                      5         0.33               0.09        0.2       -0.33\n\n                                                C\u2081             C\u2082          C\u2083          C\u2084\n                                      1      -0.085      0.138             -0.153      -0.112\n                                   Output                      A x B = C\n\n                                                                                               62\n---\n     RLHF \u2013 Train with Lora (example)\n\n     \u25cf  The basic operations in neural networks (Matrix version):\n\n                                                                 A\u2081       A\u2082   A\u2083     A\u2084       A\u2085\n                                                   1             0.1  0.6      0.5    0.7  -0.3\n\n        L\u2081       L\u2082                                                   B\u2081            B\u2082             B\u2083        B\u2084\n1       0.3     -0.5         R\u2081      R\u2082       R\u2083   R\u2084       1         -0.13     -0.24          0.05          0.38\n2       0.2     -0.3    1     0.4    -0.3     0.5  0.1      2         -0.07     -0.15          0.04          0.23\n3       0.1      0.7    2     0.5    0.3      0.2  -0.7     3         0.39         0.18        0.19      -0.48\n4       -0.7     0.2                                        4         -0.18        0.27    -0.31         -0.21\n5       0.2      0.5                                        5         0.33         0.09        0.2       -0.33\n\n                             L x R = B                                C\u2081       C\u2082          C\u2083          C\u2084\n                                                           1     -0.085        0.138       -0.153      -0.112\n\n                                                                               A x B = C\n                                                                                                               63\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf     The basic operations in neural networks (Matrix version):\n\n                                                           A\u2081     A\u2082           A\u2083  A\u2084   A\u2085\n\n\u25cf     How many parameters in B?                 1          0.1    0.6     0.5      0.7  -0.3\n\n\u25cf     20 (4*8)                                  L\u2081         L\u2082\n                                          1     0.3       -0.5                     R\u2081   R\u2082        R\u2083  R\u2084\n\n      How many parameters in L and R?     2     0.2       -0.3            1        0.4  -0.3     0.5  0.1\n\u25cf                                         3     0.1        0.7            2        0.5  0.3      0.2  -0.7\n\n\u25cf     18 (5*2 + 2*4)                      4    -0.7        0.2\n                                          5     0.2        0.5\n\n                                                                  C\u2081           C\u2082       C\u2083      C\u2084\n                                                       1          -0.085  0.138    -0.153       -0.112\n\n                                                                               A x L x R = C\n                                                                                                      64\n---\nRLHF \u2013 Train with Lora (example)\n\n\u25cf      The basic operations in neural networks (NN version):\n\n                                                 A\u2081     A\u2082           A\u2083  A\u2084   A\u2085\n\n0.1                                   1          0.1    0.6     0.5      0.7  -0.3\n       -0.085                         L\u2081         L\u2082\n\n0.6                             1     0.3       -0.5                     R\u2081   R\u2082        R\u2083  R\u2084\n       0.138                    2     0.2       -0.3            1        0.4  -0.3     0.5  0.1\n\n0.5                             3     0.1        0.7            2        0.5  0.3      0.2  -0.7\n       -0.153                   4    -0.7        0.2\n                                5     0.2        0.5\n0.7    -0. 112                               1          C\u2081           C\u2082       C\u2083      C\u2084\n\n                                                        -0.085  0.138    -0.153       -0.112\n-0.3\n\n                                                                                            65\n---\nRLHF \u2013 Train with Lora\n\n\u25cf  Framework:\n\n   Weight update in regular finetuning    Weight update in LoRA\n\n             Outputs                      LoRA matrices A and B  Outputs\n                              approximate the weight\n                                 update matrix \u0394W\n\n   Pretrained           Weight            Pretrained                    B\n   weights              update            weights                       r  The inner dimension r\n   W                    \u0394W                                       W         is a hyperparameter\n\n   d\n\n             Inputs                                                 Inputs x\n               d\n\n66\n---\nRLHF \u2013 Train with Lora\n\n\u25cf     Discussion:\n\n\u25cf     By using two learning mapping L: m x r, and R: r x n, to replace a linear mapping:\n      m x n (Here, m and n are the dimensions of mapping function, r is the rank in\n      LoRA, and r << m or n)\n\n\u25cf     If r is small enough, the size of m x r + r x n is significantly less than m x n\n\n\u25cf     We only need very small size of parameters to train a large model\n\n\u25cf     Do you think it is a good idea? Any cons?\n\n67\n---\n   RLHF \u2013 Train with Lora\n\n   \u25cf     Discussion:\n\n   \u25cf     Do you think it is a good idea? Any cons?\n\n   \u25cf     Example on MedMnist dataset\n\n   \u25cf     No free-lunch principle in Machine Learning. So, what\u2019s the price?\n\n          \u25cf     The drop of accuracy\n\n          \u25cf     The efficiency cannot be guaranteed as well.\n                 ACC:  0.27  \u2013 Running time: 5 min 27 sec \u2013 r=3\n                 ACC:  0.81  \u2013 Running time: 5 min 41 sec - r=10\n\nhttps://colab.research.google.com/drive/14FppHOAvi5mUX7VXEIedIuu0dgY_rZJj?usp=sharing    68\n---\n   RLHF \u2013 Train with Lora\n\n   \u25cf     Discussion:\n\n   \u25cf     If the original setting is trainable, maybe we don\u2019t need to use LoRA\n\n   \u25cf     If the original setting requires large memory (like PPO), the LoRA allow you to\n         train a large model with limited memory.\n\nhttps://colab.research.google.com/drive/14FppHOAvi5mUX7VXEIedIuu0dgY_rZJj?usp=sharing    69\n---\nRLHF \u2013 more technical details (example)\n\n \u25cf     We said that we want to increase the generative probability of human preferred\n       answer, but how?\n                                                  P=0.3                                               Advantage\n                                        May the force be with you                                     2.0\n\n What\u2019s the iconic expression    LLM                                                 Reward\n from the star wars movie?                        P=0.2                              model\n\n Current version                        If necessary, please learn to let it go                       -0.8\n\n                      P=0.2      Update based on  May the force be with you\n                                 Step1: Compute the ratio = 0.3/0.2 = 1.5\n      LLM                        Step2: unclipped term = 1.5 x 2 = 3.0\n                                 Step3: clipped ratio, make sure the ratio is within the range of 1\u00b1\u03b5.\n Previous version     P=0.1      Here we take \u03b5=0.2, then ratio should not exceed 1.2\n                                 Step4: clipped term = 1.2 x 2 = 2.4\n                                 Step5: take the minimum from 3.0 and 2.4, which is 2.4 as the objective 70\n---\nRLHF \u2013 more technical details (example)\n                                                    Ratio > 1, the current model tends to generate this\n     Update based on  May the force be with you     sentence. If this indicates a positive advantage,\n                                                    extensive updating is not necessary.\nStep1: Compute the ratio = 0.3/0.2 = 1.5\nStep2: unclipped term = 1.5 x 2 = 3.0                                        Estimate the objective\n                                                                             based on the ratio\nStep3: clipped ratio, make sure the ratio is within the range of 1\u00b1\u03b5.\nHere we take \u03b5=0.2, then ratio should not exceed 1.2\nStep4: clipped term = 1.2 x 2 = 2.4\nStep5: take the minimum from 3.0 and 2.4, which is 2.4 as the objective     We don\u2019t want the ratio is too high\n                                                                            otherwise the model will keep\n                                                                            focusing on this single case\n\nP_current      0.3\nP_previous     0.2                                 For the same we take the\nAdvantage      2.0                                 minimum objective\n\u03b5              0.2    What if we have lower probability in the current language model?\n\n71\n---\nRLHF \u2013 more technical details (example)\n                                                    Ratio < 1, the current model doesn\u2019t tend to generate\n     Update based on  May the force be with you     this sentence. If this indicates a positive advantage,\n                                                    extensive updating is necessary.\nStep1: Compute the ratio = 0.2/0.3 = 0.67\nStep2: unclipped term = 0.67 x 2 = 1.34                                        Estimate the objective\n                                                                               based on the ratio\nStep3: clipped ratio, make sure the ratio is within the range of 1\u00b1\u03b5.\nHere we take \u03b5=0.2, then ratio should less than 0.8\nStep4: clipped term = 0.8 x 2 = 1.6\nStep5: take the minimum from 1.34 and 1.6, which is 1.34 as the objective     We don\u2019t want the ratio is too low\n                                                                              otherwise the model will keep\n                                                                              focusing on this single case\n\nP_current      0.2\nP_previous     0.3                                 For the same we take the\nAdvantage      2.0                                 minimum objective\n\u03b5              0.2\n                      Formal definition:   LCLIP(\u03b8) = Et[min(r(\u03b8) At, clip(r(\u03b8), 1 \u2212 , 1 + ) At)\n\n                                                                                                          72\n---\n    Reinforcement Learning with Human Feedback (RLHF)\n\n    \u25cf     It is still complicated.\n\n    \u25cf     We need to simulate feedback: generate \u2192 get reward \u2192 do policy gradient \u2192\n          clip updates.\n\n    \u25cf     Can we simplify the progress?\n\n    \u25cf     Yes!\n\nReinforcement Learning from Human Feedback (RLHF)\nx: \"write me a poem about     label rewards\n     the history of jazz\"\na-p                          reward model    LM policy\npreference data maximum       sample completions\n                             likelihood  reinforcement learning\nDirect Preference Optimization (DPO)\nx: \"write me a poem about\nthe history of jazz\"\na.                       final LM\npreference data maximum\n                         likelihood\n\n                                                                                                          73\n---\n    Direct Preference Optimisation\n\n    \u25cf     We already know which answer humans like\n\n    \u25cf     Just directly make those more likely than the bad ones.\n\n    log \u03c3 (\u03b2 (l1og\u03c0\u03b8(y+|x)                                             \u03c0\u03b8(y\u00af|x)))]\n    LDpo(\u03b8) = \u2212E(x,y+,y )    \u03c0ref(y+|x) - log\n                                                                       \u03c0ref(y\u2212|x)\n\nReinforcement Learning from Human Feedback (RLHF)\nx: \"write me a poem about    label rewards\nthe history of jazz\"\na-p                          reward model    LM policy\npreference data maximum      sample completions\n                             likelihood  reinforcement learning\nDirect Preference Optimization (DPO)\nx: \"write me a poem about\nthe history of jazz\"\na.                       final LM\npreference data maximum\n                         likelihood\n\n                                                                                                          74\n---\nGroup Relative Policy Optimization\n\n\u25cf     Do not need a learned value function /     Build Reasoning LLMs using GRPO                          join.DailyDoseofDS.com\n      critic to estimate advantages (simple)                   Dataset    System Prompt                   Tokenization\n\n\u25cf     Sample groups of responses (actions)          Data                  \"Think step by   2              Think step by step\n      from a given prompt (state) and compute    Processing               step...\"                        before you answer a\n                                                                                                          question, include the\n                                                                                                          reasoning tokens in\n      relative advantages across that group.\n      (stable)                                                 LLM        vLLM Sampling Engine            Responses\n                                                                                                           Response 1\n      Given a prompt, generate multiple          Generation               4  - temp = 1.0         5        Response 2\n\u25cf                                                              deepseek      -num_gen = 4                  Response 3\n      candidate responses and compute                                                                      Response 4\n      statistics (mean, std) over the group.                   Reward Server    Reward Aggregator          Rewards\n                                                                                                           Reward 1\n      Then define each response\u2019s advantage      Reward        :          Format                     8        Reward 2\n\u25cf                                                Calculation        Correctness                               Reward 3\n      as something like how much better it                                Answer\n                                                                  Correctness                                 Reward 4\n      performed relative to its peers.                          GRPO Loss Calculator    Back prop.        Update Model\n\n\u25cf     The function could be very simple like        Loss        BA\n      the length of sentence.                   Calculation                                               deepseek\n                                                                                                           75\n---\nFurther discussion\n\n\u25cf     There are still many unsolved problem in the LLM\n\n\u25cf     For example,\n\n       \u25cf     Some aspects in the reward function cannot be clearly defined:\n             Helpfulness vs Safety.\n\n       \u25cf     It would be hard for user to identify the better answer\n\n       \u25cf     Lacks stability: if you change your prompt, you will get a very different\n             answer\n\n       \u25cf     \u2026\u2026\n                                                                           o\n                                                                    B0883\n\n76\n---\nKING'S\nCollege\nLONDON\n\nThank you\n\nDr Lin Gui\nLin.1.Gui@kcl.ac.uk\nwww.kcl.ac.uk/people/lin-gui\n\n"
    },
    "data/raw/Week9 - LGT.pdf": {
        "metadata": {
            "file_name": "Week9 - LGT.pdf",
            "file_type": "pdf",
            "content_length": 54823,
            "language": "en",
            "extraction_timestamp": "2025-11-26T00:15:39.438938+00:00",
            "timezone": "utc"
        },
        "content": "                    KING'S\n   Applications of  .\n                    College\n   LLM \u2013 Part I:    LONDON\n   Retrieval\n   Augmented\n   Generation\n   (RAG)\n\n   Week 9 - LGT     BVSH HOVSE\n\nG  Dr Lin Gui\n   Lin.1.gui@kcl.ac.uk\n---\nLearning outcomes\n\n\u26ab By the end of this topic, you will be able to:\n\n  \u26ab  Understand the core concepts of Retrieval-Augmented Generation and how it\n     differs from standard LLM approaches.\n\n  \u26ab  Build and configure a basic RAG pipeline using embeddings, retrievers, and\n     generators.\n\n  \u26ab  Evaluate and optimize RAG performance through effective data preparation,\n     chunking, and retrieval strategies.\n\n2\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n3\n---\n                                                        RAG overview\n\n                                                                               \u26ab                                  When answering questions\n                                                                                                                  or generating text, it first\n                                                                                                                  retrieves relevant\n                                                                                                                  information from a large\n                                                                                                                  number of documents, and\n                                                                                                                  then LLMs generates\n                                                                                                                  answers based on this\n                                                                                                                  information.\n\n                                                                               \u26ab                                  By attaching an external\n                                                                                                                  knowledge base, there is\n                                                                                                                  no need to retrain the\n                                                                                                                  entire large model for each\n                                                                                                                  specific task.\n\n                                                                               \u26ab                                  The RAG model is\n                                                                                                                  especially suitable for\n\n                                           Input        Query                       Indexing                      knowledge-intensive tasks.\nUser                                        How do you evaluate the fact       Documents\n                                            that OpenAI's CEO, Sam Altman,              Chunks Vectors\n\n        Output                              by the board in just three days,\n                                            and then was rehired by the                 embeddings\n                                            company, resembling a real-life\n                                            version of \"Game of Thrones\" in             Retrieval\n\nwithout RAG                                                                    Relevant Documents\n\nfuture events. Currently, I do not have     LLM         Generation\n\nand rehiring of OpenAI's CEO ..            Question:                           Chunk 1: \"Sam Altman Returns to                                4\nwith RAG\n\nthe company's future direction and        based on the following information:     Chunk 2: \"The Drama Concludes? Sam\n                                                                                  Altman to Return as CEO of OpenAl,\nand turns reflect power struggles and     Chunk 2:                                Board to Undergo Restructuring\"\n                                          Chunk 3 :\nOpenAl...                                 Combine Context                         OpenAl Comes to an End: Who Won\n             Answer                       and Prompts                             and Who Lost?\"\n---\n   Symbolic Knowledge or Parametric Knowledge\n\n    \u26ab Ways to optimize LLMs.\n\n    \u26ab Prompt Engineering    This week\n\n    \u26ab Instruct / Fine-tuning\n\n    \u26ab  Retrieval-Augmented\n       Generation\n\n    Week 7    Week 8\n\nExternal Knowledge\n     Required\n    High     Modular RAG\n\n             multiple modules    Retriever Fine-tuning\nAdvanced RAG                     Collaborative Fine-tuning\n\noptimization                     All of the above\n Naive RAG                       RAG             Generator Fine-tuning\n\n   XoT Prompt     Prompt Engineering  Fine-tuning          5\n e.g. CoT, ToT\nFew-shot Prompt\n    Low           Standard Prompt                      Model Adaptation\n           Low                                       High  Required\n---\nRAG vs Fine-tuning\n\n Data Processing\n                ddling.    datasets, and limited datasets may not result\n\n 6\n\n higher latency.    retrieval, resulting in lower latency.\n---\nRAG Application\n\n\u26ab Scenarios where RAG is applicable:\n\n  \u26ab  Long-tail distribution of data\n\n  \u26ab  Frequent knowledge updates\n\n  \u26ab  Answers requiring verification and traceability\n\n  \u26ab  Specialized domain knowledge\n\n  \u26ab  Data privacy preservation\n\nQuestion Answering     Fact checking    Dialog systems    Summarisation\n\nMachine translation    Code generation    Sentiment Analysis    Commonsense\n                                                                reasoning\n\n                                                                           7\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  Foundation of information Retrieval\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 8\n---\n   Foundation of information Retrieval\n\n   \u26ab What is information Retrieval?\n\n     \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n        returns those items to the user, typically in list form sorted per computed\n        relevance#\n\n   \u26ab Three main questions in information retrieval:\n\n     \u26ab  How to map the text into features (Embedding method)\n\n     \u26ab  How to measure the similarity between features (IR Modelling)\n\n     \u26ab  How to do it efficiently (Indexing)\n\n[#] Qiaozhu Mei and Dragomir Radev, \u201cInformation Retrieval,\u201d The Oxford Handbook of Computational Linguistics,\n2\u207f\u1d48 edition, Oxford University Press, 2016.    9\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n10\n---\nDiscrete representation\n\n\u26ab  In discrete representation, for both query and document, we assign each word a\n   specific dimension. If a word appears query/document, then value of the\n   corresponding dimension is:\n\n    \u26ab  In Binary representation: 1\n\n    \u26ab  In TF (term frequency) based representation: t (how many times this word\n       appears within the query/documents)\n\n    \u26ab  In TF-IDF (inverse document frequency) based representation: tlog(n/x)\n\n       \u26ab  Here, t is term frequency, n is number of documents, x is the number of\n          documents which contains this term.\n\n11\n---\nDiscrete representation (example)\n\n\u26ab We have the following documents:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n\u26ab After pre-processing:\n\n  \u26ab  D1 = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d.\n\n  \u26ab  D2 = \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n  \u26ab  D3 = \u201cshipment\u201d, \u201cgold\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n                                                  12\n---\nDiscrete representation (example)\n\n \u26ab Building vocabulary:\n\n   \u26ab V = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d, \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n \u26ab  Detect the feature for each document. If the feature occurs, the corresponding\n    value is \u20181\u2019, otherwise \u20180\u2019 (binary feature):\n\n           shipment  gold  damage  fire          delivery  silver  arrive  truck\n     D1     1        1     1       1             0         0       0       0\n     D2     0        0     0       0             1         1       1       1\n     D3     1        1     0       0             0         0       1       1\n\n 13\n---\nDiscrete representation (example)\n\n\u26ab  Definition \u2013 term frequency (TF):\n\n    \u26ab  \ud835\udc61 - how many times the term appears in the document\n\n\u26ab  Example:\n\n    \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n    \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n    \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n              shipment  gold  damage  fire  delivery  silver    arrive  truck\n        D1     1        1     1       1     0              0     0      0\n        D2     0        0     0       0     1              2     1      1\n        D3     1        1     0       0     0              0     1      1\n\n                                                                             14\n---\nDiscrete representation (example)\n\n\u26ab Definition \u2013 inverse document frequency (IDF):\n\n  \u26ab  \ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc5b/\ud835\udc65) \u2013 n is number of documents, x is the number of documents which\n     contains this term\n\n\u26ab Example:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n          shipment     gold  damage  fire  delivery  silver     arrive  truck\n          0.176     0.176    0.477   0.477  0.477    0.477      0.176   0.176\n                       Inverse document frequency vector\n\n                                                                             15\n---\nDiscrete representation (example)\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1      1         1        1      1         0        0           0         0\n D2      0         0        0      0         1        2           1         1\n D3      1         1        0      0         0        0           1         1\n                           Term frequency matrix\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n        0.176     0.176    0.477   0.477    0.477     0.477      0.176     0.176\n                           Inverse document frequency vector\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1     0.176     0.176    0.477   0.477     0        0           0         0\n D2      0        0         0      0        0.477     0.954      0.176     0.176\n D3     0.176     0.176     0      0         0        0          0.176     0.176\n\n                            TF-IDF Matrix\n                                                                                16\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n17\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n18\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n19\n---\n   Dense Passage Retrieval\n\n   \u26ab  Encode questions and text passages into continuous vectors (embeddings) and\n      retrieve passages using vector similarity instead of keyword overlapping.\n\n   \u26ab  Train directly on question\u2013passage pairs, using in-batch negatives to improve\n      efficiency.\n\n                 Question q    Passage p                                In each batch, there are multiple\n\n                 BERTQ         BERTp                Passage encoder     question\u2013answer pairs, both\n      Question encoder                                                  matched and unmatched. Matched\n                                                                        pairs should have similar\n\n                          OOOOOOOO  0OOOOOOO                            representations, while unmatched\n                                                                        pairs should have representations\n                          $h_q$                                         that are far apart.\n                                                Training phase\n\n      Similarity score: dot product  (q, =  Fine-tune two encoders\nhttps://aclanthology.org/2020.emnlp-main.550.pdf                                                 20\n---\n   ReContriever\n\n   \u26ab  What if we don\u2019t have annotated data (Matched and unmatched QA-pair).\n\n   \u26ab  Using pseudo-examples: For each passage/document p, create an augmented\n      version p\u2032. Then treat (p, p\u2032) as a positive pair:\n\n       \u26ab  Masking words (random word masking)\n\n       \u26ab  Span deletion\n\n       \u26ab  Back-translation Sentence\n\n       \u26ab  Reordering Adding noise\n\n       \u26ab  Perturbations Cropping (taking a subset of sentences)\n\nhttps://aclanthology.org/2023.findings-acl.695.pdf    21\n---\n  Using API\n\n   \u26ab  There are many APIs could do this job, for example, Mistral AI:\n\n                       YMISTRAL EMBED API\n\n                       8OPEN IN COLAB\n\n   \u26ab  Example: link    How to Generate Embeddings\n                       To generate text embeddings using Mistral Al's embeddings APl, we can make a request to the APl endpoint and specify the\n                       embedding model mistra1-embed , along with providing a list of input texts. The APl will then return the corresponding\n                       embeddings as numerical vectors, which can be used for further analysis or processing in NLP applications.\n\n   \u26ab Some other options:    PYTHON TYPESCRIPT      CURL                                                           OUTPUT\n                            import os\n                            from mistralai import Mistral\n          Sentence Bert     api_key = os.environ[\"MISTRAL_API_KEY\"]\n     \u26ab                      model = \"mistral-embed\"\n                            client = Mistral(api_key=api_key)\n\n     \u26ab    SimCSE            embeddings_batch_response = client.embeddings.create(\n                            model=model,\n                            inputs=[\"Embed this sentence.\", \"As well as this one.\"],\n\n     \u26ab    \u2026\u2026                )\n                            The output is an embedding object with the embeddings and the token usage information.\n\n                            Let's take a look at the length of the first embedding:\n\n                            PYTHON TYPESCRIPT CURL\n                            len(embeddings batch response.data[0].embedding)\n\nhttps://docs.mistral.ai/capabilities/embeddings    22\n---\nIR Modelling\n\n\u26ab What is information Retrieval?\n\n  \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n     returns those items to the user, typically in list form sorted per computed\n     relevance\n\n\u26ab Three main questions in information retrieval:\n\n  \u26ab  How to map the text into features (Embedding method)\n\n  \u26ab  How to measure the similarity between features (IR Modelling)\n\n  \u26ab  How to do it efficiently (Indexing)\n\n23\n---\nIR Modelling\n\n\u26ab  In IR modelling, we use different metric to measure the similarity/distance\n   between the query and given documents. The target is to find the top-k relevant\n   documents based on the given query.\n\n    \u26ab  Cosine similarity (for both Discrete & Continuous representation)\n\n    \u26ab  Jaccard distance (for Discrete representation only)\n\n    \u26ab  BM25 (for Discrete representation only)\n\n24\n---\nCosine similarity\n\n\u26ab Cosine similarity\n                           \u03c3\ud835\udc5b \ud835\udc65\ud835\udc56 \ud835\udc66\ud835\udc56\n                   \ud835\udc36\ud835\udc5c\ud835\udc60 \ud835\udc65, \ud835\udc66 = \u03c3\ud835\udc5b  \ud835\udc56=1 \u03c3\ud835\udc5b\n                                   \ud835\udc56=1(\ud835\udc65\ud835\udc56 )2  \ud835\udc56=1(\ud835\udc66\ud835\udc56 )2\n\n\u26ab Considering\n  \u2212  D1 = [1,1,1,1,0,0,0,0]\n  \u2212  D3 = [1,1,0,0,0,0,1,1]\n                                 \ud835\udc36\ud835\udc5c\ud835\udc60(D1,D3)=1/2\n\n25\n---\nJaccard similarity\n\n\u26ab Only considering if there is over lapping or not. We don\u2019t care about the value.\n\n\u26ab For example:            C1    sim(cl,c2)    C1  C2\n\n  \u26ab  \ud835\udc45\ud835\udc65 = [2,0,3,3]     C2\n\n  \u26ab  \ud835\udc45\ud835\udc66 = [1,1,0,5]     C3                    JACCARD SIMILARITY\n                                                  2    0.5\n                                              4  2+1+1\n\n\u26ab Jaccard similarity: \ud835\udc60\ud835\udc56\ud835\udc5a \ud835\udc65, \ud835\udc66 = \ud835\udc79\ud835\udc99\u2229\ud835\udc79\ud835\udc9a\n                                    \ud835\udc79\ud835\udc99\u222a\ud835\udc79\ud835\udc9a\n\n                                               26\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              hyperparameters         Average length of all docs\n                                                                                  27\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b      \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1      \ud835\udc56               1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n     Maybe\u2026a bit confusing                            Average length of all docs\n     Can you speak in English?  hyperparameters\n                                                                                  28\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b             \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1             \ud835\udc56        1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n Relax\u2026it is pretty simple actually    hyperparameters    Average length of all docs\n\n                                                                                  29\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:  IDF term in Q (is it an important word?)\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| )\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              The \u2018percentage\u2019 of querying words in D\n\n                                                                                  30\n---\nIndexing\n\n\u26ab Next question, how to do it efficiently (Indexing)\n\n\u26ab  Suppose we have 1k queries, and there are 1 billion documents in knowledge\n   based, how many times of comparison we need?\n\n\u26ab  1k x 1b\n    It is a really huge number.\n    In real world scenario, it could be even larger\n    If there is only one important task in information retrieval, it must be\n    \u201cindexing\u201d\n\n31\n---\nIndexing - Discrete representation\n\n\u26ab  Inverted index\n\n\u26ab  Since the discrete representation is sparse (most dims are zero), we can build\n   inverted index. For each word, we build a link list to store all the documents\n   contain this word.\n\n\u26ab  For the given query, the complexity is now only related to the #unique words in\n   the query. (In most queries, the size is just few words)\n   doclD                          geo-scopelD              geo-scopelD   docID\n     1                            Europe                   Europe        1 2 7\n     2                                Europe               France        3\n     3                                France               Portugal      5\n     4                                England              England       4\n     5                                Portugal             Quebec        6\n     6                                Quebec               Spain         8\n     7                                Europe\n     8                                Spain\n                     Forward Index                         Inverted Index\n                                                                                 32\n---\n   Indexing - Continuous representation\n\n   \u26ab  In continuous representation, it might be a bit complex. There is no sparse\n      representation anymore.\n\n   \u26ab  We can use the following method to speed up the searching.\n\n       \u26ab  Vector compression \u2013 reduce the size of vectors\n\n       \u26ab  Hierarchical clustering \u2013 in each layer only search the nearest cluster\n                                          Clustering the documents first, and then,\n                                          Only consider the nearest centroid during the searching\n          voronoi cells  xq  Centroids                        voronoi cells  xq    Centroids\n                         o\n                             o                                                     o\n                  e\n                         o\n                  o          \u00a9    o                           9                  o\n                                  -\n                       o                                           o\n         Pause (k)\n\nhttps://www.pinecone.io/learn/series/faiss/faiss-tutorial/                                       33\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n34\n---\nNaive RAG\n\n\u26ab  Step 1 \u2013 indexing\n\n    \u26ab  Divide the document into even chunks, each chunk being a piece of the\n       original text.\n\n    \u26ab  Using the encoding model to generate an embedding for each chunk.\n\n    \u26ab  Store the Embedding of each block in the vector database.\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  Retrieve the k most relevant documents using vector similarity search.\n\n\u26ab  Step 3 \u2013 Generation\n\n    \u26ab  The original query and the retrieved text are combined and input into a LLM\n       to get the final answer\n                                                                             35\n---\n  Naive RAG\n\n    \u26ab Step 1 \u2013 indexing\n\n    \u26ab Step 2 \u2013 Retrieval\n\n    \u26ab Step 3 \u2013 Generation\n\n                                    Offline\n\nDocuments  Document Chunks  Vector Database\n    8                                      36\n   User  Query    Related DocumentChunks\n                            Frozen\n    Augmented Prompt        LLM\n---\nAdvanced RAG\n\n  \u26ab Step 1 \u2013 indexing\n\n  \u26ab + index optimization\n\n  \u26ab + pre-retrieval process\n\n  \u26ab Step 2 \u2013 Retrieval\n\n  \u26ab +post-retrieval process\n\n  \u26ab Step 3 \u2013 Generation\n\n  URLS  PDFs  Database\n     Documents               Document Chunks       Vector Database\n                             Fine-grained Data Cleaning\n                             Sliding Window /Small2Big\n                             Add File Structure\n                             Query Rewrite/Clarifcation\n  User    Query              Retriever Router                          37\n                              Pre-retrieval    Related Document Chunks\n\n  Prompt              LLM                        Rerank  Filter  Prompt Compression\n                                                         Post-retrieval\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Sliding windows\n\n    \u26ab  + index optimization       Fine-grained segmentation\n\n    \u26ab  + pre-retrieval process    Adding metadata\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n38\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Sliding windows\n\n      \u26ab  + index optimization       Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process    Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n                                    Document\n\n         0                200 100   300 200  400 300  500\n\n  Split the doc into chunks, and ensure there is over lapping between chunks (WHY?)\n                                                                                   39\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                                      Sliding windows\n\n      \u26ab  + index optimization          Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                            Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n\n                          Document     Section 1             Paragraph 1.1\n\n                                       Section 2             Paragraph 1.2\n\n                          Searching on fine-grained text     Paragraph 1.3\n                                                                           40\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                               Sliding windows\n\n      \u26ab  + index optimization                        Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                     Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation                             Web page    Publishing date\n\n                                                                    Title\n     The metadata is the aspects of each chunk.\n     It will help both retriever and generator to                Parents node\n     improve the performance.\n\n  41\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process    Summarization\n\n\u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n    \u26ab  +post-retrieval process    Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n\n42\n---\nAdvanced RAG\n\n   \u26ab  Step 1 \u2013 indexing                    Retrieve routes\n\n       \u26ab  + index optimization\n\n       \u26ab  + pre-retrieval process          Summarization\n\n   \u26ab  Step 2 \u2013 Retrieval                   Rewriting\n\n       \u26ab  +post-retrieval process          Confidence judgment\n\n   \u26ab  Step 3 \u2013 Generation                  Instead of one flat \u201cretrieve chunks by embeddings\u201d step, you can:\n\n                                           Searching doc first\n\n  Retrieve routes = multiple retrieval     Query    Document    Chunk\n  paths that a RAG system can choose                            Searching chunks\n  from, depending on query intent, data                         within the doc\n  type, or document structure.\n                                                                                                             43\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing                       Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process             Summarization\n\n\u26ab  Step 2 \u2013 Retrieval                      Rewriting\n\n    \u26ab  +post-retrieval process             Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n                                           Document\n                                                           Summarise first\n                                  Query    Summarisation\n\n                                  Searching in smmarisation\n                                  instead of full documents\n                                                                          44\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing               Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process     Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval              Rewriting\n\n      \u26ab  +post-retrieval process     Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n        Benefits:                    Query\n        a more explicit query             Rewrite the query first\n        a more keyword-rich query    Rewrite the\n        a more structured query      query      Searching\n        multiple diverse sub-queries\n                                     Searching by re-written query\n                                                                  45\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process    Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n      \u26ab  +post-retrieval process    Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n  Query     Document             LLM            Confidence checking    Output\n\n            Confirm the Confidence before output\n             By similarity scores\n             By LLM Confidence scores                                        46\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing           Re-order\n\n    \u26ab  + index optimization    Filter content retrieval\n\n    \u26ab  + pre-retrieval process\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n47\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing           Re-order\n\n      \u26ab  + index optimization    Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation         Evidence#1  Evidence#2  Evidence#3  Question\n\n  LLMs is sensitive with the input order\n  The early input chunks has higher weights\n  How to organize the searched evidence for final output is\n  important\n\n  48\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Re-order\n\n      \u26ab  + index optimization       Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process    Evidence#1  Evidence#2  Evidence#3  Question\n\n  \u26ab  Step 3 \u2013 Generation\n\n                                                           Evidence#1  Evidence#3  Question\n\n  To avoid possible hallucination, filtering the irrelevant\n  evidences.\n\n                                                                                           49\n---\nModular RAG\n\n   Na\u00ef\n\u26ab  ve RAG\n\n      Read           Retrieve    Generate\n\n\u26ab  DSP\n\n   Demonstrate       Search      Predict    Generate\n\n\u26ab  Rewrite-Retrieve-Read\n\n      Rewrite        Retrieve    Read\n\n\u26ab  Retrieve-then-read\n\n   Retrieve          Read        Generate\n\n50\n---\n   Different RAG Paradigms\n\n    Modules\n    8 C 8 E2    Search\nUser Query  Documents    User Query  Documents    Routing    Predict\n\n    Indexing    Query Routing    Indexing    Rewrite  RAG  Rerank\n\n  Read\n                               Fusion\n Memory\n\n    \\Post-Retrieval    Patterns\n \u2192|l|+                               51\nSummary                        Retrieve\n\n    Output    Output\n\n    Naive RAG    Advanced RAG    Modular RAG\n---\nKey problems in RAG\n\n \u26ab  How to retrieve\n\n \u26ab  When to retrieve\n\n \u26ab  How to use the retrieved information\n\n 52\n---\nHow to retrieve\n\n \u26ab By using the information on different structuration levels\n\n \u26ab  Token level         It excels in handling long-tail and cross-domain issues with high\n                        computational efficiency, but it requires significant storage.\n\n \u26ab  Phrase level\n\n \u26ab  Chunk level         The search is broad, recalling a large amount of information, but with\n                        low accuracy, high coverage but includes much redundant information.\n\n \u26ab  Entity level\n\n \u26ab  Knowledge level     Richer semantic and structured information, but the retrieval efficiency\n                        is lower and is limited by the quality of KG.\n\n 53\n---\n   When to retrieve\n\n    \u26ab Two questions:\n\n    \u26ab When we need to retrieve information to support the QA\n\n    \u26ab How many times we need to retrieve the information\n\n    \u26ab Solution#1: Conducting once search during the reasoning process.\n\n    High efficiency, but low relevance of the\n    retrieved documents\n\n    Retrieved document d.\n              Jobs cofounded     Jobs was raisedd;    Jobs is thex    apple\n  Retriever    Apple in his      by adopted...        CEO of        pearnot\n             parents' garage\n Document    Input                 Steve Jobs         Jobs is the     apple    apple\nRetrieval    Reformulation       passed away...       CEO of        pearnot    pearnot    54\nTest Context X    Black-box      Jobs cofoundedJobs is the            apple\n Jobs is the    LM                  Apple...          CEO of           pear\n   CEO of _                                           Ensemble          not\n    Apple\n---\n  When to retrieve\n\n   \u26ab  Two questions:\n\n   \u26ab  When we need to retrieve information to support the QA\n\n   \u26ab  How many times we need to retrieve the information\n\n   \u26ab Solution#2: Adaptively conduct the search.\n\n   Balancing efficiency and information\n   might not yield the optimal solution\n\n Search results:Dx               Retriever\n [1]:Search results:Dq2\n [2]:[1]:Search results:Dq3\n [2]:[1]: ...\nP     [2]: ..                             x\n     x Generate a summary about Joe Biden.\nAy1 Joe Biden attended           q2                55\n Q2[Search(Joe Biden University)]\n y2tthe University of Pennsylvania, where he earned\n q3[Search(Joe Biden degree)]    q3\n y3 a law degree.\n---\n When to retrieve\n\n  \u26ab  Two questions:\n\n  \u26ab  When we need to retrieve information to support the QA\n\n  \u26ab  How many times we need to retrieve the information\n\n  \u26ab Solution#3: Retrieve once for every N tokens generated.\n\n  A large amount of information with low\n  efficiency and redundant information.\n\n    Masked Language Modelling:\n    Bermuda Triangle is in the    western part\n  <MASK> of the Atlantic Ocean.\nPretraining    Atlas\nFew-shot\n          Fact checking:\nBermuda Triangle is in the western                                False    56\n      part of the Himalayas.            The Bermuda\n                                    Triangle is an urban\n                                    legend focused on a\n                                      loosely-defined\n       Question answering:             region in the       Western part of the\n  Where is the Bermuda Triangle?    western part of the    North Atlantic Ocean\n                                       North Atlantic\n                                           Ocean.\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 57\n---\nKey Technologies\n\n\u26ab  Data indexing optimization\n\n\u26ab  Structured Corpus\n\n\u26ab  Retrieval Source Optimization\n\n\u26ab  KG as a Retrieval Data Source\n\n\u26ab  Query Optimization\n\n\u26ab  Embedding Optimization\n\n\u26ab  Fine-tuning on RAG\n\n58\n---\n Data indexing optimization\n\n  \u26ab Chunk optimization\n\n  \u26ab Small-2-big: Embedding at sentence level expand the window during generation\n\n  process.\n\n                   Embed Sentence \u2192 Link to Expanded Window\n\n                              Continuous observation of the Atlantic meridional\n                              overturning circulation (AMOC) has improved the\n                              understanding of its variability (Frajka-Williams et al.,    What the LLM Sees\n                              2019), but there is low confidence in the quantification\n                              of AMOC changes in the 20th century because of low\n                   Embeddingagreement in quantitative reconstructed and simulated\n                              trends. Direct observational records since the\n                   Lookup     mid-2000s remain too short to determine the relative\nQuestion:                     contributions of internal variability, natural                                59\n                              forcing and anthropogenic forcing to AMOC change\nWhat are the                  (high confidence). Over the 21st century, AMOC wil\nconcerns                      very likely decline for all SSP scenarios but will not\n                              involve an abrupt collapse before 2100. 3.2.2.4 Sea Ice\nsurrounding the               Changes\nAMOC?                         Sea ice is a key driver of polar marine life, hosting        What the LLM Sees\n                              unique ecosystems and affecting diverse marine\n                              organisms and food webs through its impact on light\n                              penetration and supplies of nutrients and organic\n                              matter (Arrigo, 2014)\n---\nData indexing optimization\n\n\u26ab  Chunk optimization\n\n\u26ab  Sliding window: sliding chunk covers the entire text, avoiding semantic ambiguity.\n\n                          Maintain overlap for\n                          contextual continuity\n\nLoaded large\ndocument  Dividing into  Merging units into\n          compact units  larger chunks\n\n60\n---\nData indexing optimization\n\n \u26ab  Chunk optimization\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 61\n---\nStructured Corpus\n\n \u26ab  Adding meta-data: adding meta-data in the query searching to improve retrieval\n    accuracy, provide context during chunking, and enables filtering\n\n                             Indexed documents\n                                                                        Filtered subset of\n                                                                        documents    Most relevant\n    Did we implement any new                                                           documents\n       policies in 2021?     year: 2020, content: ...\n\n                             year: 2020, content:                       year: 2021, content.:..\n    year = 2021              year: 2021, content: .    Select relevant  year: 2021, content...    Vector similarity    year: 2021, content:.\n                                                       documents                                  search\n                             year: 2021, content:..                     year: 2021, content...                       year: 2021, content:...\n    Metadata filter          year: 2021, content: .\n\n Filter the irrelevant docs\n\n                           Ensure each chunk contains the metadata\n\n                                                                  62\n---\nRetrieval Source Optimization\n\n \u26ab  Adding meta-data\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 63\n---\nKG as a Retrieval Data Source\n\n\u26ab  Extract entities from the user's input query, then construct a subgraph to form\n   context, and finally feed it into the large model for generation.\n\n    \u26ab  Use LLM (or other models) to extract key entities from the question.\n\n    \u26ab  Retrieve subgraphs based on entities, delving to a certain depth, such as 2\n       hops or even more.\n\n    \u26ab  Utilize the obtained context to generate answers through LLM.Two-stage\n       method: Retrieve documents through summaries, then retrieve text blocks\n       from the documents.\n\n64\n---\n  KG as a Retrieval Data Source\n\n  \u26ab  Extract entities from the user's input query, then construct a subgraph to form\n     context, and finally feed it into the large model for generation.\n     P Meta Summary Entities        x k    Layer[i+1]\n                                    Summary Entities    Summarization by LLM  Layer[i]\n                                    Normal Entities     GMM Clustering        Layer[i-1]\n     Documents                      Hilndex: Indexing with Hierarchical Knowledge\n\n                                    6Communities        Global                                            Community Report\n                  Query             Key Entity          Bridge\n                                    Reasoning Paths                              Reasoning Paths                          Generation by LLM\n                                                        xk\n                                    Hatten KG             Local    Key Entity\n                                                                  Descriptions\n                                                        HiRetrieval: Retrieval with Hierarchical Knowledge\n\nhttps://arxiv.org/pdf/2503.10150                                                                                                           65\n---\n   Query Optimization\n\n    \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n       the Query can yield better retrieval results.\n\n\u26ab  Rewrite query:\n                 Input     Input\n                           Black-box LLM\n\n                 Retriever    Rewriter\n\nInput    Example\nSmall PrLM  Input:\n            What profession does Nicholas Ray and\nRewriter            Elia Kazan have in common?\n\n                                  Query             Quuery              Query: Nicholas Ray profession\n                     Documents    Web Search        Web Search            Query: Elia Kazan profession\n                                  Retriever         Retriever           Elia Kazan was an American film and\n                                                                         theatre director, producer,\n       Black-box LLM                                Documents            screenwriter and actor, described\n           Reader                 Documents                               Nicholas Ray American author and\n                                                                          director, original name Raymond\n                                                                          Nicholas Kienzle, born August 7,\n                     Output       Black-box LLM     Black-box LLM         1911, Galesville, Wisconsin, U.S.\n                                  Reader            Reader               Correct (reader                   director\n                                  Output            Reward Output        Hit (retriever\n                     (a) Retrieve-then-read (b)Rewrite-retrieve-read    (c) Trainable rewrite-retrieve-read\n\n https://arxiv.org/pdf/2305.14283    66\n---\n Query Optimization\n\n  \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n     the Query can yield better retrieval results.\n\n  \u26ab Clarify the query:                                        Ambiguous Question (AQ)\n                                                    \"What country has the most medals         o  M\n                                                             in Olympic history?\"             88\n\n                                                      Tree of Clarifications\n                                                      Question                       Pruned   Information\n                                                   Clarification                              Retrieval\n                                                                       *\n                                                   DQ1            DQ2   DQ3\n                                                                       \"What country has\n                                                                       the most total medals\n                                                                       in Olympic history?\"\n                                                   Question       Question\n                                                   Clarification  Clarification\n                                                   *                   *                      Passages\n                                                   DQ11 DQ12 DQ13 DQ21 DQ22 DQ23 DQ24\n                                                   \"What country has the  \"What country has\n                                                   most medals in winter  the most gold medals\n                                                     Olympic history?\"  in Olympic history?\"\n\n                                                                                     Long Form Answer\n\n                                                     Answer                          \"The United States has the\n                                                   Generation                        most total medals. .\n                                                                                        Norway has won most\n\nhttps://aclanthology.org/2023.emnlp-main.63.pdf                                      medals in winter Olympic.\"    67\n---\n Embedding Optimization\n\n  \u26ab Better embedding always indicate a better retrieval results:\n\n             \u26ab    Selecting a more suitable embedding method\n                                                                0Retriever &  HotpotQA      Dataset\n                  Fine-tuning the embedding model               Framework      EM F1        2Wiki      NQ   WebQ\n             \u26ab                                                  [BM25                       EM F1 EM F1 EM F1\n                                                                               |25.4, 37.2[16.6 21.1[26.0 32.8 [22.2 31.2\n                                                                2+SuRe      38.8 53.523.8.31.036.6 47.934 4 48.5\n                                                                 +EmbQA (ours) 42.0 55.8|27.4 36.642.2 54.438.2 52.1\n                                                                DPR            20.6 21.7[10.8 13.5[25.0 34.2[23.8 34.4\n                                                                 +SuRe         25.0 31.9 14.2 16.038.8 52.336.0 49.6\n                                                                 +EmbQA (ours) |29.8 36.3 16.8 21.0    38.0 52.0\n                                                                                                   43.0 54.4\n                                                                 Contriever   [22.6 35.4\n                                                                                     [16.6 20.7[25.8 32.8\n                                                                                                             25.2 34.2\n                                                                 +SuRe          33.8 50.6 21.0 29.3 39.0 52.834.4 48.5\n                                                                 +EmbQA (ours) 36.6 52.7 26.4 34.2 42.2 53.6\n                  Try different embedding methods in the RAG    [BM25         [21.2 29.2               36.0 49.6\n                                                                20+SuRe        32.2 46.1 [13.8 21.7 18.8 25.319.0 26.1\n                                                                                            17.8 30.1\n                                                                                                    35.2 45.131.6 45.7\n                                                                  +EmbQA (ours) 34.8 44.3 18.6 30.5 35.8 46.035.8 48.1\n                                                                [DPR               7.8 11.0 3.8 4.5[22.2,26.718.8 27.7\n                                                                +Sure          15.0 21.8 6.4 8.540.0 51.8\n                                                                 +EmbQA (ours) 16.2 23.3 7.6 9.6             32.6 47.7\n                                                                                                   |40.2 49.433.4 46.0\n                                                                Contriever     19.4 28.6[13.6 20.7[21.8 27.4117.8,244\n                                                                 +SuRe         28.0 41.6 17.2 25.4 39.8 51.630.2 45.0\n                                                                 +EmbQA (ours)29.8 42.3 17.4 26.2 40.6 51.8 31.6 43.0\n                                                                [BM25          [28.6 37.1[20.2 24.1 [24.0 29.4 [22.6 31.4\n                                                                20+Sure        43.6 54.7 28.4 34.1 41.6 49.0 36.6 47.3\n                                                                 +EmbQA (ours) 44.6 55.628.8 33.8 42.4 49.2 38.2 48.7\n                                                                [DPR           8.8 9.8 5.6 7.1\n                                                                                                  [29.2 32.6[25.6 31.1\n                                                                 +Sure         21.8 27.3 12.2 16.1\n                                                                                                   45.4 54.6\n                                                                                                             38.4 49.6\n                                                                 +EmbQA (ours) 22.6 29.1 13.8 17.345.8 54.7\n  AD                                                                                                      38.6 50.1\n  Facts                                                         Contriever     27.0 34.0[17.6 20.0 26.6 31.9 21.0 29.1\n\n0                                                                +Sure         38.8 50.323.8 30.4 44.0 52.9 36.4 48.1\n                                                                +EmbQA (ours)39.0 50.2\n            General-Purpose                                                              24.4 30.9 45.2 50.5 37.0 48.6\n     C-Pack  Text Embedding                                                                                              68\nhttps://arxiv.org/pdf/2503.01606\n\n  C-MTEB  C-MTP  C-TEM  Recipe\n---\nEmbedding Optimization\n\n \u26ab Better embedding always indicate a better retrieval results:\n\n   \u26ab  Selecting a more suitable embedding method\n\n   \u26ab  Fine-tuning the embedding model\n\n      An in-context learning based method to generate prompt\n\n                                                                                       generate Query-Doc pair    Fine-tuning with pseudo data\n\n 1           A few query and\n             relevant document\n             examples\n             for each doc     You are an award\n             in documents     winning relevance                                 GPT-x\n                              expert. Suggest          Large Language Model     BARD\n                              relevant queries for                              Flan-T5\nDocuments                     this article $article                                                               69\n                              queries:                 Synthetic queries for documents\n             LLM Query Generation Prompt\n                                                       \"Labeled data\"                  9\n---\nFine-tuning on RAG\n\n\u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n  \u26ab  Retriever fine-tuning\n\n  \u26ab  Generator fine-tuning\n\n70\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n      \u26ab  Retriever fine-tuning\n\n      \u26ab  Generator fine-tuning\n\n                              A small LM\n\n    Using the attention scores\n    annotate which documents\n    the LM \u201cprefers\u201d.\n\n                                                DecSource LM\n                                                    Fusion-in-Decoder\n                                                Enc Enc. Enc\n    Source Task                                     Q+D1 Q+D2Q+DN\n                                                    Retrieve\n                                                      N Docs\n  Positives         Negatives                   Pre-Trained Retriever    71\nGround Truth U    ANCE Sampling\n    -Top-K FiDAtt                               Target LMs Target Tasks\n    https://aclanthology.org/2023.acl-long.136.pdf\n                                     Generic                GCMETRY\n                                     Plug-In                  WAKT\n   Augmentation-Adapted Retriever                           RISTORY\n                                                            LITERATRE\n                                                            SCIENCE\n                                                              MATH\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n    \u26ab Retriever fine-tuning\n\n    \u26ab Generator fine-tuning\n\n    Product Search                 Premium hiking bag.            Unstructured Data                                             Structured Data (Negative)\n    Black large capacity hiking    Size: Max Color: Black                                     Structured Data (Positive)\n    bag, made of canvas.           Material: canvas               These erasable mood pencils     [MASK1] changing [MASK2]       Tire Specifications: Material:\n\nCode Search\nGiven two numbers, the\n\nlargest number is returned\nQuery\n]\nPretrained Language Model\n                               are made of quality wood       [MASK3] with [MASK4]            Rubber Tire Size: 16X6.50-8\n                               and color temperature          Function: removing wrong        Tire Type: Tubeless Rim\ndef compare(a, b):             coating, have non-fading        writing Material: [MASK2]      Width: 5.375\" Tread Depth:\nreturn max(a, b)          colors.                              Changing Size: 18 x 0.5 cm     7.1mm\" Pattern: P332\nStructured Data (Positive)\n Structured Data (Negative)                                               T5\n                               Structured Data Alignment (Loss: C sDA)      Masked Entity Prediction (Loss: LP)\n\n    Training    Push Away                           Prediction                                          erasers\n                                                    [MASK1] Color\n                Ground Truth                        [MASK2] mood\n                                Align               [MASK3] pencils,\n    .                                               [MASK4]\n\n    Embedding Space    Optimized Embedding Space    Add an entity prediction loss in the fine-tuning           72\n---\nK\nING'S\nCollege\nLONDON\n\nThank you\n\nDr Lin Gui\nLin.1.Gui@kcl.ac.uk\nwww.kcl.ac.uk/people/lin-gui\n\n"
    },
    "data/raw/6CCSAHAI-HumanAI Interaction-Lect.pdf": {
        "metadata": {
            "file_name": "6CCSAHAI-HumanAI Interaction-Lect.pdf",
            "file_type": "pdf",
            "content_length": 34804,
            "language": "en",
            "extraction_timestamp": "2025-11-26T00:15:54.066723+00:00",
            "timezone": "utc"
        },
        "content": "6CCSAHAI\n\n  Human-centred AI\n  as a Design Process\n  Part 1: Understanding user needs\n  & Defining the right problem\n  Dr Georgia Panagiotidou\n\n  1\n---\nOverview\n\nWhat is Human-Centred Design?\n \u2022     Who to involve in the design process and why\n \u2022     Degrees and moments of user involvement\n \u2022     Four basic activities of the design process\n \u2022     A simple lifecycle model for the design process\nThe process supports understanding:\n \u2022     How to find out what people need\n \u2022     How to decide what to design\n \u2022     How to generate alternative designs\n \u2022     How to choose among alternative designs\n\n2\n---\nWhat is Human-Centred Design?\n\n\u2022 It is a process that is:\n  \u25aa  Focused on discovering requirements, designing to fulfil requirements,\n     producing prototypes and evaluating them\n  \u25aa  Traditionally focused on users and their goals although with HAI this is\n     somewhat evolving\n  \u25aa  Involves trade-offs to balance conflicting needs and requirements\n\n3\n---\n    ENGAGEMENT\n    Connecting the dots and building relationships\n    between different citizens, stakeholders and partners\n\n                 DESIGN\n               PRINCIPLES\n         1. Be People Centered\n2. Communicate (Visually & Inclusively)\n       3. Collaborate & Co-Create\n      4 Iterate, Iterate, Iterate\n\n    CHALLENGE    OUTCOME\n\n    I Discover  Derine           Develop  Deiver\n                          METHODS\n         BANK\n\nExplore, Shape, Build\n\nCreating the conditions that allow innovation,\n including culture change, skills and mindset\n    LEADERSHIP\n              4\n---\n    INSPIRATION                       IDEATION                                   IMPLEMENTATION\n\nI have a design challenge.        I have an opportunity for design.\nHow do I get started?             How do I interpret what I've learned?\nHow do I conduct an interview?    How do I turn my insights into\nHow do I stay human-centered?     tangible ideas?\n                                  How do I make a prototype?\nI have an innovative solution.\nHow do I make my concept real?\n How do I assess if it's working?\n How do I plan for sustainability?\n\n\n    DvEs 7                            CONE                                 7\n                                                         DVERCE                  CONVERGE\n\n    IDEO\n        5\n---\nStanford d.school Design Thinking Process\n\nInterviews                       Share ideas\nShadowing                        All ideas worthy\n\u2022Seek to understand              Diverge/Converge\nNon-judgmental                   \"Yes and\" thinking\n                                 Prioritize        Mockups\n                                                    Storyboards\nEMPATHIZE              IDEATE                      Keep it simple\n                                                   \u2022Fail fast\n                                                   Iterate quickly\n\n                  DEFINE                   PROTOYPE\nPersonas\n\u00b7Role objectives\n \u2022 Decisions\n Challenges                                             Test\n \u00b7 Pain Points                    Understand impediments\n                                   What works?\n https://dschool.stanford.edu     Role play\n                                  Iterate quickly\n\n6\n---\nOther lifecycle models:\nDesign Sprints\n\n  Phase 1  Phase 2    Phase 3  Phase 4    Phase 5       Phase 6\n\n  UNDERSTAND  DEFINE    SKETCH  DECIDE    PROTOTYPE     VALIDATE\n\n  designsprintkit.withgoogle.com/methodology/overview\n\n  7\n---\n                     ENGAGEMENT\n    Today\u2019s focus    Connecting the dots and building relationships\n                 between different citizens, stakeholders and partners\n\n                                         DESIGN\n                                       PRINCIPLES\n                                 1. Be People Centered\n                        2. Communicate (Visually & Inclusively)\n                               3. Collaborate & Co-Create\n                              4 Iterate, Iterate, Iterate\n\n    - Discover / Empathise / Understand    CHALLENGE  I Discover  Derine    Develop  Deiver    OUTCOME\n\n    - Define    METHODS\n                  BANK\n\n         Explore, Shape, Build\n\nCreating the conditions that allow innovation,\n including culture change, skills and mindset\n    LEADERSHIP\n\n    Interviews                      Share ideas\n    Shadowing                       All ideas worthy\n    Seek to understand              \u00b7Diverge/Converge\n    Non-judgmental                  \"Yes and\" thinking\n                                    Prioritize        \u00b7Mockups\n                                                       Storyboards\n    EMPATHIZE             IDEATE                      Keep it simple\n                                                      \u2022Fail fast\n                                                      Iterate quickly    Phase 1  Phase 2    Phase 3  Phase 4    Phase 5  Phase 6\n\n                   DEFINE                     PROTOYPE     UNDERSTAND  DEFINE    SKETCH  DECIDE    PROTOTYPE  VALIDATE\n    Personas\n    Role objectives\n    \u2022Decisions                                             TesT\n    Challenges\n    \u2022 Pain Points                    Understand impediments\n                                     What works?\n    https://dschool.stanford.edu     Role play\n                                     Iterate quickly\n                                                                                                              8\n---\nImportance of involving stakeholders\n\nExpectation management\n\u2022  Realistic expectations\n\u2022  No surprises, no disappointments\n\u2022  Timely training\n\u2022  Communication, but no hype\n\nOwnership\n\u2022  Make the users active stakeholders\n\u2022  More likely to forgive or accept problems\n\u2022  Can make a big difference in acceptance and success of product\n\n9\n---\nDiscover: understanding users\u2019 needs\n\n                                    THE UX DESIGNER PARADOX\n\n                      WHAT WE DREAM           WHAT WE SETTLE    WHAT THE\n                      UP AT KICKOFF           FOR AT LAUNCH     USER NEEDS\n\n                                    LONG RANGE\n                                    SUPERSONIC\n ... so you don\u2019t     TITANIUM-     ANTENNA\n                      PLATED\n      end up          NOSE\n                      CONE          ONE WAY                     ANTENNA\n    with this                       MIRRO\n               COMMEMORATIVE        VIEWPORT          NOSE      4\n               CUSTOM               PLUTONIUM-        CONE\n DISCOVER      ARTWORK              FUELED\n                                    TWIN SIDE\n User needs    TITANIUM             BOOSTERS                     FINS    BIKE\n               FINS(x4)                    NICKEL-    SINGLE                 RAMP\n                                    PLAPED            BEOSPER\n                                           RIVETS\n\n                                                                             BONUS 2015\n\n                                                                             10\n---\nDegrees of stakeholder involvement\n\n\u2022  Member of the design team\n\u2022  Small group or individual activities\n\u2022  Online contributions from thousands of users\n    \u25aa  Online Feedback Exchange (OFE) systems\n    \u25aa  Crowdsourcing design ideas\n    \u25aa  Citizen engagement\n\u2022  Participatory design\n\u2022  User involvement after product release\n    \u25aa  A/B testing\n    \u25aa  Customer reviews\n\n11\n---\nHow do needs and requirements interact?\n\n              1. To understand as much as possible about the users,\n              their activities, and the context of that activity, so the\nEMPATHISE     system under development can support them in achieving\n              their goals\n\nDEFINE     2. To produce a set of initial requirements that form a\n           sound basis to start designing\n\n12\n---\n              1. To understand as much as possible about the users,\n              their activities, and the context of that activity, so the\nEMPATHISE     system under development can support them in achieving\n              their goals\n\nwhat are requirements?\n\n           A requirement is a statement about an intended product that specifies what it should\n           do or how it should perform. One of the aims of the requirements activity is to make\nDEFINE     the requirements as specific, unambiguous, and clear as possible.\n\n                      (from Interaction Design: Beyond Human-\n                      Computer Interaction 4th ed, p353)\n\n                                                                            13\n---\na need\ntakes a user's perspective (of the problem to be solved)\n\nand describes constraints, goals, hopes,\nand activities performed by the user\n\nis often contextual, embedded,                          focusing and selection\nand social                                              translation to solutions\n                                                        and { partial } commitment to approach\n\n                                                         a requirement\n                                                         refers to properties of the system (to solve the problem)\n\n                                                         and describes what operations the system\n                                                         needs to afford, or what characteristics\n                                                         and properties it needs to have, and\n                                                         how it should operate\n\n                                                         usually embodies or at least partially commits to one\n                                                         particular solution, philosophy or\n                                                         approach to addressing needs\n\n                                                         there may be many strategies approaches to address a\n                                                         user's need\n\n                                                                                                                  14\n---\nHow do we { elicit, discover, understand } needs?\n\n \u00a9 MAZK ANDEZSON  Can you list all    WWW.ANDEIZTOONS.COM\n                  your needs please\n\n JDoSl\n\n 15\n---\nHow do we { elicit, discover, understand } users' needs?\n\nIf I had asked the people what they\nwanted, they would have said\nfaster horses. -Henry Ford\n\n16\n---\nWays of identifying stakeholder needs:\n                                                                    THIRD EDITION\n                                                                    Designing Interactive\nObservation (direct / indirect)                                    Systems\n                                                                   A comprehensive guide to HCI, UX and interaction design\nInterviews\n\nQuestionnaires\n\n(Digital) ethnography\n\nTechnology probes\n\nParticipant Observation (Field studies as a Participant Observer)   David Benyon\n\nCritical Design                                                    WTELE         PEARSON\n                                                                   Chapter 7\n---\nDEFINE\n\nMoving from individual observations to generalised insights\n\n... that help us focus on the \u2018right\u2019 design problem\n\n18\n---\nWhat does this child need?\n\n   19\n---\n    Sasha Costanza-Chock @schock \u00b7 5 Nis 2023\n\n    People working on AI\n\n People most likely\nto be harmed by the\nways companies and\ngovernments use Al\n\n    20\n---\n Stakeholder involvement in\n developing Human-AI systems\n\nBusiness    Data             AI    Al testing and    Deployment\n  case    preparation    modelling    evaluation\n\nWhy is Al needed?    Collecting data      Dataset    Dataset    Integration with\n                                                                complex system\n   What can be                          0\n    improved?                           0 o\n Cleaning and                           0 Model design    System verification    User uses the\nlabelling data                                            and validation         system\n\n   Beatrice Vincenzi, Simone Stumpf, Alex S. Taylor, and Yuri Nakao. 2024. Lay User Involvement in Developing Human -centric Responsible\n   AI Systems: When and How? ACM J. Responsib. Comput. 1, 2, Article 14 (June 2024), 25 pages. https://doi.org/10.1145/3652592          21\n---\n                                           Business        Data               AI            Al testing and         Deployment\n  Early in the process                     case        preparation        modelling         evaluation             Integration with\n     Why is Al needed?                                 Collecting data      Dataset         Dataset                complex system\n        What can be                                                       a\n         improved?                                                        0 o\n                                                       Cleaning and       0 Model design    System verification    User uses the\n                                                       labelling data                       and validation         system\n\n  \u2022  Consider if the problem you're solving\n     requires an AI solution\n  \u2022  Decide if AI adds unique value\n\n  \u2022  Balance a people-first approach and a\n     technology-first approach\n\nhttps://pair.withgoogle.com/guidebook-v3/chapters/user-needs-and-defining-success    22\n---\n                            Business        Data               AI            Al testing and         Deployment\n    Early in the process    case        preparation        modelling         evaluation             Integration with\nWhy is Al needed?                       Collecting data      Dataset         Dataset                complex system\n   What can be                                             a\n    improved?                                              0 o\n                                        Cleaning and       0 Model design    System verification    User uses the\n                                        labelling data                       and validation         system\n\n    When AI is probably better                                          When AI is probably not better\n    \u2022     Recommending different content to different users             \u2022  Maintaining predictability\n    \u2022     Prediction of future events                                   \u2022  Providing static or limited information\n    \u2022     Personalization improves the user experience                  \u2022  Minimizing costly errors.\n    \u2022     Natural language understanding.                               \u2022  Complete transparency\n    \u2022     Recognition of an entire class of entities.\n    \u2022     Detection of low occurrence events that change over time.     \u2022  Optimizing for high speed and low cost.\n    \u2022     An agent or bot experience for a particular domain.           \u2022  Automating high-value tasks.\n    \u2022     Showing dynamic content is more efficient than a\n          predictable interface.\n\n  https://pair.withgoogle.com/guidebook-v3/chapters/user-needs-and-defining-success    23\n---\n                                                  THIRD EDITION\nObservation (direct / indirect)                   Designing Interactive\n \u2022     How was this work done before AI?         Systems\n                                                 A comprehensive guide to HCI, UX and interaction design\nInterviews\n \u2022     What are people\u2019s mental models about the\n       models/processes under consideration?\n\nQuestionnaires\n\n(Digital) ethnography\n \u2022     What perceptions do people have of similar\n       products?\n \u2022     How was this work done before AI?          David Benyon\n                                                 WTELE         PEARSON\n                                                 Chapter 7\n---\n   2. Al suitability\n\n    domain    or input\n\n    25\n  https://pair.withgoogle.com/guidebook-v3/chapters/user-needs-and-defining-success\n\n  Google\nGoogle    People+AI\n---\n Data Preparation\n\n    \u2022  design new interfaces for lay users to\n       interact, understand, and manipulate\n       datasets and models\n\nBusiness    Data             AI    Al testing and    Deployment\n  case    preparation    modelling    evaluation\n\n    Why is Al needed?    Collecting data      Dataset    Dataset    Integration with\n                                                                    complex system\nWhat can be                                 II\n improved?                                  0 -                     Bo\n Cleaning and                               0 Model design    System verification    User uses the\nlabelling data                                                and validation         system\n\n  https://pair.withgoogle.com/guidebook-v3/chapters/user-needs-and-defining-success    26\n---\nData Preparation\n\n                                                                                                              56    IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 26, NO. 1, JANUARY 2020\n\n                                                                                                              The What-If Tool: Interactive Probing of Machine Learning Models\n   Use data that applies to different groups of users                                                               James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Vi\u00e9gas, and Jimbo Wilson\n\n   Your training data should reflect the diversity and cultural context of the people who will use it. Use          2\n   tools like Facets and WlT to explore your dataset and better understand its biases. In doing so, note that\n   to properly train your model, you might need to collect data from equal proportions of different user\n   groups that might not exist in equal proportions in the real world. For example, to have speech\n   recognition software work equally on all users in the United States, the training dataset might need to\n   contain 50% of data from non-native English speakers even if they are a minority of the population.\n\n        (a) Confusion matrix of a single binary classifi-    (b) Histogram of age, colored by classification    (c) Two-dimensional histogram of age and sex,\n        cation model, colored by prediction correctness                                                         colored by classification\n                                                             01- 51-65  10-13-- 1-22\n                                                             89         *8  * *\n        Female\n   2                                                                                38-59\n\n   Male\n\n                             80\n\n   ( Small multiples by sex. Each scatterplot          (e)Histograms of performance in a regression    (f) Using images as thumbnails for image\n   shows age vs positive classification score, col-    model that predicts age, faceted into 3 age buck-    datasets\n   ored by classification                              ets\n\n   https://pair-code.github.io/what-if-tool/    27\n---\n  Model building and evaluation\n\n    Allow users to shape the algorithms or e.g. develop\n    interactive machine learning approaches that can take lay\n    user feedback into account\n\nBusiness    Data             AI    Al testing and    Deployment\n  case    preparation    modelling    evaluation\n\n    Why is Al needed?    Collecting data    Dataset    Dataset    Integration with\n                                                                  complex system\n    What can be    II\n    improved?      0                        -                     Bo\n Cleaning and      0                        Model design    System verification    User uses the\nlabelling data                                              and validation         system\n\n    28\n---\n                                                                                                                                                                                                                                                            Why Hockey?\nModel building and evaluation                                                                                                                                                                                                                                         Part 1: Important words\n                                                                                                                                                                                                                                                      This message has more important words about Hockey than\n                                                                                                                                                                                                                                                             about Baseball\n\n                                                                                                                                                                                                                                                      baseball hockey stanley\nInclude Explanations to participants + receive their feedback                                                                                                                                                                                                                                                 , tiger\n\n                                                                                                                                                                                                                                                      The difference makes the computer think this message is 2.3 times more\n                                                                                                                                                                                                                                                      likely to be about Hockey than Baseball.\n\n I                                                                                                            Message Predictor 1.0.5.28868                                                                                                                 AND\n\n                                                                   Move message           Only show predictions                  OFF           SearchStanley         Clear\n                                                                   to folder...                         that just changed\n\n Folders                      Messages in the 'Unknown' folder\n       Unknown                Original                        Subject                    Predicted      Prediction     Re: Octopus in Detroit?                                                            Why Hockey?                                        Part 2: Folder size\n   (1,180 messages)           orderr    Re: Playoff Predictions                            topic       confidence      From: georgeh@gjhsun (George H)\n                              9287                                                         Hockey      99%                                            O                                                                    D\n                              9294      Re: Schedule...                                   Baseball     60%             Harold Zazula <DLMQC@CUNYVM.BITI                                          Part 1: Imp                    ords           The Baseball folder has more messages than the Hockey folder\n                                                                                                                                                                                                           This message has     rtant words\n      cori             hs     9306      Paul Kuryia and Canadian Wor                       Hockey      99%             >I was watching the Detroit-Minnesot:         ht and thought I saw an                     abo Hockey     Baseball\n       Baseball               9308      Re: My Predictions For 1993                       Baseball     64%             >(is there some custom to throw octopus                                   baseball nockey                               Hockey:      7\n              8/8             9312      Re: NHL Team Captains                             Baseball     64%             It is a long standing good luck Redwing's tradition to throw an octopus\n correct predictions          9316      Re: ugliest swing                                 Baseball     63%             onthe eduring a Stanley Cup game They say it dates back '                 stanley tiger\n                              9319      Re: Octopus in Detroit?                            Hockey      67%             at the Olympia when the Wings became the 1st team (I think) to sweep\n      Prediction totals                                                                                                the cup in 8 games. A lot hardet to throw one from Joe Louis seats                                                      Baseball:    8\n      Hockey  278             9339      Sparky Anderson Gets win #2000, Tigers beat A's   Baseball     99%             than from the old Olympia balcony, though.                                The difference makes the computer think this\n                              9347      Re: Goalie masks                                  Baseball     53%             Funniest I ever saw was when some Tiger fans threw one on the field       message is 2.3 times more likely to be about\n      Baseball 917            9362      Re: Young Catchers                                Baseball     82%             during a Detroit/Toronto baseball game .. I was living in California      Hockey than Baseball.\n                                                                                                                       and the folks I was watching with had never heard of hockey and were                                                           The difference makes the computer thinks each Unknown message is 1.1\n Messages containing          9371      Re: Winning Streaks                               Baseball     53%             incredulous when I recognized the octopus BEFORE the camera closeup !!                                                 AND     times more likely to be about Baseball than Hockey.\n       Stanley                9379      Royals                                            Baseball     64%\n Baseball                         9390  Phillies Mailing List?                            Baseball     65%                                                                                                                            Part 2: Folder size\n                                  9410  Reds snap 5-game losing streak: RedReport 4-18    Baseball     98%                                                                                                                 The Baseball folder has more messages than\n  Hockey                      9423      Re: Juggling Dodgers                              Baseball     57%                                                                                                                             the Hockey folder     YIELDS\n                              9424      RCandlestick ark experience on)                   Baseball     99%                                                                                       Hockey:\n      Unknown                 9433      Re: Notes on Jays vs. Indians Series              Baseball     53%                                                                                       Baseball:\n      I                       9434      Re: When did Dodgers move from NY to LA?          Baseball     53%\n                                  9439  Playoff pool                                       Hockey      96%                                                                                       The difference makes the computer thinks each\n                              9441      R: Hockey and the Hispanic community               Hockey      99%                                                                                       Unknown message is 1.1 times more likely to be\n              E 9449 Re: Yoai-isms                                                  Baseball 53%                                 F                                                               about Baseball than Hockey.                                67% probability this message is about Hockey\n                                                                   Important words\n                                                                   These are all of the words the computer used to make its p         more).                                                                                                          Combining 'Important words' and 'Folder size' makes\n                                                                                                                                                                                                                                                      the computer think this message is 2.0 times more likely\n                                                                         20                                                                                                                    Add a new word or phrase                               to be about Hockey than about Baseball.\n                                                                                                                                                                                                        Remove word\n\n                                                                                baseball bill canadian davedavid hockey player players prime stanley stats           tiger  time                 Undo importance adjustment\n\n Figure 1. The EluciDebug prototype. (A) List of folders. (B) List of messages in the selected folder. (C) The selected message.                                                                                                               Figure 2. The Why explanation tells users how features and\n (D) Explanation of the selected message's predicted folder. (E) Overview of which messages contain the selected word. (F) Complete                                                                                                            folder size were used to predict each message's topic. This figure\n list of words the learning system uses to make predictions.                                                                                                                                                                                   is a close-up of Figure 1 part D.\n\nT. Kulesza, M. Burnett, W.-K. Wong, and S. Stumpf, \u2018Principles of Explanatory Debugging to Personalize Interactive Machine Learn ing\u2019, in Proceedings of the 20th International\nConference on Intelligent User Interfaces, Atlanta Georgia USA: ACM, Mar. 2015, pp. 126\u2013\n137. doi: 10.1145/2678025.2701399.\n---\nModel building and evaluation\n\nCo-design/imagine effects with users before deploying\n\n  Leverage participatory or     Consider using participatory approaches from speculative and\n  speculative design            critical design practices to evaluate potential downstream effects\n  methods                       of your Al model. These approaches will provoke discussion\n                                 about the kinds of outcomes you want to enable with your Al\n                                 model, and help you identify risks and behaviors that you want to\n                                 avoid or mitigate. Use these to prioritize short-term UX\n                                 interventions while planning long-term strategies across your\n                                 product's lifecycle.\n\n Q&A: Participatory Machine Learning\n https://medium.com/people-ai-research/participatory-machine-learning-69b77f1e5e23    30\n---\nBusiness    Data    AI    Al testing and    Deployment\n  case    preparation    modelling    evaluation\n\n    Why is Al needed?    Collecting data      Dataset    Dataset    Integration with\nWhat can be                                                         complex system\n improved?                                  0 I\n                         Cleaning and       0 Model design    System verification    User uses the\n                         labelling data                       and validation         system\n\n    Fig. 8. Al system development process.\n\n   Beatrice Vincenzi, Simone Stumpf, Alex S. Taylor, and Yuri Nakao. 2024. Lay User Involvement in Developing Human -centric Responsible\n   AI Systems: When and How? ACM J. Responsib. Comput. 1, 2, Article 14 (June 2024), 25 pages. https://doi.org/10.1145/3652592          31\n---\nParticipation in the Age of Foundation Models\n\n  \u2022  Reinforcement learning with human feedback (RLHF).\n  \u2022  Rulesets and policies\n  \u2022  Red teaming\n  \u2022  Domain-oriented efforts\n\n  H. Suresh, E. Tseng, M. Young, M. Gray, E. Pierson, and K. Levy, \u2018Participation in the age of foundation models\u2019, in The 2024 ACM Conference    32\n  on Fairness Accountability and Transparency, Rio de Janeiro Brazil: ACM, June 2024, pp. 1609\u20131621. doi: 10.1145/3630106.3658992.\n---\n  Participation in the Age of Foundation Models\n\n    RLHF\n    (3.2.1)            e.g., crowd-worker annotations to\n                       improve output quality\n\n    GENERAL RULESETS/\n    GUIDELINES\n    (3.2.2)            e.g., one-time deliberation session with\n                       public stakeholders produce general\n                       guidelines for model behavior\n\n    RED-TEAMING               \u2022\n    (3.2.3)           e.g., adversarial testing to find technical\n\n                      model vulnerabilities to document\n\nDOMAIN-ORIENTED\n    EFFORTS                                                              e.g., stakeholders create\n    (3.2.4)          e.g., interviews with marginalized stakeholders     collaborative licensed dataset and\n                     produce implications for data curation              receive payment for usage\n\n                     CONSULT   INCLUDE                                  COLLABORATE  OWN\n\n    H. Suresh, E. Tseng, M. Young, M. Gray, E. Pierson, and K. Levy, \u2018Participation in the age of foundation models\u2019, in The 2024 ACM Conference    33\n    on Fairness Accountability and Transparency, Rio de Janeiro Brazil: ACM, June 2024, pp. 1609\u20131621. doi: 10.1145/3630106.3658992.\n---\n6CCSAHAI\n\n  Human-centred AI\n  as a Design Process\n  Part 1: Understanding user needs\n  & Defining the right problem\n  Dr Georgia Panagiotidou\n\n  34\n\n"
    },
    "data/raw/Week 17 - All slides in one.pdf": {
        "metadata": {
            "file_name": "Week 17 - All slides in one.pdf",
            "file_type": "pdf",
            "content_length": 8289,
            "language": "en",
            "extraction_timestamp": "2025-11-26T00:18:35.760134+00:00",
            "timezone": "utc"
        },
        "content": "1\n\nVariational autoencoders\n\nLuis C. Garcia Peraza Herrera\n---\n   2\nAgenda\n\n   Latent variable models\n   Nonlinear latent variable model\n   Training\n   ELBO properties\n   Variational approximation\n   The variational autoencoder\n   The reparametrisation trick\n   Applications\n   Conclusion\n---\n3\n\nLatent variable models\n---\n   4\nLatent variable models\n\n   Model a joint distribution P rpx, zq where z is an unobserved latent variable\n\n   Describe the probability of P rpxq as a marginalization of this joint\n   probability so that:\n                       \u017c    \u017c\n   P rpxq \u201c                 P rpx, zqdz \u201c  P rpx|zqP rpzqdz\n---\n  5\nExample: mixture of Gaussians\n\n  In a 1D mixture of Gaussians:\n\n  The likelihood P rpxq is given by the marginalization over the latent variable\n  z:\n---\n   6\nExample: mixture of Gaussians\n---\n  7\n\nNonlinear latent variable model\n---\n  8\nNonlinear latent variable model\n\n  The prior P rpzq is a standard multivariate normal:\n\n  The likelihood P rpx|z, \u03d5q is also normally distributed:\n\n  The data probability P rpx|\u03d5q is found by marginalizing over the latent\n  variable z:\n---\n   9\nNonlinear latent variable model\n---\n   10\n\nGeneration\n\n   Ancestral sampling process:\n   \u201a Draw latent variable \u00d1 sample a latent variable z\u02da from the prior\n   distribution P rpzq:\n                              z\u02da \u201e P rpzq\n   \u201a Compute likelihood mean \u00d1 pass z\u02da through the network f rz\u02da, \u03d5s to\n   compute the mean of the likelihood distribution P rpx | z\u02da, \u03d5q:\n\n                              Mean `P rpx | z\u02da, \u03d5q\u02d8\n   \u201a Draw data sample \u00d1 draw a new example x\u02da from the likelihood distribution\n   based on the computed mean:\n\n                              x\u02da \u201e P rpx | z\u02da, \u03d5q\n---\n   11\nGeneration\n---\n12\n\nTraining\n---\n   13\nTraining\n\n   To train the model, we maximise the log-likelihood over a training dataset\n   txiuI  with respect to the model parameters:\n   i\u201c1\n\n   where:\n---\n   14\nEvidence lower bound (ELBO)\n\n   We define a lower bound on the log-likelihood\n\n   This is a function that is always less than or equal to the log-likelihood for a\n   given value of \u03d5 and will also depend on some other parameters \u03b8\n\n   To define this lower bound, we need Jensen\u2019s inequality\n---\n   15\nJensen\u2019s inequality\n\n   Jensen\u2019s inequality says that a concave function g of the expectation of data\n   y is greater than or equal to the expectation of the function of the data:\n\n   If the concave function is the logarithm:\n---\n   16\nJensen\u2019s inequality: discrete case\n---\n   17\nJensen\u2019s inequality: continuous case\n---\n   18\nDeriving the bound\n\n   We use Jensen\u2019s inequality to derive the lower bound for the log-likelihood:\n---\n   19\nDeriving the bound\n\n   In practice the distribution qpzq has parameters \u03b8, so the ELBO can be\n   written as:\n\n   To learn the nonlinear latent variable model, we maximize this quantity as a\n   function of both \u03d5 and \u03b8\n\n   The neural architecture that computes this quantity is the VAE\n---\n20\n\nELBO properties\n---\n   21\nELBO properties\n\n   The original log-likelihood of the data is a function of the parameters \u03d5\n\n   We want to find its maximum\n\n   Depending on our choice of \u03b8 the lower bound may move closer or further\n   from the log-likelihood\n\n   When we change \u03d5, we move along the lower bound function\n---\n   22\nELBO properties\n---\n                                                                           23\nTightness of bound\n  To find the distribution qpz|\u03b8q that makes the bound tight, we factor the\n  numerator of the log term in the ELBO using the definition of conditional\n  probability:\n\n  The KL distance will be zero and the bound tight when:\n\n  qpz|\u03b8q \u201c P rpz|x, \u03d5q                                  (1)\n---\n   24\nTightness of bound\n---\n                                                  25\nELBO as a reconstruction loss minus KL distance to\nprior\n\n   We have seen two different ways to express the ELBO:\n   1.\n\n   2.\n---\n                                                  26\nELBO as a reconstruction loss minus KL distance to\nprior\n\n   A third way is to consider the bound as reconstruction error minus the\n   distance to the prior:\n---\n27\n\nVariational approximation\n---\n   28\nVariational approximation\n\n   The ELBO is tight when qpz|\u03b8q is the posterior P rpz|x, \u03d5q\n   Cannot use Bayes\u2019 rule because P rpx|\u03d5q \u00d1 intractable\n   One solution: we choose a simple parametric form for qpz|\u03b8q and use this to\n   approximate the true posterior\n   Since the optimal choice for qpz|\u03b8q was the posterior P rpz|xq, and this\n   depends on the data example x, the variational approximation should do\n   the same:\n\n   where grx, \u03b8s is a second neural network with parameters \u03b8 that predicts\n   the mean \u00b5 and variance \u03a3 of the normal variational approximation\n---\n   29\nVariational approximation\n---\n30\n\nThe variational autoencoder\n---\n   31\nThe variational autoencoder\n\n   We build a network that computes the ELBO:\n\n   where the distribution qpz|x, \u03b8q is the approximation:\n\n   The first term involves an intractable integral, we can approximate it by\n   sampliing, for any function a we have:\n\n   where z\u02da is the n-th sample from qpz|x, \u03b8q\n   n\n---\n   32\nThe variational autoencoder\n\n   For a very approximate estimate, we can just use a single sample z\u02da from\n   qpz|x, \u03b8q:\n\n   The second term is the KL divergence between the variational distribution\n   qpz|x, \u03b8q \u201c Normz r\u00b5, \u03a3s and the prior P rpzq \u201c Normz r0, Is:\n\n   where Dz is the dimensionality of the latent space\n---\n   33\n\nVAE algorithm\n\n   We aim to build a model that computes the evidence lower bound for a\n   point x\n   We use an optimization algorithm to maximize this lower bound over the\n   dataset and hence improve the log-likelihood\n   To compute the ELBO we:\n    \u201a Compute the mean \u00b5 and variance \u03a3 of the variational posterior distribution\n    qpz|\u03b8, xq for this data point x using the network gpx, \u03b8q\n    \u201a Draw a sample z\u02da from this distribution\n    \u201a Compute the ELBO using:\n---\n   34\nVAE algorithm\n---\n   35\nVAE algorithm\n---\n36\n\nThe reparametrisation trick\n---\n   37\nThe reparametrisation trick\n\n   The network involves a sampling step that is difficult to differentiate\n   We can move the stochastic part into a branch of the network that draws a\n   sample \u03f5\u02da from Norm\u03f5r0, Is and then use the following relation to draw\n   from the Gaussian:\n                     z\u02da \u201c \u00b5 ` \u03a31{2\u03f5\u02da                                      (2)\n---\n   38\nThe reparametrisation trick\n---\n39\n\nApplications\n---\n   40\nApproximating sample probability\n\n   The VAE describes the probability of a sample as:\n\n   In principle we could approximate this probability by drawing samples from\n   P rpzq \u201c Normz r0, Is and computing:\n\n   Bad news: huge number of samples to get a reliable estimate\n---\n   41\nApproximating sample probability\n\n   A better approach is to use importance sampling:\n\n   where now we draw samples from qpzq\n---\n   42\nGeneration\n\n   Samples from vanilla VAEs are generally low-quality:\n    \u201a Naive spherical Gaussian noise model\n    \u201a Gaussian models used for prior and variational posterior\n\n   To improve generation quality, sample from the aggregated posterior:\n\n    qpz|\u03b8q \u201c 1 \u00ff qpz|xi, \u03b8q                                            (3)\n    I  i\n---\n   43\nGeneration\n---\n   44\n\nResynthesis\n\n   VAEs can also be used to modify real data:\n\n   1. Taking the mean of the distribution predicted by the encoder\n   2. Solving an optimisation problem to find the latent variable z that maximises\n   the posterior probability\n---\n   45\nResynthesis\n---\n   46\nDisentanglement\n\n   When each dimension of z represents an independent real-world factor, the\n   latent space is described as disentangled\n\n   To encourage disentanglement:\n\n   The beta VAE upweights the second term in the ELBO:\n---\n   47\nDisentanglement\n---\n48\n\nConclusion\n---\n   49\n\nConclusion\n\n   VAEs learn a nonlinear latent variable model over x\n   Generation process:\n    \u201a Sample from the latent variable\n    \u201a Pass the result through a deep network\n    \u201a Add independent Gaussian noise\n\n   Challenges:\n    \u201a Not possible to compute likelihood of a data point in closed form\n    \u201a Computing the posterior probability of the latent variable given observed\n    data is intractable\n\n   Solution: variational approximation\n\n   Enhancement: sophisticated latent space modeling (e.g. hierarchical priors)\n---\n   50\nThanks!\n\n"
    },
    "data/raw/vector25aug-v2.pdf": {
        "metadata": {
            "file_name": "vector25aug-v2.pdf",
            "file_type": "pdf",
            "content_length": 68177,
            "language": "en",
            "extraction_timestamp": "2025-11-26T00:21:29.002952+00:00",
            "timezone": "utc"
        },
        "content": "Vector     Word Meaning\nSemantics &\nEmbeddings\n---\nWhat do words mean?\n\nN-gram or text classification methods we've seen so far\n\u25e6  Words are just strings (or indices wi in a vocabulary list)\n\u25e6  That's not very satisfactory!\nIntroductory logic classes:\n\u25e6  The meaning of \"dog\" is DOG; cat is CAT\n   \u2200x DOG(x) \u27f6 MAMMAL(x)\nOld linguistics joke by Barbara Partee in 1967:\n\u25e6  Q: What's the meaning of life?\n\u25e6  A: LIFE\nThat seems hardly better!\n---\nDesiderata\n\nWhat should a theory of word meaning do for us?\nLet's look at some desiderata\nFrom lexical semantics, the linguistic study of word\nmeaning\n---\n   Lemmas and senses\n          lemma\n          mouse (N)\n\nsense     1. any of numerous small rodents...\n          2. a hand-operated device that controls\n          a cursor...    Modified from the online thesaurus WordNet\n\n  A sense or \u201cconcept \u201d is the meaning component of a word\n  Lemmas can be polysemous (have multiple senses)\n---\nRelations between senses: Synonymy\n\nSynonyms have the same meaning in some or all\ncontexts.\n\u25e6  filbert / hazelnut\n\u25e6  couch / sofa\n\u25e6  big / large\n\u25e6  automobile / car\n\u25e6  vomit / throw up\n\u25e6  water / H\u20820\n---\nRelations between senses: Synonymy\n\nNote that there are probably no examples of perfect\nsynonymy.\n\u25e6  Even if many aspects of meaning are identical\n\u25e6  Still may differ based on politeness, slang, register, genre,\n   etc.\n---\nRelation: Synonymy?\n\nwater/H\u20820\n\"H\u20820\" in a surfing guide?\nbig/large\nmy big sister != my large sister\n---\nThe Linguistic Principle of Contrast\n\nDifference in form \u00e0 difference in meaning\n---\nAbb\u00e9 Gabriel Girard 1718                       LA' JUSTESSE\n                                               DE LA\nRe: \"exact\" synonyms                           LANGUE FRANCOISE.\n                                               o v\n\"jc ne crois pas qu'il y ait de                LES DIFFERENTES SIGNIFICATIONS\n                                               DES MOTS QUI PASSENT\nmot fynonime dans aucune                       POU.R\n \"                                             SYNONIMES\nLangue. Je le dis par con-                     PAr M.PAbb\u00c9 GIRARD C.D.M.D.D.B.\n [I do not believe that there      SPIRAT\n is a synonymous word in any\n language]                         A PARIS,\n                                   CheZ I.AURENT D'HOURY, IMprimeur-\n Lbraire, au bas de la rue de la Harpe,vis-\n d vis la rue S. Severin, au Saint Efprit.\n                                               M DCC.XVIII.\n                                   Avce Approbason & Frivilegs dus Roy.\n                        Thanks to Mark Aronoff!\n---\nRelation: Similarity\n\nWords with similar meanings. Not synonyms, but sharing\nsome element of meaning\n\ncar,  bicycle\ncow,  horse\n---\nAsk humans how similar 2 words are\n\nword1      word2          similarity\nvanish     disappear      9.8\nbehave     obey           7.3\nbelief     impression     5.95\nmuscle     bone           3.65\nmodest     flexible       0.98\nhole       agreement      0.3\n\n          SimLex-999 dataset (Hill et al., 2015)\n---\n    Relation: Word relatedness\n\nAlso called \"word association\"\nWords can be related in any way, perhaps via a semantic\nframe or field\n\n \u25e6  coffee, tea: similar\n \u25e6  coffee, cup: related, not similar\n---\nSemantic field\n\nWords that\n\u25e6  cover a particular semantic domain\n\u25e6  bear structured relations with each other.\n\n hospitals\n   surgeon, scalpel, nurse, anaesthetic, hospital\n restaurants\n   waiter, menu, plate, food, menu, chef\n houses\n   door, roof, kitchen, family, bed\n---\nRelation: Antonymy\n\nSenses that are opposites with respect to only one\nfeature of meaning\nOtherwise, they are very similar!\n    dark/light      short/long fast/slow  rise/fall\n    hot/cold        up/down      in/out\nMore formally: antonyms can\n \u25e6    define a binary opposition or be at opposite ends of a scale\n   \u25e6  long/short, fast/slow\n \u25e6    Be reversives:\n   \u25e6  rise/fall, up/down\n---\nConnotation (sentiment)\n\n\u2022 Words have affective meanings\n  \u2022     Positive connotations (happy)\n  \u2022     Negative connotations (sad)\n\u2022 Connotations can be subtle:\n  \u2022     Positive connotation: copy, replica, reproduction\n  \u2022     Negative connotation: fake, knockoff, forgery\n\u2022 Evaluation (sentiment!)\n  \u2022     Positive evaluation (great, love)\n  \u2022     Negative evaluation (terrible, hate)\n---\nConnotation\n                                          Osgood et al. (1957)\nWords seem to vary along 3 affective dimensions:\n\u25e6  valence: the pleasantness of the stimulus\n\u25e6  arousal: the intensity of emotion provoked by the stimulus\n\u25e6  dominance: the degree of control exerted by the stimulus\n\n                  Word          Score      Word         Score\n    Valence       love           1.000     toxic              0.008\n                  happy          1.000     nightmare          0.005\n    Arousal       elated         0.960     mellow             0.069\n                  frenzy         0.965     napping            0.046\n    Dominance     powerful       0.991     weak               0.045\n                  leadership     0.983     empty              0.081\n\n                                           Values from NRC VAD Lexicon (Mohammad 2018)\n---\nSo far\n\nConcepts or word senses\n\u25e6  Have a complex many-to-many association with words (homonymy,\n   multiple senses)\nHave relations with each other\n\u25e6  Synonymy\n\u25e6  Antonymy\n\u25e6  Similarity\n\u25e6  Relatedness\n\u25e6  Connotation\n---\nVector     Word Meaning\nSemantics &\nEmbeddings\n---\nVector     Vector Semantics\nSemantics &\nEmbeddings\n---\nComputational models of word meaning\n\nCan we build a theory of how to represent word\nmeaning, that accounts for at least some of the\ndesiderata?\nWe'll introduce vector semantics\n The standard model in language processing!\n Handles many of our goals!\n---\nLudwig Wittgenstein\n\nPI #43:\n\"The meaning of a word is its use in the language\"\n---\nLet's define words by their usages\n\nOne way to define \"usage\":\nwords are defined by their environments (the words around them)\n\nZellig Harris (1954):\nIf A and B have almost identical environments we say that they\nare synonyms.\n---\nWhat does recent English borrowing ongchoi mean?\n\nSuppose you see these sentences:\n    \u2022 Ong choi is delicious saut\u00e9ed with garlic.\n    \u2022 Ong choi is superb over rice\n    \u2022 Ong choi leaves with salty sauces\nAnd you've also seen these:\n    \u2022  \u2026spinach saut\u00e9ed with garlic over rice\n    \u2022  Chard stems and leaves are delicious\n    \u2022  Collard greens and other salty leafy greens\nConclusion:\n\u25e6 Ongchoi is a leafy green like spinach, chard, or collard greens\n  \u25e6 We could conclude this based on words like \"leaves\" and \"delicious\" and \"sauteed\"\n---\nOngchoi: Ipomoea aquatica \"Water Spinach\"\n\n  \u7a7a\u5fc3\u83dc\n  kangkong\n  rau mu\u1ed1ng\n  \u2026\n\n  Yamaguchi, Wikimedia Commons, public domain\n---\nIdea 1: Defining meaning by linguistic distribution\n\nLet's define the meaning of a word by its\ndistribution in language use, meaning its\nneighboring words or grammatical environments.\n---\nIdea 2: Meaning as a point in space (Osgood et al. 1957)\n3 affective dimensions for a word\n\u25e6    valence: pleasantness\n\u25e6    arousal: intensity of emotion\n\u25e6    dominance: the degree of control exerted\n              Word          Score      Word         Score\nValence       love           1.000     toxic             0.008\n              happy          1.000     nightmare         0.005\nArousal       elated         0.960     mellow            0.069  NRC VAD Lexicon\n              frenzy         0.965     napping           0.046  (Mohammad 2018)\nDominance     powerful       0.991     weak              0.045\n\u25e6             leadership     0.983     empty             0.081\nHence the connotation of a word is a vector in 3-space\n---\nIdea 1: Defining meaning by linguistic distribution\n\nIdea 2: Meaning as a point in multidimensional space\n---\nDefining meaning as a point in space based on distribution\n?\nEach word = a vector (not just \"good\" or \"w\u2084\u2085\")\nSimilar words are \"nearby in semantic space\"   drinks\nWe build this space automatically by           alcoholic\n                                         seeing which words are\nnearby in text       candy chocolate           cider\n\n                     cream\n                                         juice\n                     0     honey                    wine\n\n                     0  corn  rice\n\n \u2022                                       beef  fried  soup drink\n                                         potatoes\n                              wheat      foods   pork   cooking\n\n                                         vegetablesbread\n---\nWe define meaning of a word as a vector\n\nCalled an \"embedding\" because it's embedded into a\nspace (see textbook)\nThe standard way to represent meaning in NLP\n Every modern NLP algorithm uses embeddings as\n the representation of word meaning\nFine-grained model of meaning for similarity\n---\nIntuition: why vectors?\n\nConsider sentiment analysis:\n\u25e6   With words, a feature is a word identity\n  \u25e6  Feature 5: 'The previous word was \"terrible\"'\n  \u25e6  requires exact same word to be in training and test\n\u25e6   With embeddings:\n  \u25e6  Feature is a word vector\n  \u25e6  'The previous word was vector [35,22,17\u2026]\n  \u25e6  Now in the test set we might see a similar vector [34,21,14]\n  \u25e6  We can generalize to similar but unseen words!!!\n---\nWe'll discuss 2 kinds of embeddings\n\nSimple count embeddings\n\u25e6  Sparse vectors\n\u25e6  Words are represented by the counts of nearby words\n\nWord2vec\n\u25e6  Dense vectors\n\u25e6  Representation is created by training a classifier to predict whether a\n   word is likely to appear nearby\n\u25e6  Later we'll discuss extensions called contextual embeddings\n---\n    From now on:\n    Computing with meaning representations\n    Vector Semantics and\n    instead of string representations\n    Embeddings\n\n   C\u8005@\u00c2(|\uff0c\u00f3|\u800c\u00ffC Nets are for fish;\n                Once you get the fish, you can forget the net.\n   \u8a00\u8005@\u00c2(\u270f\uff0c\u00f3\u270f\u800c\u00ff\u8a00 Words are for meaning;\n                Once you get the meaning, you can forget the words\n                                      \u00d1P(Zhuangzi), Chapter 26\n\n         The asphalt that Los Angeles is famous for occurs mainly on its freeways. But\n in the middle of the city is another patch of asphalt, the La Brea tar pits, and this\nasphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-\n---\nVector     Vector Semantics\nSemantics &\nEmbeddings\n---\nVector     Count-based embeddings\nSemantics &\nEmbeddings\n---\n Remember the intuition: words with similar\n neighborhoods have similar meanings\nHow to measure a word's neighborhood?\nWord-context matrix (a kind of co-occurrence matrix)\n  \u25e6  each row represents a word in the vocabulary\n  \u25e6  each column represents how often each other word in the\n     vocabulary appears nearby\n---\nWord-Context Matrix\n\n            aardvark\n            abacus    zydeco\n            adept\n                    affect\n                      agate     How often does\n\naardvark                  \u2026     agate occur\nabacus                          near abacus?\nadept\naffect\nagate\n\u2026\n\nzydeco\n---\nWord-Context Matrix\nWhat does \"nearby\" mean?\nFor right now let's say \"within 4 words\"\n---\n most common, however, to use smaller contexts, generally a window around the\nThe word-context matrix\n word, for example of 4 words to the left and 4 words to the right, in which case\n the cell represents the number of times (in some training corpus) the column word\nOne set of 4-word contexts\n occurs in such a \u00b14 word window around the row word. For example here is one\n example each of some words in their windows:\n\n is traditionally followed by       cherry       pie, a traditional dessert\n often mixed, such as               strawberry   rhubarb pie. Apple pie\n computer peripherals and personal  digital      assistants. These devices usually\n a computer. This includes          information  available on the internet\n If we then take every occurrence of each word (say strawberry) and count the con-\n text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n simplified subset of the word-word co-occurrence matrix for these four words com-\n puted from the Wikipedia corpus (Davies, 2015).\n Note in Fig. 6.5 that the two words cherry and strawberry are more similar to\n each other (both pie and sugar tend to occur in their window) than they are to other\n words like digital; conversely, digital and information are more similar to each other\n than, say, to strawberry. Fig. 6.6 shows a spatial visualization.\n---\nLet's consider a mini-matrix of 3 words.\n most common, however, to use smaller contexts, generally a window around the\n word, for example of 4 words to the left and 4 words to the right, in which case\nHow often do \"a\", \"computer\", and \"pie\n the cell represents the number of times (in some training corpus) the column word\noccur in the context of \"cherry\"?\n occurs in such a \u00b14 word window around the row word. For example here is one\n example each of some words in their windows:\n\n is traditionally followed by       cherry       pie, a traditional dessert\n often mixed, such as               strawberry   rhubarb pie. Apple pie\n computer peripherals and personal  digital      assistants. These devices usually\n a computer. This includes          information  available on the internet\n If we then take every occurrence of each word (say strawberry) and count the con-\n text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n simplified subset of the word-word co-occurrence matrix for these four words com-\n puted from the Wikipedia corpus (Davies, 2015).\n Note in Fig. 6.5 that the two words cherry and strawberry are more similar to\n each other (both pie and sugar tend to occur in their window) than they are to other\n words like digital; conversely, digital and information are more similar to each other\n than, say, to strawberry. Fig. 6.6 shows a spatial visualization.\n---\n  most common, however, to use smaller contexts, generally a window around the\n The word-          5.3             \u2022    S IMPLE COUNT- BASED EMBEDDINGS              7\n               context mini-matrix for just 4 words\n  word, for example of 4 words to the left and 4 words to the right, in which case\n  the cell represents the number of times (in some training corpus) the column word\n and 3 contexts\ncontext co-occurrence matrix is very large, because for each word in the vocabulary\n  occurs in such a \u00b14 word window around the row word. For example here is one\n(since |V |) we have to count how often it occurs with every other word in the vo-\n  example each of some words in their windows:\ncabulary, hence dimensionality |V | \u21e5 |V |. Let\u2019s therefore instead sketch the process\n               is traditionally followed by  cherry         pie, a traditional dessert\non a smaller scale. Imagine that we are going to look at only the 4 words, and only\nconsider       often mixed, such as          strawberry     rhubarb pie. Apple pie\n             the following 3 context words:     a, computer, and pie.       Furthermore let\u2019s\nassume computer peripherals and personal     digital        assistants. These devices usually\n we only count occurrences in the mini-corpus above.\n So before     a computer. This includes     information    available on the internet\n  If           looking at Fig. 5.2, compute by hand the counts for these 3 context\nwords for we then take every occurrence of each word (say strawberry) and count the con-\n             the four words cherry, strawberry, digital, and information.\n  text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n  simplified subset of the word-word co-occurrence matrix for these four words com-\n  puted from   a                                computer                    pie\n cherry        the Wikipedia corpus (Davies, 2015).\n               Note in Fig. 6.51that the two words     0                    1\nstrawberry     0                                      cherry and strawberry are more similar to\n  each other (both pie and sugar                       0                    2\n digital       0                    tend to occur in their window) than they are to other\n  words like digital; conversely, digital and          1                    0\ninformation    1                                      information are more similar to each other\n  than, say, to strawberry. Fig. 6.6 shows a 1                              0\nFigure 5.2     Co-occurrence vectors for four         spatial visualization.\n                                                words with counts from the 4 windows above,\n---\n         consider the following 3 context words: a, computer, and pie. Furthermore let\u2019s\n         assume we only count occurrences in the mini-corpus above.\n        The word-context mini-matrix for just 4 words\n          So before looking at Fig. 5.2, compute by hand the counts for these 3 context\n        and 3 contexts\n         words for the four words cherry, strawberry, digital, and information.\n\n                         a                       computer              pie\n         cherry          1                          0                  1\n         strawberry      0                          0                  2\n          digital        0                          1                  0\n         information     1                          1                  0\n\n        \u2022 Figure 5.2    Co-occurrence vectors for four words with counts from the 4 windows above,\n          This 4x3 matrix is a subset of full |V| x |V| matrix\n         showing just 3 of the potential context word dimensions. The vector for cherry is outlined in\n\n        \u2022 red. Note that a real vector would have vastly more dimensions and thus be even sparser.\n          Each word is represented by a row vector with\n          Hopefully your count matches what is shown in Fig. 5.2, so that each cell repre-\n          dimensionality [1 x |V|]\n         sents the number of times a particular word (defined by the row) occurs in a partic-\n        \u2022 ular context (defined by the word column).\n          With co-occurrence counts with each other word\n          Each row, then, is a vector representing a word.   To review some basic linear\nctor     algebra, a vector is, at heart, just a list or array of numbers. So cherry is represented\n---\n    most common, however, to use smaller contexts, generally a window around the\nhere is one example each of some words in their windows:\n    word, for example of 4 words to the left and 4 words to the right, in which case\n      is traditionally followed by      cherry           pie, a traditional dessert\n    the cell represents the number of times (in some training corpus) the column word\nA                  often mixed, such as strawberry       rhubarb pie. Apple pie\nselection from a larger word-context matrix\n    occurs in such a \u00b14 word window around the row word. For example here is one\ncomputer peripherals and personal       digital          assistants. These devices usually\n    example each of some words in their windows:\n            a computer. This includes   information      available on the internet\n      If we  is traditionally followed by cherry         pie, a traditional dessert\n            then take every occurrence of each word (say strawberry) and count the\ncontext            often mixed, such as   strawberry     rhubarb pie. Apple pie\n            words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a\n      computer peripherals and personal   digital        assistants. These devices usually\nsimplified subset of the word-word co-occurrence matrix for these four words com-\n             a computer. This includes    information    available on the internet\nputed from the Wikipedia corpus (Davies, 2015).\n    If we then take every occurrence of each word (say strawberry) and count the con-\n    text words around it, we get a word-word co-occurrence matrix. Fig. 6.5 shows a\n    simplified     aardvark     ...    computer    data  result        pie    sugar     ...\n       cherry     subset of the word-word co-occurrence matrix for these four words com-\n    puted from    0             ...       2         8     9            442     25       ...\n     strawberry    the Wikipedia corpus (Davies, 2015).\n     Note in Fig. 0             ...       0         0     1            60      19       ...\n      digital      6.5 that the two words cherry and strawberry are more similar to\n    each other (both 0          ...     1670       1683  85            5       4        ...\n    information    pie and sugar tend to occur in their window) than they are to other\n    words like    0             ...     3325       3982  378           5       13       ...\nFigure 6.6        digital; conversely, digital and information are more similar to each other\n    than,    Co-occurrence vectors for four words in the Wikipedia corpus, showing six of\nthe         say, to strawberry. Fig. 6.6 shows a spatial visualization.\n      dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\nred. Note that a real vector would have vastly more dimensions and thus be much sparser.\n---\ncomputer\n\n4000\n        information\n3000    [3982,3325]\n        digital\n2000    [1683,1670]\n\n1000\n\n1000 2000 3000 4000\ndata\n---\nThe word-context matrix\n\nWord context matrix is |V| x |V|\nThis could be 50,000 x 50,000\nMost of these numbers are zero!\nSo these are sparse vectors\nThere are efficient algorithms for storing and\ncomputing with sparse matrices\n---\nVector     Count-based embeddings\nSemantics &\nEmbeddings\n---\n           Cosine for computing word similarity\nVector\nSemantics &\nEmbeddings\n---\nhence of length |V |, or both with documents as dimensions as documents, of length\n|D|) and gives a measure of their similarity. By far the most common similarity\n Computing word similarity: Dot product and cosine\nmetric is the cosine of the angle between the vectors.\n The cosine\u2014like most measures for vector similarity used in NLP\u2014is based on\nthe dot product operator from linear algebra, also called the inner product:\n The dot product between two vectors is a scalar:\n N\n dot product(v, w) = v \u00b7 w = X vi wi = v1 w1 + v2 w2 + ... + vN wN          (6.7)\n i=1\n The dot product tends to be high when the two\nAs we will see, most metrics for similarity between vectors are based on the dot\n vectors have large values in the same dimensions\nproduct. The dot product acts as a similarity metric because it will tend to be high\njust when the two vectors have large values in the same dimensions. Alternatively,\n Dot product can thus be a useful similarity metric\nvectors that have zeros in different dimensions\u2014orthogonal vectors\u2014will have a\n between vectors\ndot product of 0, representing their strong dissimilarity.\n This raw dot product, however, has a problem as a similarity metric: it favors\nlong vectors. The vector length is defined as\n---\n  ill see, most metrics for similarity between vectors are based on the dot\nt. The dot product acts as a similarity metric because it will tend to be high\n           Problem with raw dot-product\n  n the two vectors have large values in the same dimensions. Alternatively,\n  that have zeros in different dimensions\u2014orthogonal vectors\u2014will have a\n  uct of Dot product favors long vectors\n           0, representing their strong dissimilarity.\n  s raw dot product, however, has a problem as a similarity metric: it favors\n           Dot product is higher if a vector is longer (has higher\n  ctors. The vector length is defined as\n           values in many dimension)\n           Vector length:        v\n                                 u N\n                                 uX\n                             |v| = t    v2                        (6.8)\n                                        i\n                                     i=1\n           Frequent words (of, the, you) have long vectors (since\n product is higher if a vector is longer, with higher values in each dimension.\n           they occur many times with other words).\n equent words have longer vectors, since they tend to co-occur with more\n  nd have higher co-occurrence values with each of them. The raw dot product\n  l be     So dot product overly favors frequent words\n           higher for frequent words. But this is a problem; we\u2019d like a similarity\n---\n             This raw dot product, however, has a problem as a similarity metric: it favors\nctor length  long vectors. The vector length is defined as\n  The cosine similarity metric between two vectors ~\n                                                                      v and ~\n    Alternative: cosine for                                            w thus can be computed\n                                                  computing word similarity\n :                                                          v\n                                                            u N\n                                                            uX\n                                                  |v| = t             v2            (6.8)\n                                                                      iN\n\n             The dot product is higher if                    i=1 X vi wi\n                                              a vector is longer, with higher values in each dimension.\n                                              ~\n                                              v \u00b7 ~\n               cosine(                            w          v        i=1 v\n                                ~\n                                v, ~\n                                 w) =                  =                            (6.10)\n             More frequent words have longer vectors, since they tend to co-occur with more\n                                              |              u              u\n                                              ~\n                                                   v||~\n             words and have                        w|           N            N\n                                higher co-occurrence values with each of them. The raw dot product\n                                                             uX uX\n             thus will be higher for frequent                t              2 t  2\n                                                   words. But this is a problem; we\u2019d like a similarity\n             metric that tells us how similar two words are           vi         wi\n                                                                regardless of their frequency.\n             We modify the dot                                  i=1              i=1\n                                   product to normalize for the vector length by dividing the\n      For some applications we pre-normalize each vector, by dividing it by its length,\n             dot product by the lengths of each of the two vectors. This normalized dot product\neating a     turns out to be the same as the cosine of the angle between the two vectors, following\n             unit vector of length 1. Thus we could compute a unit vector from ~\n               Based on the definition of the dot product between two vectors a                        a by\n             from the definition of the dot product between two vectors a and b:              and b\nviding it by |~\n             a|. For unit vectors, the dot product is the same as the cosine.\n      The cosine value ranges from 1 for vectors pointing in the same direction, through\n                                                   a \u00b7 b = |a||b| cos q\n or vectors that are orthogonal, to -1 for vectors pointing in opposite directions.\n                                                   |a \u00b7 b  = cos q                            (6.9)\nut raw frequency values are                        a||b|\n                                   non-negative, so the cosine for these vectors ranges\n---\nCosine as a similarity metric\n\n                                         1\n-1: vectors point in opposite directions 0.5\n+1: vectors point in same directions        50  400  150  200  250  300  350\n0: vectors are orthogonal                -0.5\n\n                                         -1\n\nBut since raw frequency values are non-negative, the\n50\ncosine for term-term matrix vectors ranges from 0\u20131\n---\n        0 for vectors that are orthogonal, to -1 for vectors pointing in opposite direction\nraw frequency values are non-negative, so the cosine for these vectors ranges\n rs        Let\u2019s see how the cosine computes which of the words cherry or digital is c\n        that are orthogonal, to -1 for vectors pointing in opposite directions.\n        But raw frequency values are non-negative, so the cosine for these vectors rang\n  0\u20131.  Cosine examples\n        in meaning to information, just using raw counts from the following shortened t\nequency values are non-negative, so the cosine for these vectors ranges\n        from 0\u20131.\n Let\u2019s see how the cosine computes which of the words cherry or digital is closer\n           Let\u2019s see how the cosine computes which of the words cherry or digital is clos\n eaning to information, just                                          pie  data  computer\n        in meaning to                   using raw counts from the following shortened table:\nsee how the cosine information, just using raw counts from the following shortened tabl\n                             computes which of the words cherry or digital is closer\n                                                  cherry              442     8   pie     data     computer\ng to       v \u2022 w  v      w       \u2211 N v wpie                     data       computer  2\n        information, just using raw counts from the following shortened table:\n                                                  i  i\n        cos(v, w) =  =   \u2022  =                i=1  digital             5    1683   1670\n                                                                   piecherry      442     8        2\n                     v w  v  w  \u2211 N               \u2211 N                     data   computer\n                                cherry               442           8       2\n                                             information              5    3982   3325\n                                     v 2                 w 2\n                                                  cherry           442     8      2\n                                             i             i          digital     5       1683     1670\n                                 i=1pie              data       computer\n                                digital              i=1\n                                                      5         1683       1670\n                                                  digital        5       1683    1670\n                          cherry     442              8               information 5       3982     3325\n                                                                4422\n                             information              5         3982 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                     information                 5         3325\n                                                         p               3982    3325\n        cos(cherry, information) =                                               p                      = .017\n                             digital         5       1683             21670\n                          information        5             442        + 82 + 22       52 + 39822 + 33252\n                                                     3982              3325\n                                                  442 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                                                442 \u21e4 5 + 8 \u21e4 3982 + 2 \u21e4 3325\n                                        p                          5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\ns(cherry, information) =                             p                p          p                = .017\n        cos(cherry, information) =                       p                            p                = .017\n        cos(digital, information) =                                                                          = .\n                                         4422 + 82 + 22                    52 + 39822 + 33252\n                                                           4422 + 82 + 22        52 + 39822 + 33252\n                                        442 \u21e4 5 + 8 5\u00b2 + 1683\u00b2 + 1670\u00b2                   52 + 39822 + 33252\n                                                           \u21e4 3982 + 2 \u21e4 3325\n                                                  5 \u21e4 5 + 5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\n                             p                                  1683 \u21e4 3982 + 1670 \u21e4 3325\nrry, information) =                                  p          p                     p   = .017\n        cos(digital, information) =                                                                        = .996\ns(digital, information) =               p                                     p                        = .996\n           The model decides that information is way closer to digital than it is to cher\n                                4422 + 82 + 22                  52 + 39822 + 33252\n                                             2             52 + 16832 + 16702         52 + 39822 + 33252\n        result that seems                5        + 16832 + 16702                52 + 39822 + 33252\n                                sensible. Fig. 6.7 shows a visualization.\n                             51         5 \u21e4 5 + 1683 \u21e4 3982 + 1670 \u21e4 3325\n tal, information) =         p                                        p                           = .996\n           The model decides that information is way closer to digital than it is to cherry,\n The model decides that information is way closer to digital than it is to cherry, a\n        result that seems          52 + 16832 + 16702                    52 + 39822 + 33252\n lt                                sensible. Fig. 6.7 shows a visualization.\n        that seems sensible. Fig. 6.7 shows a visualization.\n---\n   V Visualizing cosines\n    S                   E\n    (well, angles)\n    ECTOR EMANTICS AND   MBEDDINGS\n\n        \u2019\n        pie\n        \u2018\n        1:  500\n        Dimension  cherry\n                         digital    information\n\n                   500   1000  1500  2000  2500  3000\n\n                         Dimension 2: \u2018computer \u2019\nre 6.7   A (rough) graphical demonstration of cosine similarity, showing vec\n---\nVector       Cosine for computing word\nSemantics &  similarity\nEmbeddings\n---\nVector     TF-IDF\nSemantics &\nEmbeddings\n---\nBut raw frequency is a bad representation\n\n\u2022  The co-occurrence matrices we have seen represent each\n   cell by word frequencies.\n\u2022  Frequency is clearly useful; if sugar appears a lot near\n   apricot, that's useful information.\n\u2022  But overly frequent words like the, it, or they are not very\n   informative about the context\n\u2022  It's a paradox! How can we balance these two conflicting\n   constraints?\n---\n    fool                36     0.012\n    Two good            37     0\n    common solutions for word weighting\n    sweet               37     0\n ighting of the value for word t in document d , wt ,d thus combines\nth idf: tf-idf:  tf-idf value for word t in document d:\n\n                 wt ,d = tft ,d \u21e5 idft                     (6.13)\n\n idf weighting to the Shakespeare term-document matrix in Fig. 6.2.\n        Words like \"the\" or \"it\" have very low idf\nvalues for the dimension corresponding to the word         good have\n ince   PMI: (Pointwise mutual information)\n       this word appears in every document, the tf-idf algorithm\nd in any comparison                       #(% ,% )\n                        of the plays. Similarly, the word  fool, which\n        \u25e6 PMI !! , !\"   = $%& # % !#( \"\n the 37 plays, has a much lower           !  %\" )\n                                          weight.\n hting is by far the dominant way of weighting co-occurrence ma-\n          See if words like \"good\" appear more often with \"great\" than\n n retrieval, but also plays a role in many other aspects of natural\n          we would expect by chance\n akespeare\u2019s favorite adjectives, a fact probably related to the increased use of\n---\n     Term frequency (tf) in the tf-idf algorithm\n                       tft , d  = count(t , d )\n\n commonly we squash the raw frequency a bit, by using the lo\n cy  We could imagine using raw count:\n     instead. The intuition is that a word appearing 100 times\nn\u2019t make that word 100 times more likely to be relevant to the\n     tf\u209c,d = count(t,d)\n ment. We also need to do something special with counts of 0,\n he log of 0. 2\n     But instead of using raw count, we usually squash a bit:\n               (\n     tft , d   =  1 + log10 count(t , d )     if count(t , d ) > 0\n                  0                           otherwise\n\nuse log weighting, terms which occur 0 times in a document wou\n---\n      for discriminating those documents from the rest of the collection; terms that occur\nt     frequently across the entire collection aren\u2019t as helpful. The document frequency\n      df  Document frequency (df)\n      t   of a term t is the number of documents it occurs in. Document frequency is\n      not the same as the collection frequency of a term, which is the total number of\n      times the word appears in the whole collection in any document. Consider in the\n          df is the number of documents t occurs in.\n      collection of Shakespeare\u2019s 37 plays the two words Romeo and action. The words\n             t\n      have identical collection frequencies (they both occur 113 times in all the plays) but\n          (note this is not collection frequency: total count across\n      very different document frequencies, since Romeo only occurs in a single play. If\n          all documents)\n      our goal is to find documents about the romantic tribulations of Romeo, the word\n          \"Romeo\" is very distinctive for one Shakespeare play:\n      Romeo should be highly weighted, but not action:\n                         Collection Frequency  Document Frequency\n              Romeo      113                   1\n              action     113                   31\n          We emphasize discriminative words like Romeo via the inverse document fre-\nf     quency or idf term weight (Sparck Jones, 1972). The idf is defined using the frac-\n      tion N /df , where N is the total number of documents in the collection, and df is\n---\ner of documents in many collections, this measure\n      common as to be completely non-discriminative since they o\n      Inverse document frequency (idf)\n      good or sweet.3\ng function. The resulting definition for inverse\nus                                           Word         df     idf\n            \u2713        \u25c6                       Romeo        1      1.57\n      idf  = log  N                          salad        2      1.27\n                                             Falstaff     4         (6.13)\n      t           10  dft                    forest       12     0.967\n                                                                 0.489\n                                             battle       21     0.246\n e words in the Shakespeare corpus, ranging from\n      N is the total number of documents     wit          34     0.037\n hich occur in only one play like fool                    36     0.012\n      in the collection                      Romeo, to those that\n alstaff, to those which are very            good         37     0\n                                            common like fool or so\n                                             sweet        37     0\nn-discriminative since they occur in all 37 plays like\n---\nWhat is a document?\n\nCould be a play or a Wikipedia article\nBut for the purposes of tf-idf, documents can be\nanything; we often call each paragraph a document!\n---\n defined either by Eq. 6.11 or by Eq. 6.12) with id\n            Final tf-idf weighted value for a word\n                            wt , d           = tft , d \u21e5 idft\n           Raw counts:                           6.3            \u2022    W ORDS AND VECTORS  7\n            As You Like It   Twelfth Night    Julius Caesar    Henry V\n f-idf weighting to the Shakespeare term-documen\n            battle          1      0                7             13\n             good           114    80               62            89\n on          fool           36     58               1             4\n    Eq.                 6.12. Note that the tf-idf values for the\n             wit            20     15               2             3\n            Figure 6.2       The term-document matrix for four words in four Shakespeare plays. Each cell\n ord 6      \u2022  V            S      E\n           tf good have now all become 0; since this wor\n HAPTER     -idf: ECTOR          EMANTICS AND    MBEDDINGS\n            contains the number of times the (row) word occurs in the (column) document.\n idf algorithm leads it to be ignored. Similarly, the\n                        As You Like It  Twelfth Night           Julius Caesar  Henry V\n            represented as a count vector, a column in Fig. 6.3.\n            battle      0.246           0                       0.454          0.520\n vector     To review some basic linear algebra, a vector is, at heart, just a list or array of\n  f the 37 plays, has a much lower weight.\n            good        0               0                       0              0\n            numbers. So As You Like It is represented as the list [1,114,36,20] (the first column\n            fool        0.030           0.033                   0.0012         0.0019\n            vector in Fig. 6.3) and Julius Caesar is represented as the list [7,62,1,2] (the third\n            wit         0.085           0.081                   0.048          0.054\nr space     column vector).      A vector space is a collection of vectors, characterized by their\n            Figure 6.9       A tf-idf weighted term-document matrix for four words in four Shakespeare\n---\nVector     TF-IDF\nSemantics &\nEmbeddings\n---\nVector     Word2vec\nSemantics &\nEmbeddings\n---\nSparse versus dense vectors\n\nCount vectors (even if weighted by tf-idf)\n\u25e6  long (length |V|= 20,000 to 50,000)\n\u25e6  sparse (most elements are zero)\nAlternative: learn vectors which are\n\u25e6  short (length 50-1000)\n\u25e6  dense (most elements are non-zero)\n---\nSparse versus dense vectors\n\nWhy dense vectors?\n\u25e6   Short vectors may be easier to use as features in machine\n    learning (fewer weights to tune)\n\u25e6   Dense vectors may generalize better than explicit counts\n\u25e6   Dense vectors may do better at capturing synonymy:\n  \u25e6 car and automobile are synonyms; but are distinct dimensions\n    \u25e6  a word with car as a neighbor and a word with automobile as a\n       neighbor should be similar, but aren't\n\u25e6   In practice, they work better\n       56\n---\nCommon methods for getting short dense vectors\n\n\u201cNeural Language Model\u201d-inspired models\n\u25e6  Word2vec (skipgram, CBOW), GloVe\nSingular Value Decomposition (SVD)\n\u25e6  A special case of this is called LSA \u2013 Latent Semantic\n   Analysis\nAlternative to these \"static embeddings\":\n  \u2022     Contextual Embeddings (ELMo, BERT)\n  \u2022     Compute distinct embeddings for a word in its context\n  \u2022     Separate embeddings for each token of a word\n---\nSimple static embeddings you can download!\n\nWord2vec (Mikolov et al)\nhttps://code.google.com/archive/p/word2vec/\n\nGloVe (Pennington, Socher, Manning)\nhttp://nlp.stanford.edu/projects/glove/\n---\nWord2vec\nPopular embedding method\n\u2022     Very fast to train\n\u2022     Code available on the web\nIdea: predict rather than count\nWord2vec provides various options. We'll do:\nskip-gram with negative sampling (SGNS)\n---\n  Word2vec\nInstead of counting how often each word w occurs near \"apricot\"\n\u25e6   Train a classifier on a binary prediction task:\n  \u25e6  Is w likely to show up near \"apricot\"?\nWe don\u2019t actually care about this task\n  \u25e6  But we'll take the learned classifier weights as the word embeddings\nBig idea: self-supervision:\n  \u25e6  A word c that occurs near apricot in the corpus cats as the gold \"correct\n     answer\" for supervised learning\n  \u25e6  No need for human labels\n  \u25e6  Bengio et al. (2003); Collobert et al. (2011)\n---\nApproach: predict if candidate word c is a \"neighbor\"\n\n1.  Treat the target word t and a neighboring context word c\n    as positive examples.\n2.  Randomly sample other words in the lexicon to get\n    negative examples\n3.  Use logistic regression to train a classifier to distinguish\n    those two cases\n4.  Use the learned weights as the embeddings\n---\nSkip-Gram Training Data\n\nAssume a +/- 2 word window, given training sentence:\n\n\u2026lemon, a [tablespoon of apricot jam, a] pinch\u2026\nc1    c2 [target]      c3             c4\n---\nSkip-Gram Classifier\n\n(assuming a +/- 2 word window)\n\n\u2026lemon, a [tablespoon of apricot jam, a] pinch\u2026\n c1            c2 [target]          c3  c4\nGoal: train a classifier that is given a candidate (word, context) pair\n (apricot, jam)\n (apricot, aardvark)\n\u2026\nAnd assigns each pair a probability:\nP(+|w, c)\nP(\u2212|w, c) = 1 \u2212 P(+|w, c)\n---\nSimilarity is computed from dot product\n\nRemember: two vectors are similar if they have a high\ndot product\n\u25e6 Cosine is just a normalized dot product\nSo:\n\u25e6 Similarity(w,c) \u221d w \u00b7 c\nWe\u2019ll need to normalize to get a probability\n\u25e6 (cosine isn't a probability either)\n           64\n---\ndel   Turning dot products into probabilities\n      the probability that word  c is a real context word for target word w\n\n      Sim(    P(+|w, c) =       s (c \u00b7 w) =    1\n              w,c) \u2248 w \u00b7 c                     1 + exp (\u2212c \u00b7 w)\n                                                6.8  \u2022         W ORD 2 VEC\n      To turn this into a probability\nmoid function returns a number between 0 and 1, but to make it a prob\n del the probability that word   c is a real context word for target word w\n      We'll use the sigmoid from logistic regression:\n so need the total probability of the two possible events ( c is a context\nsn\u2019t a context word) to sum to 1. We thus estimate the probability that\neal context   P(+|w, c) =       s (c \u00b7 w) =    1\n      word for          w as:                  1 + exp (\u2212c \u00b7 w)\n\n      moid function returns a number between 0 and 1, but to make it a proba\n              P(\u2212|w, c) =         1 \u2212 P(+|w, c)\n lso need the total probability of the two possible events ( c is a context\nisn\u2019t a context word)   = s (\u2212c \u00b7 w) =         1\n                        to sum to 1. We thus estimate the probability that w\n                                               1 + exp (c \u00b7 w)\n---\n    How  P(\u2212|w, c) =                    1 \u2212 P(+|w, c)           6.8  \u2022  W ORD 2 VEC  19\n         Skip-Gram Classifier computes P(+|w, c)\n    We model the probability that word c is a real context           1\n                        = s (\u2212c \u00b7 w) =                         word for target word w as:     (6.29)\n                                                      1 + exp (c \u00b7 w)\n                     P(+|w, c) =        s (c \u00b7 w) = 1 + exp1                         (6.28)\nation 6.28 gives us                                            (\u2212c \u00b7 w)\n    This is             the probability for one word, but there are many context\n             for one context word, but we have lots of context words.\nds  The sigmoid function returns a number between 0 and 1, but to make it a probability\n    in the window. Skip-gram makes the simplifying assumption that all context\n    We'll assume independence and just multiply them:\n    we\u2019ll also need the total probability of the two possible events (c is a context word,\nds are independent, allowing us to just multiply their probabilities:\n    and c isn\u2019t a context word) to sum to 1. We thus estimate the probability that word c\n    is not a real context word for w as:\n                                                   L\n                     P(\u2212|w, c) =        1 \u2212 P(+|w, Y\n                        P(+|w, c1:L ) = c)            s (ci \u00b7 w)                              (6.30)\n                        = s (\u2212c \u00b7 w) = i=1                     1                     (6.29)\n                                                   1 + exp (c \u00b7 w)\n                                                  L\n    Equation 6.28 gives us the probability for    X\n                     log P(+|w, c                      ) one word, but there are many context\n    words in the window. Skip-gram makes   =          log s (c          \u00b7 w)                  (6.31)\n                                                  the simplifying assumption that all context\n                                        1:L                             i\n    words are independent, allowing us to just multiply their probabilities:\n                                                   i=1\n                    L\nmmary, skip-gram    Y\n                     trains a probabilistic classifier that, given a test target word\n---\nSkip-gram classifier: summary\n\nA probabilistic classifier, given\n\u2022     a test target word w\n\u2022     its context window of L words c1:L\nEstimates probability that w occurs in this window based\non similarity of w (embeddings) to c1:L (embeddings).\n\nTo compute this, we just need embeddings for all the\nwords.\n---\nThese embeddings we'll need: a set for w, a set for c\n\n1..d\naardvark  1\napricot\n\n\u2026         \u2026  W target words\n\n& =  zebra  |V|\n     aardvark  |V|+1\n     apricot\n                    C  context & noise\n     \u2026         \u2026       words\n\n     zebra     2V\n---\nVector     Word2vec\nSemantics &\nEmbeddings\n---\nVector       Word2vec: Learning the\nSemantics &  embeddings\nEmbeddings\n---\nWord2vec learns embeddings by starting with an initial set of embedding vecto\nSkip-Gram Training data\nand then iteratively shifting the embedding of each word w to be more like the em\nbeddings of words that occur nearby in texts, and less like the embeddings of word\nthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n... lemon,     a [tablespoon of apricot jam,      a] pinch ...\n           \u2026lemon, a [tablespoon of apricot jam,     a] pinch\u2026\n                c1 c1    c2  t       c3           c4\nThis example has a target word c2 [target]  c3       c4\n                             t (apricot), and 4 context words in the L = \u00b1\nwindow, resulting in 4 positive training instances (on the left below):\n positive examples +                        negative examples -\n t         c                    t           c           t        c\n apricot   tablespoon           apricot     aardvark    apricot  seven\n apricot   of                   apricot     my          apricot  forever\n                                apricot     where       apricot  dear\n apricot   jam                                                         71\n apricot   a                    apricot     coaxial     apricot  if\n---\nWord2vec learns embeddings by starting with an initial set of embedding vecto\nSkip-Gram Training data\nand then iteratively shifting the embedding of each word w to be more like the em\nbeddings of words that occur nearby in texts, and less like the embeddings of word\nthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n... lemon,     a [tablespoon of apricot jam,        a] pinch ...\n           \u2026lemon, a [tablespoon of apricot jam,       a] pinch\u2026\n                c1 c1    c2    t        c3          c4\nThis example has a target word c2 [target]    c3       c4\n                             t (apricot), and 4 context words in the L = \u00b1\nwindow, resulting in 4 positive training instances (on the left below):\n positive examples +         For              negative examples -\n t         c                       each positive\n                                   t          c           t        c\n                             example we'll grab k\n apricot   tablespoon              apricot    aardvark    apricot  seven\n                             negative examples,\n apricot   of                      apricot    my          apricot  forever\n                             sampling by frequency\n                                   apricot    where       apricot  dear\n apricot   jam                                                         72\n apricot   a                       apricot    coaxial     apricot  if\n---\n ord2vec learns embeddings by starting with an initial set of embedding vectors\n    Word2vec learns embeddings by starting with an initial set of embedding vecto\n d then Skip-Gram Training data\n         iteratively shifting the embedding of each word w to be more like the em-\n    and then iteratively shifting the embedding of each word w to be more like the em\n ddings of words that occur nearby in texts, and less like the embeddings of words\n    beddings of words that occur nearby in texts, and less like the embeddings of word\nat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n    that don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:\n . lemon, a [tablespoon of apricot jam,                   a] pinch ...\n    ... lemon,          a [tablespoon of apricot jam,                     a] pinch ...\n                    \u2026lemon, a [tablespoon of apricot jam,                 a] pinch\u2026\n                c1       c1  c2     t        c3           c4\n                             c1     c2       t           c3               c4\n  This example has a target word t           c2 [target]       c3         c4\n         This example has a         (apricot), and 4 context words in the L = \u00b12\n                              target word t (apricot), and 4 context words in the L = \u00b1\nindow, resulting in 4 positive training instances (on the left below):\n    window, resulting in 4 positive training instances (on the left below):\n          positive examples +\n    positive examples +                                        negative examples -\n    t     tc        c                            negative examples -\n                                        t        t  c          c     t      t  c     c\n                                        apricotapricot         aardvark     apricot  seven\n          apricot   tablespoon                      aardvark         apricot seven\n    apricot tablespoon                  apricotapricot         my           apricot  forever\n          apricot   of                              my               apricot forever\n    apricot of                                   apricot       where        apricot  dear\n          apricot   jam                 apricot     where            apricot dear    73\n    apricot jam                                  apricot       coaxial      apricot  if\n          apricot   a                   apricot     coaxial          apricot if\n    apricot a\n---\n        Word2vec: how to learn vectors\n\nGiven the set of positive and negative training instances,\nand an initial set of embedding vectors\nThe goal of learning is to adjust those word vectors such\nthat we:\n \u25e6  Maximize the similarity of the target word, context word pairs\n    (w , c\u209a\u2092\u209b) drawn from the positive data\n \u25e6  Minimize the similarity of the (w , cneg) pairs drawn from the\n    negative data.\n\n    8/24/25                                74\n---\n \u2022 Minimize the similarity of the (w, cneg ) pairs from the negative examples.\n If we consider one word/context pair (w, c  ) with its k noise words c  ...c  ,\nLoss function for one w with c               , c                  ...c\n                   pos                       pos  neg1               neg1     negk\n we can express these two goals as the following loss function L         negk\n                                                                  to be minimized\n (hence the \u2212); here the first term expresses that we want the classifier to assign the\nMaximize the similarity of the target with the actual context words,\n real context word cpos a high probability of being a neighbor, and the second term\nand minimize the similarity of the target with the k negative sampled\n expresses that we want to assign each of the noise words cnegi a high probability of\nnon-neighbor words.\n being a non-neighbor, all multiplied because we assume independence:\n                   [    k                         ]\n   LCE  = \u2212 log    P(+|w, cpos ) \u220f P(\u2212|w, cnegi )\n        [               i=1 k                          ]\n        = \u2212 log P(+|w, cpos ) + \u2211 log P(\u2212|w, cnegi )\n        [                    i=1                                  ]\n                             k\n        = \u2212 log P(+|w, cpos ) + \u2211 log \u23081 \u2212 P(+|w, cnegi )\u2309\n        [               ki=1                           ]\n        = \u2212 log s (cpos \u00b7 w) + \u2211 log s (\u2212cnegi \u00b7 w)                        (6.34)\n                        i=1\n---\nLearning the classifier\n\nHow to learn?\n\u25e6  Stochastic gradient descent!\n\nWe\u2019ll adjust the word weights to\n\u25e6  make the positive pairs more likely\n\u25e6  and the negative pairs less likely,\n\u25e6  over the entire training set.\n---\nIntuition of one step of gradient descent\n\naardvark\n                    move apricot and jam closer,\napricot              w  increasing c\u209a\u2092\u209b z w\nW\n\n                              \u201c\u2026apricot jam\u2026\u201d\n!           zebra\n         aardvark             move apricot and matrix apart\n\n   jam               c\u209a\u2092\u209b     decreasing cneg1 z w\n   C k=2 matrix      cneg1    move apricot and Tolstoy apart\n\n          Tolstoy    cneg2    decreasing cneg2 z w\n            zebra\n---\n   Reminder: gradient descent\n\n   \u2022 At each step\n     \u2022      Direction: We move in the reverse direction from the\nISTIC R     gradient of the loss function\n     EGRESSION\n     \u2022      Magnitude: we move the value of this gradient\n            ! ! (# $ ; & , ( ) weighted by a learning rate \u03b7\n\n     \u2022      !\"\n            Higher learning rate means move w faster\n\n              wt +1 = wt \u2212 h d L( f (x; w), y)\n                             dw\n---\n    pos                                         k\nesses that we want to assign each of        \u2211       \u2308    \u2309\n    = \u2212 log P(+|w, c                        the noise words c  a high probability o\n                      ) +                           log 1 \u2212 P(+|w, c  )\n    The derivatives of the loss                                neg\n           pos                                           function\ng a non-neighbor, all                                          i  negi\n \u2022  V  S                    multiplied because we assume independence:\n       ECTOR EMANTICS AND E MBEDDINGS\n             [            [                 ki=1         ]     ]\n   k\n   \u220f\n       L  = \u2212 log         P(+|w, c         )\u2211 P(\u2212|w, c   )\n          = \u2212 log s (c         \u00b7 w) +           log s (\u2212c  \u00b7 w)        (6.34\n       CE                      pos  pos                  neg\nof as an exercise at the end of the chapter):              neg\n                                                           i  i\n                                           i=1\n             [                             i=1 k                   ]\ns, we want to maximize the dot                  \u2211\n             \u2202 LCE                  product of the word with the actual contex\n             = \u2212 log P(+|w, c              ) +      log P(\u2212|w, c   )\n , and minimize the = [s (c         \u00b7 w) \u2212 1]w\n                       dot products of the word with the k negative sampled non\nbor words.   \u2202 cpos [          pos  pos         i=1               negi    ]\n             \u2202 L                                k\n e minimize                                     \u2211\n                this loss function using stochastic gradient descent.      Fig. 6.1\n                = CE  = [s (cneg \u00b7 w)]w                 \u2308                 \u2309\ns the        \u2202 c    \u2212 log P(+|w, c         ) +         log 1 \u2212 P(+|w, c   )\n          intuition of one step of learning.\n                neg                 pos                                   negi\n                       [                        i=1          k     ]\n             \u2202 L                         k                   X\n                 CE    = [s (c      \u00b7 w) \u2211\n             aardvark          pos         \u2212 1]cpos +             [s (cneg \u00b7 w)]cneg\n                = \u2212 log s (c        \u00b7 w) +       log s (\u2212c        \u00b7 w)\n                \u2202 w            pos  move apricot and neg                   i    i    (6.3\n                                                                  jam closer,\n                                            i=1              i=1  i\n                apricot        w            increasing c          z w\n---\n               \u2202 w   = [s (cpos \u00b7 w) \u2212 1]cpos +  [s (cnegi \u00b7 w)]cnegi\n    Update equation in SGD                       i=1\n he update equations going from time step t to t + 1 in stochastic gradient de\nre thus:\n    Start with randomly initialized C and W matrices, then incrementally do updates\n\n        ct +1  = ct      \u2212 h [s (ct  \u00b7 wt ) \u2212 1]wt\n        pos       pos    pos\n        ct +1  = ct      \u2212 h [s (ct  \u00b7 wt )]wt\n        neg       neg    \"  neg                     k                              #\n\n        wt +1  = wt \u2212 h  [s (cpos \u00b7 wt ) \u2212 1]cpos + X[s (cnegi \u00b7 wt )]cnegi\n                                                  i=1\nust as in logistic regression, then, the learning algorithm starts with randoml\n alized W and C matrices, and then walks through the training corpus using gra\nescent to move W and C so as to maximize the objective in Eq. 6.34 by makin\n---\nTwo sets of embeddings\n\nSGNS learns two sets of embeddings\nTarget embeddings matrix W\nContext embedding matrix C\nIt's common to just add them together,\nrepresenting word i as the vector wi + ci\n---\n Summary: How to learn word2vec (skip-gram)\n embeddings\nStart with V random d-dimensional vectors as initial\nembeddings\nTrain a classifier based on embedding similarity\n  \u25e6 Take a corpus and take pairs of words that co-occur as positive\n  examples\n  \u25e6 Take pairs of words that don't co-occur as negative examples\n  \u25e6 Train the classifier to distinguish these by slowly adjusting all\n  the embeddings to improve the classifier performance\n  \u25e6 Throw away the classifier code and keep the embeddings.\n---\nVector       Word2vec: Learning the\nSemantics &  embeddings\nEmbeddings\n---\nVector     Properties of Embeddings\nSemantics &\nEmbeddings\n---\nThe kinds of neighbors depend on window size\n\nSmall windows (C= +/- 2) : nearest words are syntactically\nsimilar words in same taxonomy\n\u25e6Hogwarts nearest neighbors are other fictional schools\n\u25e6Sunnydale, Evernight, Blandings\nLarge windows (C= +/- 5) : nearest words are related\nwords in same semantic field\n\u25e6Hogwarts nearest neighbors are Harry Potter world:\n\u25e6Dumbledore, half-blood, Malfoy\n---\nability to capture relational meanings. In an important early vector space model of\n   Analogical relations\ncognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model\nfor solving simple analogy problems of the form a is to b as a* is to what?. In such\n   The classic parallelogram model of analogical reasoning\nproblems, a system given a problem like apple:tree::grape:?, i.e., apple is to tree as\ngrape is to  , and must fill in the word vine. In the parallelogram model, illus-\n   (Rumelhart and Abrahamson 1973)    #  \u00bb  # \u00bb\ntrated in Fig. 6.15, the vector from the word apple to the word tree (= apple \u2212 tree)\n   To solve: \"apple         #  \u00bb\nis added to the vector for  is to tree as grape is to _____\"\n                            grape (grape); the nearest word to that point is returned.\n   Add tree \u2013 apple to grape to get vine\n                                      tree\n             apple\n\n   vine\n   grape\n---\n llelogram method received more modern attention because of\n       Analogical relations via parallelogram\n rd2vec or GloVe vectors ( Mikolov et al. 2013b, Levy and Gold\n gton et al. 2014). For example, the result       #\n                 # of the expression (kin\n       The parallelogram                        \u00bb  #  \u00bb  #  \u00bb\n                 #           method can solve analogies with\n is a vector close to        \u00bb\n                 #  \u00bb  queen. Similarly, Paris \u2212 France + Italy)\n       both sparse and dense embeddings (Turney and\n hat is close to Rome. The embedding model thus seems to be ex\n       Littman 2005, Mikolov et al. 2013b)\n tions of relations like MALE - FEMALE , or CAPITAL - CITY- OF, or\n       /    king \u2013 man + woman is close to queen\nIVE SUPERLATIVE, as shown in Fig. 6.16 from GloVe.\nr a         Paris \u2013 France + Italy is close to Rome\n       a:b::a*:b* problem, meaning the algorithm is given a, b, and\n , the parallelogram method is thus:\n            For a problem a:a*::b:b*, the parallelogram method is:\n                 \u02c6 \u2217                     \u2217\n                 b      = argmax distance(x, a  \u2212 a + b)\n                        x\n---\nStructure in GloVE Embedding space\n0.5                               heiress\n\n0.4\n            niece                        countess\n     0.3    aunt                         duchess\n            $ister\n     0.2                                 empress\n\n     0.1                          madam\n\n0           nephew    heir\n\n-0.1        uncle     ;woman             ue earl,\n                                         queen\n-0.2         brother                     /duke\n\n-0.3                                     emperor\n\n-0.4                    man     sir       king\n\n-0.5\n\n            -0.5  -0.4  -0.3   0.2  -0.1  0  0.1  0.2  0.3  0.4  0.5\n---\nCaveats with the parallelogram method\n\nIt only seems to work for frequent words, small\ndistances and certain relations (relating countries to\ncapitals, or parts of speech), but not others. (Linzen\n2016, Gladkova et al. 2016, Ethayarajh et al. 2019a)\n\nUnderstanding analogy is an open area of research\n(Peterson et al. 2020)\n---\nEmbeddings as a window onto historical semantics\n\nTrain embeddings on different decades of historical text to see meanings shift\n\n                      ~30 million books, 1850-1990, Google Books data\na daft gay (1900s)                 b spread                          C  solemn\nflaunting       sweet                                                   awful (1850s)\ntasteful              cheerful       broadcast (1850s) soW              awe majestic\n                       pleasant                seed                     dread       pensive\n            frolicsom\u2091             circulated                       SOWS         gloomy\n              witty gay (1950s)                  scatter\n                bright               broadcast (1900s)                      horrible\ngays        bisexual                 newspapers                             appalling terrible\n                homosexual           television                             awful (1900s)     wonderful\n gay (1990s)                         radio                                       awful (1990s)\n     lesbian                       bbcbroadcast (1990s)                          aulweird\n\n            William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic Word Embeddings Reveal\n            Statistical Laws of Semantic Change. Proceedings of ACL.\n---\nEmbeddings reflect cultural bias!\n\n   Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. \"Man is to computer\n   programmer as woman is to homemaker? debiasing word embeddings.\" In NeurIPS, pp. 4349-4357. 2016.\n\n Ask \u201cParis : France :: Tokyo : x\u201d\n \u25e6 x = Japan\n Ask \u201cfather : doctor :: mother : x\u201d\n \u25e6 x = nurse\n Ask \u201cman : computer programmer :: woman : x\u201d\n \u25e6 x = homemaker\nAlgorithms that use embeddings as part of e.g., hiring searches for\nprogrammers, might lead to bias in hiring\n---\nHistorical embedding as a tool to study cultural biases\n             Garg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes.\n             Proceedings of the National Academy of Sciences 115(16), E3635\u2013E3644.\n\n\u2022     Compute a gender or ethnic bias for each adjective: e.g., how\n      much closer the adjective is to \"woman\" synonyms than\n      \"man\" synonyms, or names of particular ethnicities\n      \u2022     Embeddings for competence adjective (smart, wise,\n            brilliant, resourceful, thoughtful, logical) are biased toward\n            men, a bias slowly decreasing 1960-1990\n      \u2022     Embeddings for dehumanizing adjectives (barbaric,\n            monstrous, bizarre) were biased toward Asians in the\n            1930s, bias decreasing over the 20\u1d57\u02b0 century.\n\u2022     These match the results of old surveys done in the 1930s\n---\nVector     Properties of Embeddings\nSemantics &\nEmbeddings\n\n"
    }
}