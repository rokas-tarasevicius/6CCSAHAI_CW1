{
    "data/raw/Words and tokens - EXTENDED.pdf": {
        "metadata": {
            "file_name": "Words and tokens - EXTENDED.pdf",
            "file_type": "pdf",
            "content_length": 46645,
            "language": "en",
            "extraction_timestamp": "2025-11-26T02:00:31.165573+00:00",
            "timezone": "utc"
        },
        "content": "Words  Words\nand\nTokens\n---\nHow many words in a sentence?\n\nThey picnicked by the pool, then\nlay back on the grass and looked at\nthe stars.\n\n 16 words\n \u25e6  if we don\u2019t count punctuation marks as words\n 18 if we count punctuation\n---\nHow many words in an utterance?\n\n\"I do uh main- mainly business data\nprocessing\"\n\nDisfluencies\n\u25e6  Fragments main-\n\u25e6  Filled pauses: uh and um\n\n\u25e6  Should we consider these to be words?\n---\nHow many words in a sentence?\n\nThey picnicked by the pool, then\nlay back on the grass and looked at\nthe stars.\n\n Type: an element of the vocabulary V\n  \u25e6  The number of types is the vocabulary size |V|\n Instance: an instance of that type in running text.\n  \u25e6  14 types and 16 instances (if we ignore punctuation).\n  \u25e6  More questions: Are They and they the same word?\n---\nHow many words in a sentence?\n\nI'm\nOrthographically one word (in the English\nwriting system)\n\nBut grammatically two words:\n1.     the subject pronoun I\n2.     the verb \u2019m, short for am.\n---\nHow many words in a sentence?\n\nNot every written language uses spaces!!\n\nChinese, Japanese and Thai don't!\n---\n How to choose tokens in Chinese\n\nChinese words are composed of characters\ncalled \"hanzi\" (\u6c49\u5b57) (or sometimes just \"zi\")\nEach one represents a meaning unit called a\nmorpheme.\nEach word has on average 2.4 of them.\nBut deciding what counts as a word is complex\nand not agreed upon.\n---\n How to do choose tokens in Chinese?\n\n\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5b \u201cYao Ming reaches the finals\u201d\n   \u25e6y\u00e1o m\u00edng j\u00ecn r\u00f9 z\u01d2ng ju\u00e9 s\u00e0i\n3 words?\n\u59da\u660e      \u8fdb\u5165 \u603b\u51b3\u8d5b                  Chinese Treebank\nYaoMing reaches finals\n5 words?\n\u59da   \u660e   \u8fdb\u5165  \u603b                   \u51b3\u8d5b        Peking University\nYao Ming reaches overall        finals\n7 words?\n\u59da   \u660e   \u8fdb  \u5165     \u603b              \u51b3     \u8d5b   Just use characters\nYao Ming enter enter overall decision game\n---\nTokenization across languages\n\nSo in Chinese we use characters (zi) as\ntokens\n But that doesn't work for, e.g., Thai and\n Japanese\n These differences make it hard to use words as\n tokens\nAnd there's another reason why we don't\nuse words as tokens!\n---\nThere are simply too many words!\n\nNotice that (roughly) the bigger the corpora,\nthe more words we find!\n\n                              Types = |V|     Instances = N\nShakespeare                   31 thousand     884,000\nBrown Corpus                  38 thousand     1 million\nSwitchboard conversations     20 thousand     2.4 million\nCOCA                          2 million       440 million\nGoogle N-grams                13+ million     1 trillion\n---\n erdan\u2019s Law (Herdan, 1960) or Heap\n    There are simply too many words!\nnguistics and information retrieval resp\n  N    = number of instances\n  b are positive constants, and 0 < b <\n  |V | = number of types in vocabulary V\n  Heaps Law = Herdan's Law\n       |V | = kN b           Roughly 0.5\n  Vocab size for a text goes up with the square\n  root of its length in words\n---\nTwo kinds of words\n\nFunction words\nContent words\n---\nTria, Loreto, Servedio, 2018\n---\nWhy is too many words a problem?\n\nNo matter how big our vocabulary\nThere will always be words we missed!\nWe will always have unknown words!\n---\nWords and Subwords\n\nBecause of these problems:\n \u25e6  Many languages don't have orthographic words\n \u25e6  Defining words post-hoc is challenging\n \u25e6  The number of words grows without bound\n\nNLP systems don't use words, but smaller units called\nsubwords\nIn the next lecture we'll start by introducing smaller units like\nmorphemes and characters\n---\nWords  Words\nand\nTokens\n---\nWords  Morphemes\nand\nTokens\n---\nWords have parts\n\nMorpheme: a minimal meaning-bearing unit in a\nlanguage.\n fox: one morpheme\n cats: two morphemes cat and \u2013s\n\nMorphology: the study of morphemes\n---\nword fox consists of one morpheme (the morpheme fox) while the word cats consists\nof two: the morpheme cat and the morpheme -s that indicates plural.\n          Morphemes in English and Chinese\n   Here\u2019s a sentence in English segmented into morphemes with hyphens:\n(2.6)    Doc work-ed care-ful-ly wash-ing the glass-es\n         Doc work-ed care-ful-ly wash-ing the\n   As we mentioned above, in Chinese, conveniently, the writing system is set up\n         glass-es\nso that each character mainly describes a morpheme. Here\u2019s a sentence in Mandarin\nChinese with each morpheme character glossed, followed by the translation:\n(2.7)     \u00d6 r \u2039        ( \u21e7 4 \u00b7 o \uff0c^                      \u02d9 \u540e       \uff0c\u2022 r\n          plum dry vegetable use clear water soak soft , remove out after , drip dry\n          \u2325 \u00e9\n          chop fragment\n          Soak the preserved vegetable in water until soft, remove, drain, and chop\n   We generally distinguish two broad classes of morphemes: roots\u2014the central\nmorpheme of the word, supplying the main meaning\u2014and affixes\u2014adding \u201cad-\n---\nTypes of morphemes\n\nroot: central morpheme of the word\n       - supplying the main meaning\naffix: adding additional meanings\n\nworked\n root work\n affix -ed\nglasses\n root glass\n affix -es\n---\nTypes of affixes\nInflectional morphemes\n\u25e6  grammatical morphemes\n\u25e6  often syntactic role like agreement\n    \u2013ed past tense on verbs\n    -s/-es plural on nouns\nDerivational morphemes\n\u25e6  more idiosyncratic in application and meaning\n\u25e6  often change grammatical class\n    care (noun)\n     + -full \u00e0 careful (adjective)\n     + -ly \u00e0 carefully (adverb)\n---\nClitics\nA morpheme that acts syntactically like a word but:\n \u25e6  is reduced in form\n \u25e6  and attached to another word\n\nEnglish: 've in I've  ('ve can't appear alone)\nEnglish: \u2019s in the teacher\u2019s book\nFrench:  l\u2019 in l\u2019opera\nArabic:  b \u2018by/with\u2019, w \u2018and\u2019.\n---\nMorphological Typology\n\nDimensions along which languages vary\nTwo are salient for tokenization:\n 1.     number of morphemes per word\n 2.     how easy it is to segment the\n        morphemes\n---\nNumber of morphemes per word\n\nFew. Cantonese, spoken in Guangdong, Guangxi, Hong Kong\nkeoi5 waa6 cyun4 gwok3 zeoi3 daai6 gaan1  uk1  hai6 ni1 gaan1\nhe  say  entire country most big  building house is  this building\n\u201cHe said the biggest house in the country was this one\u201d\n\nMany. Koryak, Kamchatka peninsula in Russia,\nt-\u0259-nk\u2019e-mej\u014b-\u0259-jetem\u0259-nni-k\n1SG.S-E-midnight-big-E-yurt.cover-E-sew-1SG.S[PFV]\n\u201cI sewed a lot of yurt covers in the middle of a night.\u201d\n---\nJoseph Greenberg (1960) scale\n\nVietnamese    Old English    Greenlandic\n              English        Sanskrit\n                         Swahili\nFarsi                    Yakut       (Inuit)\n\n1.1    1.5    1.7        2.1 2.2  2.5 2.6    3.7\n\nAnalytic                 Synthetic           Polysynthetic\n\n                     Morphemes per Word\n---\nHow easily segmentable\n\nAgglutinative languages like Turkish\n \u25e6   Very clean boundaries between morphemes\n\nFusion languages\n \u25e6   a single affix may conflate multiple morphemes,\n\n \u25e6   Russian -om in stolom (table-SG-INSTR- DECL1)\n   \u25e6 instrumental, singular, and first declension.\n \u25e6   English \u2013s in \"She reads the article\"\n   \u25e6 Means both \"third person\" and \"present tense\"\n\nThese are tendencies rather than absolutes\n---\nWords  Morphemes\nand\nTokens\n---\nWords  Unicode\nand\nTokens\n---\nUnicode\n\na method for representing text written using\n\u2022         any character (more than 150,000!)\n\u2022         in any script (168 to date!)\n\u2022         of the languages of the world\n     \u2022     Chinese, Arabic, Hindi, Cherokee, Ethiopic, Khmer, N\u2019Ko,\u2026\n     \u2022     dead ones like Sumerian cuneiform\n     \u2022     invented ones like Klingon\n     \u2022     plus emojis, currency symbols, etc.\n---\nASCII: Some history for English\n1960s American Standard Code for Information Exchange\n1 byte per character\n \u25e6  In principle 256 characters\n \u25e6  But high bit set to 0\n \u25e6  So 7 bits = 128\n \u25e6  However only 95 used        TEEE\n                                T\n The rest were for teletypes        ro-p    ciele\n---\nthem; the high-order bit of ASCII bytes is always set to 0. (Actually it only used 95\n      ASCII: Some history for English\nof them and the rest were control codes for an obsolete machine called a teletype).\nHere\u2019s a few ASCII characters with their representation in hex and decimal:\n\n Ch Hex           Dec      Ch Hex       Dec            Ch Hex       Dec     Ch Hex       Dec\n <     3C         60       @     40     64     ...     \\     5C     92      `     60     96\n =     3D         61       A     41     65     ...     [     5D     93      a     61     97\n >     3E         62       B     42     66     ...     \u02c6     5E     94      b     62     98\n ?     3F         63       C     43     67     ...     _     5F     95      c     63     99\nFigure 2.4        Some selected ASCII codes for some English letters, with the codes shown both\nin hexadecimal and decimal.\n      h           e      l      l       o\n      But ASCII is of course insufficient since there are lots of other characters in the\nworld\u2019s writing systems! Even for scripts that use Latin characters, there are many\n      68 65 6C 6C 6F\nmore than the 95 in ASCII. For example, this Spanish phrase (meaning \u201cSir, replied\nSancho\u201d) has two non-ASCII characters, \u02dc               \u00b4\n                                               n and o:\n(2.10)       \u02dc                  \u00b4\n        Senor- respondio Sancho-\n---\n               =     3D         61     A     41     65     ...     [     5D     93     a     61     97\n               >     3E         62     B     42     66     ...     \u02c6     5E     94     b     62     98\n   ASCII wasn't enough!\n               ?     3F         63     C     43     67     ...     _     5F     95     c     63     99\n              Figure 2.4        Some selected ASCII codes for some English letters, with the codes shown both\n              in hexadecimal and decimal.\n   Spanish: Se\u00f1or- respondi\u00f3 Sancho\n                    But ASCII is of course insufficient since there are lots of other characters in the\n              world\u2019s writing systems! Even for scripts that use Latin characters, there are many\n                    This sentence has non-ASCII \u00f1 and \u00f3\n              more than the 95 in ASCII. For example, this Spanish phrase (meaning \u201cSir, replied\n              Sancho\u201d) has two non-ASCII characters, \u02dc             \u00b4\n   About 100,000                                           n and o:\n                           \u02dc                Chinese/CJKV characters\n              (2.10) Se                     \u00b4\n                           nor- respondio Sancho-\n   (Chinese, Japanese, Korean, or Vietnamese)\nDevanagari          And lots of languages aren\u2019t based on Latin characters at all! The Devanagari\n   Devanagari script for 120 languages like\n              script is used for 120 languages (including Hindi, Marathi, Nepali, Sindhi, and San-\n              skrit). Here\u2019s a Devanagari example from the Hindi text of the Universal Declaration\n   Hindi, Marathi, Nepali, Sindhi, Sanskrit, etc.\n              of Human Rights:\n                                      I     P\n                           1  F ( 2\n                    Chinese has about 100,000 Chinese characters in Unicode (including overlap-\n---\nCode Points\n\nUnicode assigns a unique ID, a code point,\nto each of its 150,000 characters\n1.1 million possible code points\n\u25e6 0 \u2013 0x10FFFF\nWritten in hex, with prefix \"U+\"\n\u25e6 a is U+0061 which = 0x0061\nFirst 127 code points = ASCII\n\u25e6 For backwards compatibility\n---\n               compatible with ASCII, which means that the first 127 code points, including the\nSome code points\n               code for a, are identical with ASCII.) Here are some sample code points; some (but\n               not all) come with descriptions:\n                0061     a     LATIN SMALL LETTER A\n                0062     b     LATIN SMALL LETTER B\n                0063     c     LATIN SMALL LETTER C\n                00F9     `\n                         u     LATIN SMALL LETTER U WITH GRAVE\n                00FA     \u00b4\n                         u     LATIN SMALL LETTER U WITH ACUTE\n                00FB     \u02c6\n                         u     LATIN SMALL LETTER U WITH CIRCUMFLEX\n                00FC     \u00a8\n                         u     LATIN SMALL LETTER U WITH DIAERESIS\n                8FDB 5/23/25, 5:26 PM\n                         \u20ac\n                8FDC     \u2039\n                8FDD     \ud83c\udc0e\n                         \u203a\n                8FDE     fi\n                        5/23/25, 5:26 PM\n                1F600          GRINNING FACE\n                1F00E   \ud83c\udc0e MAHJONG TILE EIGHT OF CHARACTERS\n\nA code point has no visuals; it is not a glyph!\n               2.3.2  UTF-8 Encoding\nGlyphs are stored in fonts:             a or a or a or a\n               While the code point (the unique id) is the abstract Unicode representation of the\nBut all of     character, we don\u2019t just stick that id in a text file.\n               them are U+0061, abstract \"LATIN SMALL A\"\n               Instead, whenever we need to represent a character in a text string, we write an\nencoding       encoding of the character. There are many different possible encoding methods, but\n---\nEncodings and UTF-8\n\nWe don't stick code points directly in files\nWe store encodings of chars.\nThe most popular encoding is UTF-8\nMost of the web is stored in UTF-8\n---\nEncodings\n\nhello has these 5 code points:\nU+0068 U+0065 U+006C U+006C U+006F\nHow to write in a file?\nThere are more than 1 million code points\nSo need 4 bytes (or 3 but 3 is inconvenient):\n00 00 00 68 00 00 00 65 00 00 00 6C 00 00 00 6C 00 00 00 6F\nBut that makes files very long!\n\u25e6 Also zeros are bad (since mean \"end of string\" in ASCII)\n---\nInstead: Variable Length Encoding\n\nUTF-8 (Unicode Transformation Format 8)\nFor the first 127 code points, same as ASCII\nUTF-8 encoding of hello is :\n \u25e6  68 65 6C 6C 6F\nCode points \u2265128 are encoded as a sequence\nof 2, 3, or 4 bytes\n \u25e6  In range 128 - 255, so won\u2019t be confused with ASCII\n \u25e6  First few bits say if its 2-byte, 3-byte, or 4-byte\n---\n              encoded as a sequence of two, three, or four bytes. Each of these bytes are between\n              128 and 255, so they won\u2019t be confused with ASCII, and each byte indicates in the\n       UTF-8 Encoding\n              first few bits whether it\u2019s a 2-byte, 3-byte, or 4-byte encoding.\n\n                   Code Points                                             UTF-8 Encoding\n From - To          Bit Value                                 Byte 1       Byte 2     Byte 3     Byte 4\n U+0000-U+007F      0xxxxxxx                                  xxxxxxxx\n U+0080-U+07FF      00000yyy yyxxxxxx                         110yyyyy     10xxxxxx\n U+0800-U+FFFF      zzzzyyyy yyxxxxxx                         1110zzzz     10yyyyyy   10xxxxxx\n U+010000-U+10FFFF  000uuuuu zzzzyyyy yyxxxxxx                11110uuu     10uuzzzz   10yyyyyy   10xxxxxx\n Figure 2.5  Mapping from Unicode code point to the variable length UTF-8 encoding. For a given code point\nin the From-To range, the bit value in column 2 is packed                  yyy       yyxxxxxx\n                                                     into 1, 2, 3, or 4 bytes. Figure adapted from Unicode\n  n, code point U+00F1, = 00000000 11110001\n16.0 Core Spec Chapter 3 Table 3-6.\n\n  \u25e6 Gets encoded with pattern 110yyyyy 10xxxxxx\n                  Fig. 2.5 shows how this mapping occurs. For example these rules explain how\n              the character \u02dc\n  \u25e6 So              n, which has code point U+00F1, is mapped to the two-byte bit se-\n              is mapped to a two-byte bit sequence\n              quence 11000011 10110001 or 0xC3B1. As a result of these rules, the first 127\n  \u25e6           characters (ASCII) are mapped to one byte, most remaining characters in European,\n       11000011 10110001 = 0xC3B1.\n              Middle Eastern, and African scripts map to two bytes, most Chinese, Japanese, and\n              Korean characters map to three bytes, and rarer CJKV characters and emojis and\n---\nUTF-8 encoding\n\nThe first 127 characters (ASCII) map to 1 byte\nMost remaining characters in European, Middle\nEastern, and African scripts map to 2 bytes\nMost Chinese, Japanese, and Korean characters\nmap to 3 bytes\nRarer CJKV characters, emojis/symbols map to\n4 bytes.\n---\nUTF-8 encoding\n\nEfficient: fewer bytes for common characters,\nDoesn't use zero bytes (except for NULL\ncharacter U+0000),\nBackwards compatible with ASCII,\nSelf-synchronizing,\n\u25e6  If a file is corrupted, the nearest character boundary\n   is always findable by moving only up to 3 bytes\n---\nUTF-8 and Python 3\n\nPython 3 strings stored internally as Unicode\n\u25e6   each string a sequence of Unicode code points\n\u25e6   string functions, regex apply natively to code points.\n  \u25e6 len() returns string length in code points, not bytes\nFiles need to be encoded/decoded when\nwritten or read\n\u25e6   Every file is stored in some encoding\n\u25e6   No such thing as a text file without an encoding\n  \u25e6 If it's not UTF-8 it's something older like ASCII or iso_8859_1\n---\nWords  Unicode\nand\nTokens\n---\nWords  Byte Pair Encoding\nand\nTokens\n---\nThe NLP standard for tokenization\n\nInstead of\n\u2022 white-space / orthographic words\n \u2022  Lots of languages don't have them\n \u2022  The number of words grows without bound\n\u2022 Unicode characters\n \u2022  Too small as tokens for many purposes\n\u2022 morphemes\n \u2022  Very hard to define\nWe use the data to tell us how to tokenize.\n---\nWhy tokenize?\n\nUsing a deterministic series of tokens means\nsystems can be compared equally\n\u25e6 Systems agree on the length of a string\nEliminates the problem of unknown words\n---\nSubword tokenization\n\nTwo most common algorithms:\n\u25e6  Byte-Pair Encoding (BPE) (Sennrich et al., 2016)\n\u25e6  Unigram language modeling tokenization (Kudo,\n   2018) (sometimes confusingly called\n   \"SentencePiece\" after the library it's in)\nAll have 2 parts:\n\u25e6  A token learner that takes a raw training corpus and\n   induces a vocabulary (a set of tokens).\n\u25e6  A token segmenter that takes a raw test sentence and\n   tokenizes it according to that vocabulary\n---\n    Byte Pair Encoding (BPE) token learner\nIteratively merge frequent neighboring tokens to create longer tokens.\n\nRepeat:                            Vocabulary\n \u25e6  Choose most frequent            [A, B, C, D, E]\n\n \u25e6  neighboring pair ('A', 'B')     [A, B, C, D, E, AB]\n    Add a new merged symbol         [A, B, C, D, E, AB, CAB]\n    ('AB') to the vocabulary\n \u25e6  Replace every 'A' 'B' in the   Corpus\n    corpus with 'AB'.              A B D C A B E C A B\nUntil k merges                     AB D C AB E C AB\n                                   AB D CAB E CAB\n---\nBPE algorithm\n\nGenerally run within words\nDon't merge across word boundaries\n \u25e6  First separate corpus by whitespace\n \u25e6  This gives a set of starting strings, with whitespace\n    attached to front of them\n \u25e6  Counts come from the corpus, but can only merge\n    within strings.\n---\n  BPE token learner algorithm\n  2.4  \u2022  T EXT N ORMALIZATION  19\n\n  function B YTE - PAIR ENCODING(strings C, number of merges k) returns vocab V\n\n  V     all unique characters in C  # initial set of tokens is characters\n  for i = 1 to k do                 # merge tokens til k times\n     tL , tR  Most frequent pair of adjacent tokens in C\n     t NEW    tL + tR               # make new token by concatenating\n     V      V + t NEW               # update the vocabulary\n     Replace each occurrence of tL , tR in C with t NEW  # and update the corpus\n  return V\n\n Figure 2.13  The token learner part of the BPE algorithm for taking a corpus broken up\ninto individual characters or bytes, and learning a vocabulary by iteratively merging tokens.\n---\nByte Pair Encoding (BPE) Addendum\n\nMost subword algorithms are run inside\nspace-separated tokens.\nSo we commonly first add a special end-of-\nword symbol '__' before space in training\ncorpus\nNext, separate into letters.\n---\nxplicitly marked the spaces between words: 3\n plicitly marked the spaces between words: 3\nset   BPE token learner\n et   new new renew reset renew\n      new new renew reset renew\n      Original (very fascinating\ud83d\ude44) corpus:\n t, we\u2019ll break up the corpus into words, with leading whitespace\n , we\u2019ll break up the corpus into words, with leading whitespace,\n ir   set\u2423new\u2423new\u2423renew\u2423reset\u2423renew\n      counts; no merges will be allowed to go beyond these word bo\nr counts; no merges will be allowed to go beyond these word bou\n lt looks like the following list of 4 words and a starting vocabu\nlt looks like the following list of 4 words and a starting vocabul\ners:  Put space token at start of words\n rs:\n      corpus             vocabulary\n      corpus             vocabulary\n      22        n e w          , e, n, r, s, t, w\n                n e w    , e, n, r, s, t, w\n      22        r e n e w\n                r e n e w\n      11    s e t\n            s e t\n      11        r e s e t\n                r e s e t\n---\nr counts; no merges will be allowed to go beyond these word boundaries.\ntheir counts; no merges will be allowed to go beyond these word boundaries.\nt looks like the following list of 4 words and a starting vocabulary of 7\nesult looks like the following list of 4 words and a starting vocabulary of 7\nrs:  BPE token learner\nacters:\n W      corpus T             vocabulary\n              corpus         vocabulary\n     ORDS AND       OKENS\n        2     2     n e w    , e, n, r, s, t, w\n                        n e w    , e, n, r, s, t, w\n        2     2     r e n e w\n e                      r e n e w\n       BPE training algorithm first counts all pairs of adjacent symbols: the most\n        1     1 s e t\n nt is the pair n s e t\n        1           e because it occurs in new (frequency of 2) and renew (fre-\n              1     r e s e t\ny of 2) for a           r e s e t\n                total of 4 occurrences. We then merge these symbols, treating ne\n       Merge n      e to ne (count 4 = 2 new + 2 renew)\nsymbol, and count again:\nrealize this isn\u2019t a particularly likely or exciting sentence.\ns, we realize this isn\u2019t a particularly likely or exciting sentence.\n       corpus                    vocabulary\n       2           ne w          , e, n, r, s, t, w, ne\n       2           r e ne w\n       1           s e t\n       1           r e s e t\n---\n  ent is the pair n e because it occurs in new (frequency of 2) and renew (fre-\n f 2) for a total of 4 occurrences. We then merge these symbols, treating ne\ncy of 2) for a total of 4 occurrences. We then merge these symbols, treating ne\n          BPE token learner\nmbol, and count again:\ne symbol, and count again:\n      corpus               vocabulary\n      2   corpus           vocabulary\n          2     ne w       , e, n, r, s, t, w, ne\n      2         r ne w     , e, n, r, s, t, w, ne\n          2         e ne w\n      1      s e r e ne w\n          1     s t\n      1         r e t\n          1         e s e t\n                    r e s e t\n the most frequent pair is ne w (total count=4), which we merge.\n    Merge ne            w to new (count 4)\n ow the most frequent pair is ne w (total count=4), which we merge.\n  corpus                     vocabulary\n     corpus                  vocabulary\n  2  2       new             , e, n, r, s, t, w, ne, new\n                new          , e, n, r, s, t, w, ne, new\n  2  2       r e new\n                r e new\n  1  1 s e t\n             s e t\n  1  1       r e s e t\n                r e s e t\n---\n         Now 1              r e s e t\n                 the most frequent pair is ne w (total count=4), which we merge.\n                 BPE token learner\n   ow the most frequent pair is ne w (total count=4), which we merge.\n                  corpus             vocabulary\n                  2            new    , e, n, r, s, t, w, ne, new\n            corpus                    vocabulary\n            2     2     newr e new    , e, n, r, s, t, w, ne, new\n            2     1     s e t\n                  1     r e new\n            1     s e tr e s e t\n   Next 1 r (total count of 3) get merged to r, and then r e (total count 3) gets\n   merged to            r e s e t\nt        r        re. The system has essentially induced that there is a word-initial prefix\n            (total count of 3) get merged to r, and then r e (total count 3) gets\n   re-: Merge \u2423                r      to \u2423r (count 4) and \u2423r e to \u2423re (count 3)\n   ed to     re. The system has essentially induced that there is a word-initial prefix\n             corpus                   vocabulary\n             2          new           , e, n, r, s, t, w, ne, new, r,           re\n    corpus                            vocabulary\n             2       re new\n    2        1   new                  , e, n, r, s, t, w, ne, new, r,           re\n    2             s e t               System has learned prefix re- !\n             1 re new\n                     re s e t\n    1        s e t\n---\nre-:\nBPE\n    corpus            vocabulary\n    2         new     , e, n, r, s, t, w, ne, new,          r,   re\n    2         re new\n    1       s e t\nThe next merges are:\n    1         re s e t\nIf we continue, the next merges are:\nmerge         current vocabulary\n( , new)       , e, n, r, s, t, w, ne, new,  r,  re,  new\n( re, new)     , e, n, r, s, t, w, ne, new,  r,  re,  new,  renew\n(s, e)         , e, n, r, s, t, w, ne, new,  r,  re,  new,  renew, se\n(se, t)        , e, n, r, s, t, w, ne, new,  r,  re,  new,  renew, se, set\n\nfunction B YTE - PAIR ENCODING(strings C, number of merges k) returns vocab V\n\nV  all unique characters in C  # initial set of tokens is characters\n---\nBPE encoder algorithm\nTokenize a test sentence: run each merge learned\nfrom the training data:\n \u25e6  Greedily, in the order we learned them\n \u25e6  (test frequencies don't play a role)\nFirst: segment each test word into characters\nThen run rules: (1) merge every n e to ne, (2) merge\nne w to new, (3) \u2423r, (4)    \u2423re         etc.\nResult:\n \u25e6  Recreates training set words\n \u25e6  But also learns subwords like \u2423re   that might appear in\n    new words like rearrange\n---\nBPE and Unicode\n\nRun on large Unicode corpora, with vocabulary\nsizes of 50,000 to 200,000\nOn individual bytes of UTF-8-encoded text.\n\u25e6  BPE rediscovers 2-byte and common 3-byte UTF-8\n   sequences\n\u25e6  Only 256 possible values of a byte, so no unknown\n   tokens\n\u25e6  (BPE might learn a few illegal UTF-8 sequences\n   across character boundaries, but these can be filtered)\n---\n tps://tiktokenizer.vercel.app/) to see the number of tokens in a giv\n    Visualizing GPT4o tokens\n tence. For example here\u2019s the tokenization of a nonsense sentence we made u\n visualizer uses a center dot to indicate a space:\n                        Tat Dat Duong\u2019s Tiktokenizer visualizer\n\n  Anyhow,\u00b7she's\u00b7seen\u00b7Jane's\u00b7224123\u00b7flowers\u00b7anyhow!\n    The visualization shows colors to separate out words, but of course the true o\n Tokens: 11865, 8923, 11, 31211, 6177, 23919, 885, 220, 19427, 7633, 18887, 147065, 0\n t of the tokenizer is simply a sequence of unique token ids. (In case you\u2019re\n    Most words are tokens, w/initial space\n ested, they were the following 13 tokens: 11865, 8923, 11, 31211, 6177, 2391\n 5, 220, Clitics like \u2019s\n    19427, 7633, 18887, 147065, 0)\n    \u25e6 Are segmented off Jane\n Notice that most words are their own token, usually including the leading spac\n    \u25e6 But part of frequent words like she\u2019s\nitics like \u2019s are segmented off when they appear on proper nouns like Jane, b\n    Numbers segmented into chunks of 3 digits\n counted as part of a word for frequent words like she\u2019s. Numbers tend to\n    Some of this is from preprocessing\n mented into chunks of 3 digits. And some words (like anyhow) are segment\n    \u25e6 regular expressions for chunking digits, stripping clitics\n ferently if they appear capitalized sentence-initially (two tokens, Any and ho\n---\nTokenizing across languages\n\nEven though BPE tokenizers are multilingual\nLLM training data is still vastly dominated by\nEnglish\n Most BPE tokens used for English, leaving less for\n other languages\n Words in other languages are often split up\n---\n odels is vastly dominated by English text, these multilingual BPE tokenizers ten\n ntence from a recipe for plantains, together with an English translation.\n use most of the tokens for English, leaving fewer of them for other languages. Th\n         Tokenization is better in English\n      The English has 18 tokens; each of the 14 words is a token (none of the word\n sult is that they do a better job of tokenizing English, and the other languages ten\n split into multiple tokens):     Tat Dat Duong\u2019s Tiktokenizer visualizer on GPT4o\n get their words split up into shorter tokens. For example let\u2019s look at a Spanis\n         Figure 1: SuperBPE tokenizers encode text much  ~~more ~~      efficiently than BPE, and the\n         gap grows with larger vocabulary size.         Encoding efficiency (y-axis) is measured with\n ntence from a recipe for plantains, together with an English translation.\n      A recipe sentence in two languages\n      In\u00b7a\u00b7deep\u00b7bowl,\u00b7mix\u00b7the\u00b7orange\u00b7juice\u00b7with\u00b7the\u00b7sugar,\u00b7g\n         bytes-per-token, the number of bytes encoded per token on average over a large corpus of text.\n      The English has 18 tokens; each of the 14 words is a token (none of the word\n         In the above text with 40 bytes, SuperBPE uses 7 tokens and BPE uses 13, so the methods\u2019\n                                =    =\n      inger,\u00b7and\u00b7nutmeg.\ne        efficiencies are 40/7       5.7 and 40/13  3.1 bytes-per-token, respectively. In the graph,\n      split into multiple tokens):\n      English: 18 tokens; no words are split into multiple tokens):\n         the encoding efficiency of BPE plateaus early due to exhausting the valuable whitespace-\n         Figure 1: SuperBPE tokenizers encode text much more efficiently than BPE, and the\n         delimited words in the training data. In fact, it is bounded above by the gray dotted line,\n         gap grows with larger vocabulary size.     Encoding efficiency (y-axis) is measured with\n         which shows the maximum achievable encoding efficiency with BPE, if every whitespace-\n      By contrast, the original 16 words in Spanish have been encoded into 33 token\n      In\u00b7a\u00b7deep\u00b7bowl,\u00b7mix\u00b7the\u00b7orange\u00b7juice\u00b7with\u00b7the\u00b7sugar,\u00b7g\n         bytes-per-token, the number of bytes encoded per token on average over a large corpus of text.\n         delimited word were in the vocabulary. On the other hand, SuperBPE has dramatically\n         In the above text with 40 bytes, SuperBPE uses 7 tokens and BPE uses 13, so the methods\u2019\n uch larger number. Notice that many basic words have been broken into pieces\n         better encoding efficiency that continues to improve with increased vocabulary size, as\n                                =                   =\n      inger,\u00b7and\u00b7nutmeg.\n         efficiencies are 40/7       5.7 and 40/13   3.1 bytes-per-token, respectively. In the graph,\n r example it can continue to add common word sequences to treat as tokens to the vocabulary. The\n         hondo, \u2018deep\u2019, has been segmented into h and ondo.         Similarly fo\n         the encoding efficiency of BPE plateaus early due to exhausting the valuable whitespace-\n         different gradient lines show different transition points from learning subword to superword\n         delimited words in the training data. In fact, it is bounded above by the gray dotted line,\n go,     tokens, which always gives an immediate improvement. SuperBPE also has better encoding\n         \u2018juice\u2019, nuez, \u2018nut\u2019 and jenjibre \u2018ginger\u2019):\n      Spanish: 33 tokens; 6/16 words are split\n         which shows the maximum achievable encoding efficiency with BPE, if every whitespace-\n      By contrast, the original 16 words in Spanish have been encoded into 33 token\n         efficiency than a naive variant of BPE that does not use whitespace pretokenization at all.\n         delimited word were in the vocabulary. On the other hand, SuperBPE has dramatically\n         performing well on these languages. Including multi-word tokens promises to be beneficial\n uch larger number. Notice that many basic words have been broken into pieces\n         better encoding efficiency that continues to improve with increased vocabulary size, as\n      En\u00b7un\u00b7recipiente\u00b7hondo,\u00b7mezclar\u00b7el\u00b7jugo\u00b7de\u00b7naranja\u00b7con\n         in several ways: it can lead to shorter token sequences, lowering the computational costs of\n     r example it can continue to add common word sequences to treat as tokens to the vocabulary. The\n         hondo, \u2018deep\u2019, has been segmented into h and ondo.         Similarly f\n         LM training and inference, and may also offer representational advantages by segmenting\n         different gradient lines show different transition points from learning subword to superword\n      \u00b7el\u00b7az\u00facar,\u00b7jengibre,\u00b7y\u00b7nuez\u00b7moscada.\n         text into more semantically cohesive units (Salehi et al., 2015; Otani et al., 2020; Hofmann\n go,     tokens, which always gives an immediate improvement. SuperBPE also has better encoding\n         \u2018juice\u2019, nuez, \u2018nut\u2019 and jenjibre \u2018ginger\u2019):\n         et al., 2021).\n         efficiency than a naive variant of BPE that does not use whitespace pretokenization at all.\n---\nWords  Byte Pair Encoding\nand\nTokens\n---\nWords      Rule-based tokenization\nand        and\nTokens     Simple Unix tools\n---\nRule-based tokenization\n\nAlthough subword tokenization is the norm\nSometimes we need particular tokens\nLike for parsing, where the parser needs\ngrammatical words, or social science\n---\nIssues for rule-based tokenization\nMostly but not always remove punctuation:\n\u25e6  m.p.h., Ph.D., AT&T, cap\u2019n\n\u25e6  prices ($45.55)\n\u25e6  dates (01/02/06)\n\u25e6  URLs (http://www.stanford.edu)\n\u25e6  hashtags (#nlproc)\n\u25e6  email addresses (someone@cs.colorado.edu)\nNumbers are tokenized differently across\nlanguages\n\u25e6  English 555,500.50 = French 555 500,50\nMultiword expressions (MWE)?\n\u25e6  New York, rock \u2019n\u2019 roll\n---\n         One commonly used tokenization standard is known as the Penn Treebank\nnk     kenization standard, used for the parsed corpora (treebanks) released by the\nion      Penn Treebank Tokenization Standard\n       guistic Data Consortium (LDC), the source of many useful datasets. This stan\n       separates out clitics (doesn\u2019t becomes does plus n\u2019t), keeps hyphenated word\n       gether, and separates out all punctuation (to save space we\u2019re showing visible sp\n       \u2018 \u2019 between tokens, although newlines is a more common output):\n\n        Input:     \"The San Francisco-based restaurant,\" they said,\n                   \"doesn\u2019t charge $10\".\n        Output:    \" The San Francisco-based restaurant , \" they said ,\n                   \" does n\u2019t charge $ 10 \" .\n\n         In practice, since tokenization is run before any other language processin\n       needs to be very fast. For rule-based word tokenization we generally use d\n       ministic algorithms based on regular expressions compiled into efficient finite\n---\n           Tokenization in NLTK\n    2  \u2022   W            T\nER     Bird, Loper and Klein (2009), Natural Language Processing with Python. O\u2019Reilly\n              ORDS AND   OKENS\n\n    >>> text =  'That U.S.A. poster-print costs $12.40...'\n    >>> pattern = r'''(?x)         # set flag to allow verbose regexps\n    ...         (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n    ...     | \\w+(?:-\\w+)*           # words with optional internal hyphens\n    ...     | \\$?\\d+(?:\\.\\d+)?%?     # currency, percentages, e.g. $12.40, 82%\n    ...     | \\.\\.\\.               # ellipsis\n    ...     | [][.,;\"'?():_`-]     # these are separate tokens; includes ], [\n    ... '''\n    >>> nltk.regexp_tokenize(text, pattern)\n    ['That',    'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n    Figure 2.8  A Python trace of regular expression tokenization in the NLTK Python-based\n---\n    Sentence Segmentation\n!, ? mostly unambiguous but period \u201c.\u201d is very\nambiguous\n \u25e6  Sentence boundary\n \u25e6  Abbreviations like Inc. or Dr.\n \u25e6  Numbers like .02% or 4.3\nCommon algorithm: Tokenize first: use rules or ML\nto classify a period as either (a) part of the word or\n(b) a sentence-boundary.\n \u25e6  An abbreviation dictionary can help\nSentence segmentation can then often be done by\nrules based on this tokenization.\n---\n  Space-based tokenization\n\nA very simple way to tokenize\n\u25e6   For languages that use space characters between\n    words\n  \u25e6 Arabic, Cyrillic, Greek, Latin, etc., based writing systems\n\u25e6   Segment off a token between instances of spaces\nUnix tools for space-based tokenization\n\u25e6   The \"tr\" command\n\u25e6   Inspired by Ken Church's UNIX for Poets\n\u25e6   Given a text file, output the word tokens and their\n    frequencies\n---\n      Simple Tokenization in UNIX\n (Inspired by Ken Church\u2019s UNIX for Poets.)\n Given a text file, output the word tokens and their frequencies\ntr -sc \u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt          Change all non-alpha to newlines\n      | sort\n      | uniq \u2013c     Sort in alphabetical order\n                          Merge and count each type\n1945 A\n  72 AARON\n  19 ABBESS\n  5 ABBOT    25 Aaron\n ... ...         6  Abate\n                 1  Abates\n                 5  Abbess\n                 6  Abbey\n                 3  Abbot\n             ....  \u2026\n---\nThe first step: tokenizing\n\ntr  -sc  \u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt | head\n\nTHE\nSONNETS\nby\nWilliam\nShakespeare\nFrom\nfairest\ncreatures\nWe\n...\n---\nThe second step: sorting\n\ntr -sc \u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt | sort | head\n\nA\nA\nA\nA\nA\nA\nA\nA\nA\n...\n---\n  More counting\n\n Merging upper and lower case\ntr \u2018A-Z\u2019 \u2018a-z\u2019 < shakes.txt | tr \u2013sc \u2018A-Za-z\u2019 \u2018\\n\u2019 | sort | uniq \u2013c\n Sorting the counts\ntr \u2018A-Z\u2019 \u2018a-z\u2019 < shakes.txt | tr \u2013sc \u2018A-Za-z\u2019 \u2018\\n\u2019 | sort | uniq \u2013c | sort \u2013n \u2013r\n               23243 the\n               22225 i\n               18618 and\n               16339 to\n               15687 of\n               12780 a        What happened here?\n               12163 you\n               10839 my\n               10005 in\n               8954  d\n---\nWords      Rule-based tokenization\nand        and\nTokens     Simple Unix tools\n---\nWords  Corpora\nand\nTokens\n---\nCorpora\n\nWords don't appear out of nowhere!\nA text is produced by\n \u2022 a specific writer(s),\n \u2022 at a specific time,\n \u2022 in a specific variety,\n \u2022 of a specific language,\n \u2022 for a specific function.\n---\nCorpora vary along dimensions like\n\nLanguage: 7097 languages in the world\nIt's important to test algorithms on multiple\nlanguages\nWhat may work for one may not work for\nanother\n---\nCorpora vary along dimensions like\n\nVariety, like African American English\nvarieties\n\u25e6  AAE Twitter posts might include forms like \"iont\" (I\n   don't)\nGenre: newswire, fiction, scientific articles,\nWikipedia\nAuthor Demographics: writer's age, gender,\nethnicity, socio-economic status\n---\nCode Switching\n\nSpeakers use multiple languages in the same\nutterance\nThis is very common around the world\nEspecially in spoken language and related\ngenres like texting and social media\n---\nCode Switching: Spanish/English\n\nPor primera vez veo a @username actually\nbeing hateful! It was beautiful:)\n\n[For the first time I get to see @username\nactually being hateful! it was beautiful:) ]\n---\nCode Switching: Hindi/English\n\ndost tha or ra- hega ... dont wory ... but dherya\nrakhe\n\n[\u201che was and will remain a friend ... don\u2019t worry ...\nbut have faith\u201d]\n---\nCorpus datasheets\n    Gebru et al (2020), Bender and Friedman (2018)\nMotivation:\n \u2022  Why was the corpus collected?\n \u2022  By whom?\n \u2022  Who funded it?\nSituation: In what situation was the text written?\nCollection process: If it is a subsample how was it\nsampled? Was there consent? Pre-processing?\n +Annotation process, language variety, demographics,\netc.\n---\nWords  Corpora\nand\nTokens\n---\nWords  Regular Expressions\nand\nTokens\n---\nRegular expressions are used everywhere\n\n\u25e6   A formal language for specifying text\n    strings\n\u25e6   Part of every text processing task\n  \u25e6  Often a useful pre-processing or text formatting\n     step, for example for BPE tokenization\n\u25e6   Also necessary for data analysis of text\n\u25e6   A widely used tool in industry and\n    academics\n\n                                                     84\n---\nRegular expressions\n\nWe use regular expressions to search for a\npattern in a string\n\nFor example, the Python function\nre.search(pattern,string)\n\nscans through the string and returns the first\nmatch inside it for the pattern\n---\nPython syntax\n\nWe'll show regex as raw string with double quotes:\n\nr\"regex\"\n\nRaw strings treat backslashes as literal characters\nMany regex patterns use backslashes.\n---\nA note about Python regular expressions\n\n\u25e6   Regex and Python both use backslash \"\\\" for\n    special characters. You must type extra backslashes!\n  \u25e6  \"\\\\d+\" to search for 1 or more digits\n  \u25e6  \"\\n\" in Python means the \"newline\" character, not a\n     \"slash\" followed by an \"n\". Need \"\\\\n\" for two characters.\n\u25e6   Instead: use Python's raw string notation for regex:\n  \u25e6  r\"[tT]he\"\n  \u25e6  r\"\\d+\" matches one or more digits\n    \u25e6  instead of \"\\\\d+\"\n\n                                                        87\n---\n  Regular expressions\n\nThe pattern\n\n r\"Buttercup\"\nmatches the substring Buttercup in any string, like\nthe string\n\n I\u2019m called little Buttercup\n---\n  Regular Expressions: Disjunctions\n\nLetters inside square brackets []\n\n   Pattern                              Matches\n   r\"[mM]ary\"                           Mary or mary\n   r\"[1234567890]\"                      Any one digit\nRanges using the dash [A-Z]\n\n  Pattern     Matches\n  r\"[A-Z]\"    An upper case letter     Drenched Blossoms\n  r\"[a-z]\"    A lower case letter      my beans were impatient\n  r\"[0-9]\"    A single digit           Chapter 1: Down the Rabbit Hole\n---\n Regular Expressions: Negation in Disjunction\n\n Carat as first character in [] negates the list\n \u25e6  Note: Carat means negation only when it's first in []\n \u25e6  Special characters (., *, +, ?) lose their special meaning inside []\n\nPattern       Matches                 Examples\nr\"[^A-Z]\"     Not upper case          Oyfn pripetchik\nr\"[^Ss]\"      Neither \u2018S\u2019 nor \u2018s\u2019     I have no exquisite reason\u201d\nr\"[^.]\"       Not a period            Our resident Djinn\nr\"[e^]\"       Either e or ^           Look up ^ now\n---\nKleene star and Kleene plus\n\n baa!\n baaa!\n baaaa! ...\n\nKleene star * (0 or more of previous characters)  Stephen C Kleene\nKleene plus + (1 or more of previous character)\n\n r\"baaa*\"\n r\"baa+\"\n---\nWildcard\n\nThe period means \"any character\"\n\nr\".\"  matches anything\nr\".*\" matches any sequence of 0 or more\nof anything\n---\nRegular Expressions: Anchors ^ $\n\n Pattern       Matches\n r\"^[A-Z]\"     Palo Alto\n r\"\\.$\"        The end.\n r\".$\"         The end?  The end!\n---\n Regular Expressions: More Disjunction\n\n Groundhog is another name for woodchuck!\n The pipe symbol | for disjunction\n\nPattern                       Matches\nr\"groundhog|woodchuck\"        woodchuck\nr\"yours|mine\"                 yours\nr\"a|b|c\"                      = [abc]\nr\"[gG]roundhog|[Ww]oodchuck\"  Woodchuck\n---\n Regular Expressions: Convenient aliases\n\nPattern   Expansion       Matches                 Examples\nr\"\\d\"     [0-9]           Any digit               Fahreneit 451\nr\"\\D\"     [^0-9]          Any non-digit           Blue Moon\nr\"\\w\"     [a-ZA-Z0-9_]    Any alphanumeric or     Daiyu\n                          _\nr\"\\W\"     [^\\w]           Not alphanumeric or _   Look!\nr\"\\s\"     [ \\r\\t\\n\\f]     Whitespace (space,      Look\u2423up\n                          tab)\nr\"\\S\"     [^\\s]           Not whitespace          Look up\n---\nThe iterative process of writing regex's\nFind me all instances of the word \u201cthe\u201d in a text.\n\nthe\nMisses capitalized examples\n\n[tT]he\nIncorrectly returns other or Theology\n\n\\W[tT]he\\W\n---\nFalse positives and false negatives\n\nThe process we just went through was\nbased on fixing two kinds of errors:\n\n1.     Not matching things that we should have\n       matched (The)\nFalse negatives\n\n2.     Matching strings that we should not have\n       matched (there, then, other)\nFalse positives\n---\nCharacterizing work on NLP\n\nIn NLP we are always dealing with these kinds of\nerrors.\nReducing the error rate for an application often\ninvolves two antagonistic efforts:\n \u25e6  Increasing coverage (or recall) (minimizing false\n    negatives).\n \u25e6  Increasing accuracy (or precision) (minimizing false\n    positives)\n---\nRegular expressions play a surprisingly\nlarge role\n\nWidely used in both academics and industry\n\n1.     Part of most text processing tasks, even for\n       big neural language model pipelines\n\u25e6     including text formatting and pre-processing\n2.     Very useful for data analysis of any text data\n\n99\n---\nWords  Regular Expressions\nand\nTokens\n---\nWords     Substitutions, Capture\nand       Groups, and Lookahead\nTokens\n---\nRegex Substitutions in Python\n\nTo change every instance of cherry to apricot in\nstring:\nre.sub(r\"cherry\", r\"apricot\",\nstring)\n\nUpper case all examples of a name:\nre.sub(r\"janet\", r\"Janet\", string)\n---\n Substitutions often need capture\n groups\n\nChange US format dates (mm/dd/yyyy) to\nEU : (dd-mm-yyyy)\nPattern to match US:\nr\"\\d{2}/\\d{2}/\\d{4}\"\nHow to specify in the replacement that we\nwant to swap the date and month values?\n---\nCapture group\n\nUse parentheses to capture (store) the values\nthat we matched in the search,\nGroups have numbers\nIn repl, we refer back to that group with a\nnumber command.\n---\nCapture group\n\nre.sub(r\"(\\d{2})/(\\d{2})/(\\d{4})\",\nr\"\\2-\\1-\\3\", string)}\nParens ( and ) around the two month digits, the\ntwo day digits, and the four year digits,\nThis stores\n \u25e6  the first 2 digits in group 1,\n \u25e6  the second 2 digits in group 2,\n \u25e6  final digits in group 3.\nThen in the repl string,\n \u25e6  \\1, \\2, and \\3, refer to the 1st, 2nd, and 3rd registers.\n---\nThat regex will\n\nmap\nThe date is 10/15/2011\nto\n  The date is 15-10-2011\n---\n But suppose we don't want to capture?\n\nParentheses have a double function: grouping terms, and\ncapturing\nNon-capturing groups: add a ?: after paren:\n r\"(?:some|a  few) (people|cats) like some \\1/\"\n matches\n \u25e6 some  cats  like  some  cats\n but not\n \u25e6 some  cats  like  some  some\n---\nLookahead assertions\n\n(?= pattern) is true if pattern matches, but\nis zero-width; doesn't advance character\npointer\n(?! pattern) true if a pattern does not\nmatch\nHow to capture the first word on the line, but\nonly if it doesn\u2019t start with the letter T:\nr\"\u02c6(?![tT])(\\w+)\\b\"\n---\nSimple Application: ELIZA\nEarly NLP system that imitated a Rogerian\npsychotherapist\n\u25e6 Joseph Weizenbaum, 1966.\n\nUses pattern matching to match, e.g.,:\n\u25e6 \u201cI need X\u201d\nand translates them into, e.g.\n\u25e6 \u201cWhat would it mean to  you if you  got X?\n---\nSimple Application: ELIZA\nMen are all alike.\nIN WHAT WAY\nThey're always bugging us about something or\nother. CAN YOU THINK OF A SPECIFIC EXAMPLE\nWell, my boyfriend made me come here.\nYOUR BOYFRIEND MADE YOU COME HERE\nHe says I'm depressed much of the time.\nI AM SORRY TO HEAR YOU ARE DEPRESSED\n---\nHow ELIZA works\n\n r.sub(r\".*  I\u2019M (depressed|sad) .*\",r\"I AM\nSORRY TO HEAR YOU   ARE \\1\",input)\n r.sub(r\".*  I AM (depressed|sad) .*,r\"WHY\nDO YOU THINK YOU  ARE   \\1\",input)\n r.sub(r\".*  all  .*\",r\"IN WHAT WAY?\",input)\n r.sub(r\".*  always .*\",r\"CAN YOU THINK OF  A\nSPECIFIC EXAMPLE?\",input)\n---\nWords     Substitutions, Capture\nand       Groups, and Lookahead\nTokens\n\n",
        "quiz": [
            {
                "question_text": "Which of the following best describes Words And Tokens - Extended?",
                "answers": [
                    {
                        "text": "A control structure that executes code conditionally",
                        "is_correct": false,
                        "explanation": null
                    },
                    {
                        "text": "A data structure used for storing collections of items",
                        "is_correct": false,
                        "explanation": null
                    },
                    {
                        "text": "Key concepts from Words and tokens - EXTENDED.pdf",
                        "is_correct": true,
                        "explanation": null
                    },
                    {
                        "text": "A function that performs mathematical operations",
                        "is_correct": false,
                        "explanation": null
                    }
                ],
                "topic": "Words And Tokens - Extended",
                "subtopic": "Main Content",
                "concepts": [
                    "Words And Tokens - Extended"
                ],
                "difficulty": "easy",
                "explanation": "Words And Tokens - Extended: Key concepts from Words and tokens - EXTENDED.pdf"
            },
            {
                "question_text": "Which of the following best describes Words And Tokens - Extended?",
                "answers": [
                    {
                        "text": "A data structure used for storing collections of items",
                        "is_correct": false,
                        "explanation": null
                    },
                    {
                        "text": "Key concepts from Words and tokens - EXTENDED.pdf",
                        "is_correct": true,
                        "explanation": null
                    },
                    {
                        "text": "A control structure that executes code conditionally",
                        "is_correct": false,
                        "explanation": null
                    },
                    {
                        "text": "A function that performs mathematical operations",
                        "is_correct": false,
                        "explanation": null
                    }
                ],
                "topic": "Words And Tokens - Extended",
                "subtopic": "Main Content",
                "concepts": [
                    "Words And Tokens - Extended"
                ],
                "difficulty": "medium",
                "explanation": "Words And Tokens - Extended: Key concepts from Words and tokens - EXTENDED.pdf"
            },
            {
                "question_text": "Which of the following best describes Words And Tokens - Extended?",
                "answers": [
                    {
                        "text": "A control structure that executes code conditionally",
                        "is_correct": false,
                        "explanation": null
                    },
                    {
                        "text": "Key concepts from Words and tokens - EXTENDED.pdf",
                        "is_correct": true,
                        "explanation": null
                    },
                    {
                        "text": "A data structure used for storing collections of items",
                        "is_correct": false,
                        "explanation": null
                    },
                    {
                        "text": "A function that performs mathematical operations",
                        "is_correct": false,
                        "explanation": null
                    }
                ],
                "topic": "Words And Tokens - Extended",
                "subtopic": "Main Content",
                "concepts": [
                    "Words And Tokens - Extended"
                ],
                "difficulty": "hard",
                "explanation": "Words And Tokens - Extended: Key concepts from Words and tokens - EXTENDED.pdf"
            },
            {
                "question_text": "Which of the following best describes Words And Tokens - Extended?",
                "answers": [
                    {
                        "text": "A data structure used for storing collections of items",
                        "is_correct": false,
                        "explanation": null
                    },
                    {
                        "text": "A control structure that executes code conditionally",
                        "is_correct": false,
                        "explanation": null
                    },
                    {
                        "text": "Key concepts from Words and tokens - EXTENDED.pdf",
                        "is_correct": true,
                        "explanation": null
                    },
                    {
                        "text": "A function that performs mathematical operations",
                        "is_correct": false,
                        "explanation": null
                    }
                ],
                "topic": "Words And Tokens - Extended",
                "subtopic": "Main Content",
                "concepts": [
                    "Words And Tokens - Extended"
                ],
                "difficulty": "easy",
                "explanation": "Words And Tokens - Extended: Key concepts from Words and tokens - EXTENDED.pdf"
            },
            {
                "question_text": "Which of the following best describes Words And Tokens - Extended?",
                "answers": [
                    {
                        "text": "Key concepts from Words and tokens - EXTENDED.pdf",
                        "is_correct": true,
                        "explanation": null
                    },
                    {
                        "text": "A control structure that executes code conditionally",
                        "is_correct": false,
                        "explanation": null
                    },
                    {
                        "text": "A function that performs mathematical operations",
                        "is_correct": false,
                        "explanation": null
                    },
                    {
                        "text": "A data structure used for storing collections of items",
                        "is_correct": false,
                        "explanation": null
                    }
                ],
                "topic": "Words And Tokens - Extended",
                "subtopic": "Main Content",
                "concepts": [
                    "Words And Tokens - Extended"
                ],
                "difficulty": "medium",
                "explanation": "Words And Tokens - Extended: Key concepts from Words and tokens - EXTENDED.pdf"
            }
        ]
    },
    "data/raw/Week9 - LGT.pdf": {
        "metadata": {
            "file_name": "Week9 - LGT.pdf",
            "file_type": "pdf",
            "content_length": 54823,
            "language": "en",
            "extraction_timestamp": "2025-11-26T02:00:46.897451+00:00",
            "timezone": "utc"
        },
        "content": "                    KING'S\n   Applications of  .\n                    College\n   LLM \u2013 Part I:    LONDON\n   Retrieval\n   Augmented\n   Generation\n   (RAG)\n\n   Week 9 - LGT     BVSH HOVSE\n\nG  Dr Lin Gui\n   Lin.1.gui@kcl.ac.uk\n---\nLearning outcomes\n\n\u26ab By the end of this topic, you will be able to:\n\n  \u26ab  Understand the core concepts of Retrieval-Augmented Generation and how it\n     differs from standard LLM approaches.\n\n  \u26ab  Build and configure a basic RAG pipeline using embeddings, retrievers, and\n     generators.\n\n  \u26ab  Evaluate and optimize RAG performance through effective data preparation,\n     chunking, and retrieval strategies.\n\n2\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n3\n---\n                                                        RAG overview\n\n                                                                               \u26ab                                  When answering questions\n                                                                                                                  or generating text, it first\n                                                                                                                  retrieves relevant\n                                                                                                                  information from a large\n                                                                                                                  number of documents, and\n                                                                                                                  then LLMs generates\n                                                                                                                  answers based on this\n                                                                                                                  information.\n\n                                                                               \u26ab                                  By attaching an external\n                                                                                                                  knowledge base, there is\n                                                                                                                  no need to retrain the\n                                                                                                                  entire large model for each\n                                                                                                                  specific task.\n\n                                                                               \u26ab                                  The RAG model is\n                                                                                                                  especially suitable for\n\n                                           Input        Query                       Indexing                      knowledge-intensive tasks.\nUser                                        How do you evaluate the fact       Documents\n                                            that OpenAI's CEO, Sam Altman,              Chunks Vectors\n\n        Output                              by the board in just three days,\n                                            and then was rehired by the                 embeddings\n                                            company, resembling a real-life\n                                            version of \"Game of Thrones\" in             Retrieval\n\nwithout RAG                                                                    Relevant Documents\n\nfuture events. Currently, I do not have     LLM         Generation\n\nand rehiring of OpenAI's CEO ..            Question:                           Chunk 1: \"Sam Altman Returns to                                4\nwith RAG\n\nthe company's future direction and        based on the following information:     Chunk 2: \"The Drama Concludes? Sam\n                                                                                  Altman to Return as CEO of OpenAl,\nand turns reflect power struggles and     Chunk 2:                                Board to Undergo Restructuring\"\n                                          Chunk 3 :\nOpenAl...                                 Combine Context                         OpenAl Comes to an End: Who Won\n             Answer                       and Prompts                             and Who Lost?\"\n---\n   Symbolic Knowledge or Parametric Knowledge\n\n    \u26ab Ways to optimize LLMs.\n\n    \u26ab Prompt Engineering    This week\n\n    \u26ab Instruct / Fine-tuning\n\n    \u26ab  Retrieval-Augmented\n       Generation\n\n    Week 7    Week 8\n\nExternal Knowledge\n     Required\n    High     Modular RAG\n\n             multiple modules    Retriever Fine-tuning\nAdvanced RAG                     Collaborative Fine-tuning\n\noptimization                     All of the above\n Naive RAG                       RAG             Generator Fine-tuning\n\n   XoT Prompt     Prompt Engineering  Fine-tuning          5\n e.g. CoT, ToT\nFew-shot Prompt\n    Low           Standard Prompt                      Model Adaptation\n           Low                                       High  Required\n---\nRAG vs Fine-tuning\n\n Data Processing\n                ddling.    datasets, and limited datasets may not result\n\n 6\n\n higher latency.    retrieval, resulting in lower latency.\n---\nRAG Application\n\n\u26ab Scenarios where RAG is applicable:\n\n  \u26ab  Long-tail distribution of data\n\n  \u26ab  Frequent knowledge updates\n\n  \u26ab  Answers requiring verification and traceability\n\n  \u26ab  Specialized domain knowledge\n\n  \u26ab  Data privacy preservation\n\nQuestion Answering     Fact checking    Dialog systems    Summarisation\n\nMachine translation    Code generation    Sentiment Analysis    Commonsense\n                                                                reasoning\n\n                                                                           7\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  Foundation of information Retrieval\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 8\n---\n   Foundation of information Retrieval\n\n   \u26ab What is information Retrieval?\n\n     \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n        returns those items to the user, typically in list form sorted per computed\n        relevance#\n\n   \u26ab Three main questions in information retrieval:\n\n     \u26ab  How to map the text into features (Embedding method)\n\n     \u26ab  How to measure the similarity between features (IR Modelling)\n\n     \u26ab  How to do it efficiently (Indexing)\n\n[#] Qiaozhu Mei and Dragomir Radev, \u201cInformation Retrieval,\u201d The Oxford Handbook of Computational Linguistics,\n2\u207f\u1d48 edition, Oxford University Press, 2016.    9\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n10\n---\nDiscrete representation\n\n\u26ab  In discrete representation, for both query and document, we assign each word a\n   specific dimension. If a word appears query/document, then value of the\n   corresponding dimension is:\n\n    \u26ab  In Binary representation: 1\n\n    \u26ab  In TF (term frequency) based representation: t (how many times this word\n       appears within the query/documents)\n\n    \u26ab  In TF-IDF (inverse document frequency) based representation: tlog(n/x)\n\n       \u26ab  Here, t is term frequency, n is number of documents, x is the number of\n          documents which contains this term.\n\n11\n---\nDiscrete representation (example)\n\n\u26ab We have the following documents:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n\u26ab After pre-processing:\n\n  \u26ab  D1 = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d.\n\n  \u26ab  D2 = \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n  \u26ab  D3 = \u201cshipment\u201d, \u201cgold\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n                                                  12\n---\nDiscrete representation (example)\n\n \u26ab Building vocabulary:\n\n   \u26ab V = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d, \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n \u26ab  Detect the feature for each document. If the feature occurs, the corresponding\n    value is \u20181\u2019, otherwise \u20180\u2019 (binary feature):\n\n           shipment  gold  damage  fire          delivery  silver  arrive  truck\n     D1     1        1     1       1             0         0       0       0\n     D2     0        0     0       0             1         1       1       1\n     D3     1        1     0       0             0         0       1       1\n\n 13\n---\nDiscrete representation (example)\n\n\u26ab  Definition \u2013 term frequency (TF):\n\n    \u26ab  \ud835\udc61 - how many times the term appears in the document\n\n\u26ab  Example:\n\n    \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n    \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n    \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n              shipment  gold  damage  fire  delivery  silver    arrive  truck\n        D1     1        1     1       1     0              0     0      0\n        D2     0        0     0       0     1              2     1      1\n        D3     1        1     0       0     0              0     1      1\n\n                                                                             14\n---\nDiscrete representation (example)\n\n\u26ab Definition \u2013 inverse document frequency (IDF):\n\n  \u26ab  \ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc5b/\ud835\udc65) \u2013 n is number of documents, x is the number of documents which\n     contains this term\n\n\u26ab Example:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n          shipment     gold  damage  fire  delivery  silver     arrive  truck\n          0.176     0.176    0.477   0.477  0.477    0.477      0.176   0.176\n                       Inverse document frequency vector\n\n                                                                             15\n---\nDiscrete representation (example)\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1      1         1        1      1         0        0           0         0\n D2      0         0        0      0         1        2           1         1\n D3      1         1        0      0         0        0           1         1\n                           Term frequency matrix\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n        0.176     0.176    0.477   0.477    0.477     0.477      0.176     0.176\n                           Inverse document frequency vector\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1     0.176     0.176    0.477   0.477     0        0           0         0\n D2      0        0         0      0        0.477     0.954      0.176     0.176\n D3     0.176     0.176     0      0         0        0          0.176     0.176\n\n                            TF-IDF Matrix\n                                                                                16\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n17\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n18\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n19\n---\n   Dense Passage Retrieval\n\n   \u26ab  Encode questions and text passages into continuous vectors (embeddings) and\n      retrieve passages using vector similarity instead of keyword overlapping.\n\n   \u26ab  Train directly on question\u2013passage pairs, using in-batch negatives to improve\n      efficiency.\n\n                 Question q    Passage p                                In each batch, there are multiple\n\n                 BERTQ         BERTp                Passage encoder     question\u2013answer pairs, both\n      Question encoder                                                  matched and unmatched. Matched\n                                                                        pairs should have similar\n\n                          OOOOOOOO  0OOOOOOO                            representations, while unmatched\n                                                                        pairs should have representations\n                          $h_q$                                         that are far apart.\n                                                Training phase\n\n      Similarity score: dot product  (q, =  Fine-tune two encoders\nhttps://aclanthology.org/2020.emnlp-main.550.pdf                                                 20\n---\n   ReContriever\n\n   \u26ab  What if we don\u2019t have annotated data (Matched and unmatched QA-pair).\n\n   \u26ab  Using pseudo-examples: For each passage/document p, create an augmented\n      version p\u2032. Then treat (p, p\u2032) as a positive pair:\n\n       \u26ab  Masking words (random word masking)\n\n       \u26ab  Span deletion\n\n       \u26ab  Back-translation Sentence\n\n       \u26ab  Reordering Adding noise\n\n       \u26ab  Perturbations Cropping (taking a subset of sentences)\n\nhttps://aclanthology.org/2023.findings-acl.695.pdf    21\n---\n  Using API\n\n   \u26ab  There are many APIs could do this job, for example, Mistral AI:\n\n                       YMISTRAL EMBED API\n\n                       8OPEN IN COLAB\n\n   \u26ab  Example: link    How to Generate Embeddings\n                       To generate text embeddings using Mistral Al's embeddings APl, we can make a request to the APl endpoint and specify the\n                       embedding model mistra1-embed , along with providing a list of input texts. The APl will then return the corresponding\n                       embeddings as numerical vectors, which can be used for further analysis or processing in NLP applications.\n\n   \u26ab Some other options:    PYTHON TYPESCRIPT      CURL                                                           OUTPUT\n                            import os\n                            from mistralai import Mistral\n          Sentence Bert     api_key = os.environ[\"MISTRAL_API_KEY\"]\n     \u26ab                      model = \"mistral-embed\"\n                            client = Mistral(api_key=api_key)\n\n     \u26ab    SimCSE            embeddings_batch_response = client.embeddings.create(\n                            model=model,\n                            inputs=[\"Embed this sentence.\", \"As well as this one.\"],\n\n     \u26ab    \u2026\u2026                )\n                            The output is an embedding object with the embeddings and the token usage information.\n\n                            Let's take a look at the length of the first embedding:\n\n                            PYTHON TYPESCRIPT CURL\n                            len(embeddings batch response.data[0].embedding)\n\nhttps://docs.mistral.ai/capabilities/embeddings    22\n---\nIR Modelling\n\n\u26ab What is information Retrieval?\n\n  \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n     returns those items to the user, typically in list form sorted per computed\n     relevance\n\n\u26ab Three main questions in information retrieval:\n\n  \u26ab  How to map the text into features (Embedding method)\n\n  \u26ab  How to measure the similarity between features (IR Modelling)\n\n  \u26ab  How to do it efficiently (Indexing)\n\n23\n---\nIR Modelling\n\n\u26ab  In IR modelling, we use different metric to measure the similarity/distance\n   between the query and given documents. The target is to find the top-k relevant\n   documents based on the given query.\n\n    \u26ab  Cosine similarity (for both Discrete & Continuous representation)\n\n    \u26ab  Jaccard distance (for Discrete representation only)\n\n    \u26ab  BM25 (for Discrete representation only)\n\n24\n---\nCosine similarity\n\n\u26ab Cosine similarity\n                           \u03c3\ud835\udc5b \ud835\udc65\ud835\udc56 \ud835\udc66\ud835\udc56\n                   \ud835\udc36\ud835\udc5c\ud835\udc60 \ud835\udc65, \ud835\udc66 = \u03c3\ud835\udc5b  \ud835\udc56=1 \u03c3\ud835\udc5b\n                                   \ud835\udc56=1(\ud835\udc65\ud835\udc56 )2  \ud835\udc56=1(\ud835\udc66\ud835\udc56 )2\n\n\u26ab Considering\n  \u2212  D1 = [1,1,1,1,0,0,0,0]\n  \u2212  D3 = [1,1,0,0,0,0,1,1]\n                                 \ud835\udc36\ud835\udc5c\ud835\udc60(D1,D3)=1/2\n\n25\n---\nJaccard similarity\n\n\u26ab Only considering if there is over lapping or not. We don\u2019t care about the value.\n\n\u26ab For example:            C1    sim(cl,c2)    C1  C2\n\n  \u26ab  \ud835\udc45\ud835\udc65 = [2,0,3,3]     C2\n\n  \u26ab  \ud835\udc45\ud835\udc66 = [1,1,0,5]     C3                    JACCARD SIMILARITY\n                                                  2    0.5\n                                              4  2+1+1\n\n\u26ab Jaccard similarity: \ud835\udc60\ud835\udc56\ud835\udc5a \ud835\udc65, \ud835\udc66 = \ud835\udc79\ud835\udc99\u2229\ud835\udc79\ud835\udc9a\n                                    \ud835\udc79\ud835\udc99\u222a\ud835\udc79\ud835\udc9a\n\n                                               26\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              hyperparameters         Average length of all docs\n                                                                                  27\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b      \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1      \ud835\udc56               1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n     Maybe\u2026a bit confusing                            Average length of all docs\n     Can you speak in English?  hyperparameters\n                                                                                  28\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b             \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1             \ud835\udc56        1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n Relax\u2026it is pretty simple actually    hyperparameters    Average length of all docs\n\n                                                                                  29\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:  IDF term in Q (is it an important word?)\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| )\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              The \u2018percentage\u2019 of querying words in D\n\n                                                                                  30\n---\nIndexing\n\n\u26ab Next question, how to do it efficiently (Indexing)\n\n\u26ab  Suppose we have 1k queries, and there are 1 billion documents in knowledge\n   based, how many times of comparison we need?\n\n\u26ab  1k x 1b\n    It is a really huge number.\n    In real world scenario, it could be even larger\n    If there is only one important task in information retrieval, it must be\n    \u201cindexing\u201d\n\n31\n---\nIndexing - Discrete representation\n\n\u26ab  Inverted index\n\n\u26ab  Since the discrete representation is sparse (most dims are zero), we can build\n   inverted index. For each word, we build a link list to store all the documents\n   contain this word.\n\n\u26ab  For the given query, the complexity is now only related to the #unique words in\n   the query. (In most queries, the size is just few words)\n   doclD                          geo-scopelD              geo-scopelD   docID\n     1                            Europe                   Europe        1 2 7\n     2                                Europe               France        3\n     3                                France               Portugal      5\n     4                                England              England       4\n     5                                Portugal             Quebec        6\n     6                                Quebec               Spain         8\n     7                                Europe\n     8                                Spain\n                     Forward Index                         Inverted Index\n                                                                                 32\n---\n   Indexing - Continuous representation\n\n   \u26ab  In continuous representation, it might be a bit complex. There is no sparse\n      representation anymore.\n\n   \u26ab  We can use the following method to speed up the searching.\n\n       \u26ab  Vector compression \u2013 reduce the size of vectors\n\n       \u26ab  Hierarchical clustering \u2013 in each layer only search the nearest cluster\n                                          Clustering the documents first, and then,\n                                          Only consider the nearest centroid during the searching\n          voronoi cells  xq  Centroids                        voronoi cells  xq    Centroids\n                         o\n                             o                                                     o\n                  e\n                         o\n                  o          \u00a9    o                           9                  o\n                                  -\n                       o                                           o\n         Pause (k)\n\nhttps://www.pinecone.io/learn/series/faiss/faiss-tutorial/                                       33\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n34\n---\nNaive RAG\n\n\u26ab  Step 1 \u2013 indexing\n\n    \u26ab  Divide the document into even chunks, each chunk being a piece of the\n       original text.\n\n    \u26ab  Using the encoding model to generate an embedding for each chunk.\n\n    \u26ab  Store the Embedding of each block in the vector database.\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  Retrieve the k most relevant documents using vector similarity search.\n\n\u26ab  Step 3 \u2013 Generation\n\n    \u26ab  The original query and the retrieved text are combined and input into a LLM\n       to get the final answer\n                                                                             35\n---\n  Naive RAG\n\n    \u26ab Step 1 \u2013 indexing\n\n    \u26ab Step 2 \u2013 Retrieval\n\n    \u26ab Step 3 \u2013 Generation\n\n                                    Offline\n\nDocuments  Document Chunks  Vector Database\n    8                                      36\n   User  Query    Related DocumentChunks\n                            Frozen\n    Augmented Prompt        LLM\n---\nAdvanced RAG\n\n  \u26ab Step 1 \u2013 indexing\n\n  \u26ab + index optimization\n\n  \u26ab + pre-retrieval process\n\n  \u26ab Step 2 \u2013 Retrieval\n\n  \u26ab +post-retrieval process\n\n  \u26ab Step 3 \u2013 Generation\n\n  URLS  PDFs  Database\n     Documents               Document Chunks       Vector Database\n                             Fine-grained Data Cleaning\n                             Sliding Window /Small2Big\n                             Add File Structure\n                             Query Rewrite/Clarifcation\n  User    Query              Retriever Router                          37\n                              Pre-retrieval    Related Document Chunks\n\n  Prompt              LLM                        Rerank  Filter  Prompt Compression\n                                                         Post-retrieval\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Sliding windows\n\n    \u26ab  + index optimization       Fine-grained segmentation\n\n    \u26ab  + pre-retrieval process    Adding metadata\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n38\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Sliding windows\n\n      \u26ab  + index optimization       Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process    Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n                                    Document\n\n         0                200 100   300 200  400 300  500\n\n  Split the doc into chunks, and ensure there is over lapping between chunks (WHY?)\n                                                                                   39\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                                      Sliding windows\n\n      \u26ab  + index optimization          Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                            Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n\n                          Document     Section 1             Paragraph 1.1\n\n                                       Section 2             Paragraph 1.2\n\n                          Searching on fine-grained text     Paragraph 1.3\n                                                                           40\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                               Sliding windows\n\n      \u26ab  + index optimization                        Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                     Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation                             Web page    Publishing date\n\n                                                                    Title\n     The metadata is the aspects of each chunk.\n     It will help both retriever and generator to                Parents node\n     improve the performance.\n\n  41\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process    Summarization\n\n\u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n    \u26ab  +post-retrieval process    Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n\n42\n---\nAdvanced RAG\n\n   \u26ab  Step 1 \u2013 indexing                    Retrieve routes\n\n       \u26ab  + index optimization\n\n       \u26ab  + pre-retrieval process          Summarization\n\n   \u26ab  Step 2 \u2013 Retrieval                   Rewriting\n\n       \u26ab  +post-retrieval process          Confidence judgment\n\n   \u26ab  Step 3 \u2013 Generation                  Instead of one flat \u201cretrieve chunks by embeddings\u201d step, you can:\n\n                                           Searching doc first\n\n  Retrieve routes = multiple retrieval     Query    Document    Chunk\n  paths that a RAG system can choose                            Searching chunks\n  from, depending on query intent, data                         within the doc\n  type, or document structure.\n                                                                                                             43\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing                       Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process             Summarization\n\n\u26ab  Step 2 \u2013 Retrieval                      Rewriting\n\n    \u26ab  +post-retrieval process             Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n                                           Document\n                                                           Summarise first\n                                  Query    Summarisation\n\n                                  Searching in smmarisation\n                                  instead of full documents\n                                                                          44\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing               Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process     Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval              Rewriting\n\n      \u26ab  +post-retrieval process     Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n        Benefits:                    Query\n        a more explicit query             Rewrite the query first\n        a more keyword-rich query    Rewrite the\n        a more structured query      query      Searching\n        multiple diverse sub-queries\n                                     Searching by re-written query\n                                                                  45\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process    Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n      \u26ab  +post-retrieval process    Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n  Query     Document             LLM            Confidence checking    Output\n\n            Confirm the Confidence before output\n             By similarity scores\n             By LLM Confidence scores                                        46\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing           Re-order\n\n    \u26ab  + index optimization    Filter content retrieval\n\n    \u26ab  + pre-retrieval process\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n47\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing           Re-order\n\n      \u26ab  + index optimization    Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation         Evidence#1  Evidence#2  Evidence#3  Question\n\n  LLMs is sensitive with the input order\n  The early input chunks has higher weights\n  How to organize the searched evidence for final output is\n  important\n\n  48\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Re-order\n\n      \u26ab  + index optimization       Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process    Evidence#1  Evidence#2  Evidence#3  Question\n\n  \u26ab  Step 3 \u2013 Generation\n\n                                                           Evidence#1  Evidence#3  Question\n\n  To avoid possible hallucination, filtering the irrelevant\n  evidences.\n\n                                                                                           49\n---\nModular RAG\n\n   Na\u00ef\n\u26ab  ve RAG\n\n      Read           Retrieve    Generate\n\n\u26ab  DSP\n\n   Demonstrate       Search      Predict    Generate\n\n\u26ab  Rewrite-Retrieve-Read\n\n      Rewrite        Retrieve    Read\n\n\u26ab  Retrieve-then-read\n\n   Retrieve          Read        Generate\n\n50\n---\n   Different RAG Paradigms\n\n    Modules\n    8 C 8 E2    Search\nUser Query  Documents    User Query  Documents    Routing    Predict\n\n    Indexing    Query Routing    Indexing    Rewrite  RAG  Rerank\n\n  Read\n                               Fusion\n Memory\n\n    \\Post-Retrieval    Patterns\n \u2192|l|+                               51\nSummary                        Retrieve\n\n    Output    Output\n\n    Naive RAG    Advanced RAG    Modular RAG\n---\nKey problems in RAG\n\n \u26ab  How to retrieve\n\n \u26ab  When to retrieve\n\n \u26ab  How to use the retrieved information\n\n 52\n---\nHow to retrieve\n\n \u26ab By using the information on different structuration levels\n\n \u26ab  Token level         It excels in handling long-tail and cross-domain issues with high\n                        computational efficiency, but it requires significant storage.\n\n \u26ab  Phrase level\n\n \u26ab  Chunk level         The search is broad, recalling a large amount of information, but with\n                        low accuracy, high coverage but includes much redundant information.\n\n \u26ab  Entity level\n\n \u26ab  Knowledge level     Richer semantic and structured information, but the retrieval efficiency\n                        is lower and is limited by the quality of KG.\n\n 53\n---\n   When to retrieve\n\n    \u26ab Two questions:\n\n    \u26ab When we need to retrieve information to support the QA\n\n    \u26ab How many times we need to retrieve the information\n\n    \u26ab Solution#1: Conducting once search during the reasoning process.\n\n    High efficiency, but low relevance of the\n    retrieved documents\n\n    Retrieved document d.\n              Jobs cofounded     Jobs was raisedd;    Jobs is thex    apple\n  Retriever    Apple in his      by adopted...        CEO of        pearnot\n             parents' garage\n Document    Input                 Steve Jobs         Jobs is the     apple    apple\nRetrieval    Reformulation       passed away...       CEO of        pearnot    pearnot    54\nTest Context X    Black-box      Jobs cofoundedJobs is the            apple\n Jobs is the    LM                  Apple...          CEO of           pear\n   CEO of _                                           Ensemble          not\n    Apple\n---\n  When to retrieve\n\n   \u26ab  Two questions:\n\n   \u26ab  When we need to retrieve information to support the QA\n\n   \u26ab  How many times we need to retrieve the information\n\n   \u26ab Solution#2: Adaptively conduct the search.\n\n   Balancing efficiency and information\n   might not yield the optimal solution\n\n Search results:Dx               Retriever\n [1]:Search results:Dq2\n [2]:[1]:Search results:Dq3\n [2]:[1]: ...\nP     [2]: ..                             x\n     x Generate a summary about Joe Biden.\nAy1 Joe Biden attended           q2                55\n Q2[Search(Joe Biden University)]\n y2tthe University of Pennsylvania, where he earned\n q3[Search(Joe Biden degree)]    q3\n y3 a law degree.\n---\n When to retrieve\n\n  \u26ab  Two questions:\n\n  \u26ab  When we need to retrieve information to support the QA\n\n  \u26ab  How many times we need to retrieve the information\n\n  \u26ab Solution#3: Retrieve once for every N tokens generated.\n\n  A large amount of information with low\n  efficiency and redundant information.\n\n    Masked Language Modelling:\n    Bermuda Triangle is in the    western part\n  <MASK> of the Atlantic Ocean.\nPretraining    Atlas\nFew-shot\n          Fact checking:\nBermuda Triangle is in the western                                False    56\n      part of the Himalayas.            The Bermuda\n                                    Triangle is an urban\n                                    legend focused on a\n                                      loosely-defined\n       Question answering:             region in the       Western part of the\n  Where is the Bermuda Triangle?    western part of the    North Atlantic Ocean\n                                       North Atlantic\n                                           Ocean.\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 57\n---\nKey Technologies\n\n\u26ab  Data indexing optimization\n\n\u26ab  Structured Corpus\n\n\u26ab  Retrieval Source Optimization\n\n\u26ab  KG as a Retrieval Data Source\n\n\u26ab  Query Optimization\n\n\u26ab  Embedding Optimization\n\n\u26ab  Fine-tuning on RAG\n\n58\n---\n Data indexing optimization\n\n  \u26ab Chunk optimization\n\n  \u26ab Small-2-big: Embedding at sentence level expand the window during generation\n\n  process.\n\n                   Embed Sentence \u2192 Link to Expanded Window\n\n                              Continuous observation of the Atlantic meridional\n                              overturning circulation (AMOC) has improved the\n                              understanding of its variability (Frajka-Williams et al.,    What the LLM Sees\n                              2019), but there is low confidence in the quantification\n                              of AMOC changes in the 20th century because of low\n                   Embeddingagreement in quantitative reconstructed and simulated\n                              trends. Direct observational records since the\n                   Lookup     mid-2000s remain too short to determine the relative\nQuestion:                     contributions of internal variability, natural                                59\n                              forcing and anthropogenic forcing to AMOC change\nWhat are the                  (high confidence). Over the 21st century, AMOC wil\nconcerns                      very likely decline for all SSP scenarios but will not\n                              involve an abrupt collapse before 2100. 3.2.2.4 Sea Ice\nsurrounding the               Changes\nAMOC?                         Sea ice is a key driver of polar marine life, hosting        What the LLM Sees\n                              unique ecosystems and affecting diverse marine\n                              organisms and food webs through its impact on light\n                              penetration and supplies of nutrients and organic\n                              matter (Arrigo, 2014)\n---\nData indexing optimization\n\n\u26ab  Chunk optimization\n\n\u26ab  Sliding window: sliding chunk covers the entire text, avoiding semantic ambiguity.\n\n                          Maintain overlap for\n                          contextual continuity\n\nLoaded large\ndocument  Dividing into  Merging units into\n          compact units  larger chunks\n\n60\n---\nData indexing optimization\n\n \u26ab  Chunk optimization\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 61\n---\nStructured Corpus\n\n \u26ab  Adding meta-data: adding meta-data in the query searching to improve retrieval\n    accuracy, provide context during chunking, and enables filtering\n\n                             Indexed documents\n                                                                        Filtered subset of\n                                                                        documents    Most relevant\n    Did we implement any new                                                           documents\n       policies in 2021?     year: 2020, content: ...\n\n                             year: 2020, content:                       year: 2021, content.:..\n    year = 2021              year: 2021, content: .    Select relevant  year: 2021, content...    Vector similarity    year: 2021, content:.\n                                                       documents                                  search\n                             year: 2021, content:..                     year: 2021, content...                       year: 2021, content:...\n    Metadata filter          year: 2021, content: .\n\n Filter the irrelevant docs\n\n                           Ensure each chunk contains the metadata\n\n                                                                  62\n---\nRetrieval Source Optimization\n\n \u26ab  Adding meta-data\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 63\n---\nKG as a Retrieval Data Source\n\n\u26ab  Extract entities from the user's input query, then construct a subgraph to form\n   context, and finally feed it into the large model for generation.\n\n    \u26ab  Use LLM (or other models) to extract key entities from the question.\n\n    \u26ab  Retrieve subgraphs based on entities, delving to a certain depth, such as 2\n       hops or even more.\n\n    \u26ab  Utilize the obtained context to generate answers through LLM.Two-stage\n       method: Retrieve documents through summaries, then retrieve text blocks\n       from the documents.\n\n64\n---\n  KG as a Retrieval Data Source\n\n  \u26ab  Extract entities from the user's input query, then construct a subgraph to form\n     context, and finally feed it into the large model for generation.\n     P Meta Summary Entities        x k    Layer[i+1]\n                                    Summary Entities    Summarization by LLM  Layer[i]\n                                    Normal Entities     GMM Clustering        Layer[i-1]\n     Documents                      Hilndex: Indexing with Hierarchical Knowledge\n\n                                    6Communities        Global                                            Community Report\n                  Query             Key Entity          Bridge\n                                    Reasoning Paths                              Reasoning Paths                          Generation by LLM\n                                                        xk\n                                    Hatten KG             Local    Key Entity\n                                                                  Descriptions\n                                                        HiRetrieval: Retrieval with Hierarchical Knowledge\n\nhttps://arxiv.org/pdf/2503.10150                                                                                                           65\n---\n   Query Optimization\n\n    \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n       the Query can yield better retrieval results.\n\n\u26ab  Rewrite query:\n                 Input     Input\n                           Black-box LLM\n\n                 Retriever    Rewriter\n\nInput    Example\nSmall PrLM  Input:\n            What profession does Nicholas Ray and\nRewriter            Elia Kazan have in common?\n\n                                  Query             Quuery              Query: Nicholas Ray profession\n                     Documents    Web Search        Web Search            Query: Elia Kazan profession\n                                  Retriever         Retriever           Elia Kazan was an American film and\n                                                                         theatre director, producer,\n       Black-box LLM                                Documents            screenwriter and actor, described\n           Reader                 Documents                               Nicholas Ray American author and\n                                                                          director, original name Raymond\n                                                                          Nicholas Kienzle, born August 7,\n                     Output       Black-box LLM     Black-box LLM         1911, Galesville, Wisconsin, U.S.\n                                  Reader            Reader               Correct (reader                   director\n                                  Output            Reward Output        Hit (retriever\n                     (a) Retrieve-then-read (b)Rewrite-retrieve-read    (c) Trainable rewrite-retrieve-read\n\n https://arxiv.org/pdf/2305.14283    66\n---\n Query Optimization\n\n  \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n     the Query can yield better retrieval results.\n\n  \u26ab Clarify the query:                                        Ambiguous Question (AQ)\n                                                    \"What country has the most medals         o  M\n                                                             in Olympic history?\"             88\n\n                                                      Tree of Clarifications\n                                                      Question                       Pruned   Information\n                                                   Clarification                              Retrieval\n                                                                       *\n                                                   DQ1            DQ2   DQ3\n                                                                       \"What country has\n                                                                       the most total medals\n                                                                       in Olympic history?\"\n                                                   Question       Question\n                                                   Clarification  Clarification\n                                                   *                   *                      Passages\n                                                   DQ11 DQ12 DQ13 DQ21 DQ22 DQ23 DQ24\n                                                   \"What country has the  \"What country has\n                                                   most medals in winter  the most gold medals\n                                                     Olympic history?\"  in Olympic history?\"\n\n                                                                                     Long Form Answer\n\n                                                     Answer                          \"The United States has the\n                                                   Generation                        most total medals. .\n                                                                                        Norway has won most\n\nhttps://aclanthology.org/2023.emnlp-main.63.pdf                                      medals in winter Olympic.\"    67\n---\n Embedding Optimization\n\n  \u26ab Better embedding always indicate a better retrieval results:\n\n             \u26ab    Selecting a more suitable embedding method\n                                                                0Retriever &  HotpotQA      Dataset\n                  Fine-tuning the embedding model               Framework      EM F1        2Wiki      NQ   WebQ\n             \u26ab                                                  [BM25                       EM F1 EM F1 EM F1\n                                                                               |25.4, 37.2[16.6 21.1[26.0 32.8 [22.2 31.2\n                                                                2+SuRe      38.8 53.523.8.31.036.6 47.934 4 48.5\n                                                                 +EmbQA (ours) 42.0 55.8|27.4 36.642.2 54.438.2 52.1\n                                                                DPR            20.6 21.7[10.8 13.5[25.0 34.2[23.8 34.4\n                                                                 +SuRe         25.0 31.9 14.2 16.038.8 52.336.0 49.6\n                                                                 +EmbQA (ours) |29.8 36.3 16.8 21.0    38.0 52.0\n                                                                                                   43.0 54.4\n                                                                 Contriever   [22.6 35.4\n                                                                                     [16.6 20.7[25.8 32.8\n                                                                                                             25.2 34.2\n                                                                 +SuRe          33.8 50.6 21.0 29.3 39.0 52.834.4 48.5\n                                                                 +EmbQA (ours) 36.6 52.7 26.4 34.2 42.2 53.6\n                  Try different embedding methods in the RAG    [BM25         [21.2 29.2               36.0 49.6\n                                                                20+SuRe        32.2 46.1 [13.8 21.7 18.8 25.319.0 26.1\n                                                                                            17.8 30.1\n                                                                                                    35.2 45.131.6 45.7\n                                                                  +EmbQA (ours) 34.8 44.3 18.6 30.5 35.8 46.035.8 48.1\n                                                                [DPR               7.8 11.0 3.8 4.5[22.2,26.718.8 27.7\n                                                                +Sure          15.0 21.8 6.4 8.540.0 51.8\n                                                                 +EmbQA (ours) 16.2 23.3 7.6 9.6             32.6 47.7\n                                                                                                   |40.2 49.433.4 46.0\n                                                                Contriever     19.4 28.6[13.6 20.7[21.8 27.4117.8,244\n                                                                 +SuRe         28.0 41.6 17.2 25.4 39.8 51.630.2 45.0\n                                                                 +EmbQA (ours)29.8 42.3 17.4 26.2 40.6 51.8 31.6 43.0\n                                                                [BM25          [28.6 37.1[20.2 24.1 [24.0 29.4 [22.6 31.4\n                                                                20+Sure        43.6 54.7 28.4 34.1 41.6 49.0 36.6 47.3\n                                                                 +EmbQA (ours) 44.6 55.628.8 33.8 42.4 49.2 38.2 48.7\n                                                                [DPR           8.8 9.8 5.6 7.1\n                                                                                                  [29.2 32.6[25.6 31.1\n                                                                 +Sure         21.8 27.3 12.2 16.1\n                                                                                                   45.4 54.6\n                                                                                                             38.4 49.6\n                                                                 +EmbQA (ours) 22.6 29.1 13.8 17.345.8 54.7\n  AD                                                                                                      38.6 50.1\n  Facts                                                         Contriever     27.0 34.0[17.6 20.0 26.6 31.9 21.0 29.1\n\n0                                                                +Sure         38.8 50.323.8 30.4 44.0 52.9 36.4 48.1\n                                                                +EmbQA (ours)39.0 50.2\n            General-Purpose                                                              24.4 30.9 45.2 50.5 37.0 48.6\n     C-Pack  Text Embedding                                                                                              68\nhttps://arxiv.org/pdf/2503.01606\n\n  C-MTEB  C-MTP  C-TEM  Recipe\n---\nEmbedding Optimization\n\n \u26ab Better embedding always indicate a better retrieval results:\n\n   \u26ab  Selecting a more suitable embedding method\n\n   \u26ab  Fine-tuning the embedding model\n\n      An in-context learning based method to generate prompt\n\n                                                                                       generate Query-Doc pair    Fine-tuning with pseudo data\n\n 1           A few query and\n             relevant document\n             examples\n             for each doc     You are an award\n             in documents     winning relevance                                 GPT-x\n                              expert. Suggest          Large Language Model     BARD\n                              relevant queries for                              Flan-T5\nDocuments                     this article $article                                                               69\n                              queries:                 Synthetic queries for documents\n             LLM Query Generation Prompt\n                                                       \"Labeled data\"                  9\n---\nFine-tuning on RAG\n\n\u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n  \u26ab  Retriever fine-tuning\n\n  \u26ab  Generator fine-tuning\n\n70\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n      \u26ab  Retriever fine-tuning\n\n      \u26ab  Generator fine-tuning\n\n                              A small LM\n\n    Using the attention scores\n    annotate which documents\n    the LM \u201cprefers\u201d.\n\n                                                DecSource LM\n                                                    Fusion-in-Decoder\n                                                Enc Enc. Enc\n    Source Task                                     Q+D1 Q+D2Q+DN\n                                                    Retrieve\n                                                      N Docs\n  Positives         Negatives                   Pre-Trained Retriever    71\nGround Truth U    ANCE Sampling\n    -Top-K FiDAtt                               Target LMs Target Tasks\n    https://aclanthology.org/2023.acl-long.136.pdf\n                                     Generic                GCMETRY\n                                     Plug-In                  WAKT\n   Augmentation-Adapted Retriever                           RISTORY\n                                                            LITERATRE\n                                                            SCIENCE\n                                                              MATH\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n    \u26ab Retriever fine-tuning\n\n    \u26ab Generator fine-tuning\n\n    Product Search                 Premium hiking bag.            Unstructured Data                                             Structured Data (Negative)\n    Black large capacity hiking    Size: Max Color: Black                                     Structured Data (Positive)\n    bag, made of canvas.           Material: canvas               These erasable mood pencils     [MASK1] changing [MASK2]       Tire Specifications: Material:\n\nCode Search\nGiven two numbers, the\n\nlargest number is returned\nQuery\n]\nPretrained Language Model\n                               are made of quality wood       [MASK3] with [MASK4]            Rubber Tire Size: 16X6.50-8\n                               and color temperature          Function: removing wrong        Tire Type: Tubeless Rim\ndef compare(a, b):             coating, have non-fading        writing Material: [MASK2]      Width: 5.375\" Tread Depth:\nreturn max(a, b)          colors.                              Changing Size: 18 x 0.5 cm     7.1mm\" Pattern: P332\nStructured Data (Positive)\n Structured Data (Negative)                                               T5\n                               Structured Data Alignment (Loss: C sDA)      Masked Entity Prediction (Loss: LP)\n\n    Training    Push Away                           Prediction                                          erasers\n                                                    [MASK1] Color\n                Ground Truth                        [MASK2] mood\n                                Align               [MASK3] pencils,\n    .                                               [MASK4]\n\n    Embedding Space    Optimized Embedding Space    Add an entity prediction loss in the fine-tuning           72\n---\nK\nING'S\nCollege\nLONDON\n\nThank you\n\nDr Lin Gui\nLin.1.Gui@kcl.ac.uk\nwww.kcl.ac.uk/people/lin-gui\n\n",
        "quiz": [
            {
                "question_text": "What is the primary function of the RAG model when answering questions or generating text?",
                "answers": [
                    {
                        "text": "To retrieve relevant information from documents and generate answers based on that information",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that RAG first retrieves relevant information and then generates answers based on this information."
                    },
                    {
                        "text": "To retrain the entire large model for each specific task",
                        "is_correct": false,
                        "explanation": "The content context explicitly states that RAG avoids retraining the entire model for each specific task by attaching an external knowledge base."
                    },
                    {
                        "text": "To generate text without any external information",
                        "is_correct": false,
                        "explanation": "The concept description states that RAG retrieves relevant information before generating text, contradicting this option."
                    },
                    {
                        "text": "To store and manage large volumes of unstructured data",
                        "is_correct": false,
                        "explanation": "The content context does not mention data storage or management as a function of RAG, making this option irrelevant."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the primary process involved in Retrieval-Augmented Generation (RAG)?",
                "answers": [
                    {
                        "text": "Retrieving relevant information from documents and generating answers based on that information",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that RAG involves retrieving relevant information from documents and then generating answers based on this information."
                    },
                    {
                        "text": "Training a new model for each specific task",
                        "is_correct": false,
                        "explanation": "The concept description states that RAG avoids the need to retrain the entire large model for each specific task by attaching an external knowledge base."
                    },
                    {
                        "text": "Using only parametric knowledge stored within the model",
                        "is_correct": false,
                        "explanation": "The concept description highlights that RAG uses an external knowledge base, which is not parametric knowledge stored within the model."
                    },
                    {
                        "text": "Generating answers without any external information",
                        "is_correct": false,
                        "explanation": "The concept description explicitly mentions that RAG retrieves relevant information from documents before generating answers, indicating the use of external information."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "medium",
                "explanation": ""
            },
            {
                "question_text": "What is the primary process involved in Retrieval-Augmented Generation (RAG) when answering questions or generating text?",
                "answers": [
                    {
                        "text": "Retrieving relevant information from documents and generating answers based on this information",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that RAG first retrieves relevant information from documents and then generates answers based on this information."
                    },
                    {
                        "text": "Generating answers directly from the model's pre-trained knowledge without any external input",
                        "is_correct": false,
                        "explanation": "This option is incorrect because RAG specifically involves retrieving external information before generating answers, which is not described in the concept."
                    },
                    {
                        "text": "Using a pre-defined set of rules to answer questions without any external knowledge base",
                        "is_correct": false,
                        "explanation": "This option is incorrect because RAG relies on an external knowledge base and does not use a pre-defined set of rules."
                    },
                    {
                        "text": "Retraining the entire large model for each specific task to generate accurate answers",
                        "is_correct": false,
                        "explanation": "This option is incorrect because the concept description explicitly states that RAG does not require retraining the entire model for each specific task."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "hard",
                "explanation": ""
            },
            {
                "question_text": "What is the primary process involved in Retrieval-Augmented Generation (RAG)?",
                "answers": [
                    {
                        "text": "Retrieving relevant information from documents and generating answers based on this information",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that RAG first retrieves relevant information from documents and then generates answers based on this information."
                    },
                    {
                        "text": "Fine-tuning the entire large model for each specific task",
                        "is_correct": false,
                        "explanation": "The content context explicitly states that RAG avoids retraining the entire large model for each specific task by attaching an external knowledge base."
                    },
                    {
                        "text": "Generating answers without any external information",
                        "is_correct": false,
                        "explanation": "The concept description explicitly states that RAG retrieves relevant information from documents before generating answers, making this option incorrect."
                    },
                    {
                        "text": "Using symbolic knowledge to generate answers",
                        "is_correct": false,
                        "explanation": "The content context mentions symbolic knowledge as a separate optimization method for LLMs, not as the primary process in RAG."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the primary process involved in Retrieval-Augmented Generation (RAG) when answering questions or generating text?",
                "answers": [
                    {
                        "text": "Retrieving relevant information from documents and then generating answers based on this information",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that RAG first retrieves relevant information and then generates answers based on this information."
                    },
                    {
                        "text": "Generating answers directly from the model's pre-trained knowledge without retrieving external information",
                        "is_correct": false,
                        "explanation": "This contradicts the RAG process described, which involves retrieving external information before generating answers."
                    },
                    {
                        "text": "Using only symbolic knowledge to generate answers without any retrieval process",
                        "is_correct": false,
                        "explanation": "The concept description emphasizes the use of external knowledge through retrieval, not just symbolic knowledge."
                    },
                    {
                        "text": "Fine-tuning the entire large model for each specific task to generate answers",
                        "is_correct": false,
                        "explanation": "The concept description explicitly states that RAG avoids retraining the entire model for each specific task."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "medium",
                "explanation": ""
            }
        ]
    }
}