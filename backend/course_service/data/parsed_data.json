{
    "data/raw/LLMs.pdf": {
        "metadata": {
            "file_name": "LLMs.pdf",
            "file_type": "pdf",
            "content_length": 16593,
            "language": "en",
            "extraction_timestamp": "2025-11-27T18:19:25.217056+00:00",
            "timezone": "utc"
        },
        "content": "Large       Introduction to Large Language\nLanguage    Models\nModels\n---\nLarge language models\n\nComputational agents that can interact\nconversationally with people using natural language\nLLMS have revolutionized the field of NLP and AI\n---\nLanguage models\n\n\u2022       Remember the simple n-gram language model\n   \u2022     Assigns probabilities to sequences of words\n   \u2022     Generate text by sampling possible next words\n   \u2022     Is trained on counts computed from lots of text\n\u2022       Large language models are similar and different:\n   \u2022     Assigns probabilities to sequences of words\n   \u2022     Generate text by sampling possible next words\n   \u2022     Are trained by learning to guess the next word\n---\n Fundamental intuition of large language models\n\nText contains enormous amounts of knowledge\nPretraining on lots of text with all that knowledge is\n what gives language models their ability to do so\n much\n---\nWhat does a model learn from pretraining?\n\n\u2022     With roses, dahlias, and peonies, I was\n      surrounded by flowers\n\u2022     The room wasn't just big it was enormous\n\u2022     The square root of 4 is 2\n\u2022     The author of \"A Room of One's Own\" is Virginia\n      Woolf\n\u2022     The doctor told me that he\n---\n    What is a large language model?\n    A neural network with:\n    Input: a context or prefix,\n    Output: a distribution over possible next words\n\n                                                   p(w|context)\n                               output              all     .44\n\n                                                the        .33\n                                               your        .15\n    Transformer (or other decoder)             that        .08\n\n input                                ?\ncontext  So  long  and    thanks  for\n---\nLLMs can generate!\n\nA model that gives a probability distribution over next words can generate\nby repeatedly sampling from the distribution\n\n                  p(w|context)\n                  output                  all    .44\n\n                            the                  .33\n                                         your    .15\nTransformer (or other decoder)    that           .08\n                                  \u2026               \u2026\n\nSo  long  and  thanks  for  all                  p(w|context)\n                                   output        the   .77\n\n                                                 your  .22\n                                              our      .07\n          Transformer (or other decoder)       of      .02\n                                                 \u2026     \u2026\n\n          So  long  and  thanks    for  all  the\n---\nThree architectures for large language models\nw w                                          w\n\nw w w w w\n\nw  w     w w w   w  w  w w w   w  w w\n\n   Decoder                        Encoder-Decoder\nDecoders         Encoders      Encoder-decoders\n                    Encoder\n\nGPT, Claude,     BERT family,  Flan-T5, Whisper\nLlama            HuBERT\nMixtral\n---\nDecoders                                    w w w w        w\n\n                                            w  w  w w      w  w\nWhat most people think of when we say LLM Decoder\n\u2022     GPT, Claude, Llama, DeepSeek, Mistral\n\u2022     A generative model\n\u2022     It takes as input a series of tokens, and iteratively\n      generates an output token one at a time.\n\u2022     Left to right (causal, autoregressive)\n---\nEncoders              w     w w w w\n\n\u2022  Masked Language Models (MLMs)\n                      w     w  w w w  w  w  w  w     w\n\u2022  BERT family              Decoder      Encoder\n\n\u2022  Trained by predicting words from surrounding\n   words on both sides\n\u2022  Are usually finetuned (trained on supervised data)\n   for classification tasks.\n---\nEncoder-Decoders    w w w\nw w  w  w w\n\n\u2022          w  w  w  w w  w  w  w  w w  w  w  w\n    Trained to map from one sequence to another\n\u2022   Very      Decoder       Encoder       Encoder-Decoder\n           popular for:\n   \u2022     machine translation (map from one language to\n         another)\n   \u2022     speech recognition (map from acoustics to words)\n---\nLarge       Introduction to Large Language\nLanguage    Models\nModels\n---\nLarge       Pretraining Large Language\nLanguage    Models\nModels\n---\n    Three stages of training in LLMs\n\n    Instruction Data    Preference Data\n\nPretraining    Label sentiment of this sentence:    Human: How can I embezzle money?\n    The movie wasn\u2019t that great\n    Data    Summarize: Hawaii Electric urges       Assistant: Embezzling is a\n        caution as crews replace a utility pole    felony, I can't help you\u2026\n             overnight on the highway from\u2026         Assistant: Start by creating\n\n    Translate English to Chinese:                   fake expense reports...\n    When does the flight arrive?\n\n    1. Pretraining    2. Instruction    3. Preference\n                         Tuning            Alignment\n\nPretrained    Instruction    Aligned LLM\n   LLM         Tuned LLM\n---\nPretraining\n\nThe big idea that underlies all the amazing\nperformance of language models\n\nFirst pretrain a transformer model on enormous\namounts of text\nThen apply it to new tasks.\n---\nSelf-supervised training algorithm\n\nWe train them to predict the next word!\n1. Take a corpus of text\n2. At each time step t\n i.      ask the model to predict the next word\n ii.     train the model using gradient descent to minimize the\n         error in this prediction\n\n \"Self-supervised\" because it just uses the next word as the\n label!\n---\nIntuition of language model training: loss\n\n\u2022         Same loss function: cross-entropy loss\n     \u2022     We want the model to assign a high probability to true\n           word w\n     \u2022     = want loss to be high if the model assigns too low a\n           probability to w\n\u2022         CE Loss: The negative log probability that the model\n          assigns to the true next word w\n     \u2022     If the model assigns too low a probability to w\n     \u2022     We move the model weights in the direction that assigns a\n           higher probability to w\n---\n                    L   = \u2212        y [w] log \u02c6\nl to minimize the   CE             t             yt [w]\n                    error in predicting the true next word in the training sequ\n       Cross                 w\u2208V\n                 -entropy loss for language modeling\ncross-entropy as the loss function.\necall that the cross-entropy loss measures the difference between a pred\n of language modeling, the correct distribution        yt comes from kn\nbility distribution and the correct distribution.\n       CE loss: difference between the correct probability distribution and the predicted\n . This is represented as a one-hot vector corresponding to the v\n       distribution       X\nentry for the actual next word is 1, and all the other entries are\n                       L  = \u2212      y [w] log \u02c6\nentropy loss           CE          t    yt [w]                                           (\n                       for language modeling is determined by the proba\n                               w\u2208V\n igns to the correct next word (all other words get multiplied by\n ase   The correct distribution y knows the next word, so is 1 for the actual next\n   of language modeling, the correct distribution y comes from knowin\nthe CE loss in (10.5)        t                    t\n       word and 0 for the   can be simplified as the negative log prob\nord. This is                others.\n                 represented as a one-hot vector corresponding to the vocabu\n       So in this sum, all terms get multiplied by zero except one: the logp the\n igns to the next word in the training sequence.\nthe entry for the actual next word is 1, and all the other entries are 0. T\n       model assigns to the correct next word, so:\n ss-entropy loss for language modeling is determined by the probability\n                       L    ( \u02c6                   \u02c6\nl assigns to the       CE    yt , yt ) = \u2212 log yt [wt +1 ]\n                       correct next word (all other words get multiplied by zero\n t the CE loss in (10.5) can be simplified as the negative log probabilit\n---\nTeacher forcing\n\n\u2022     At each token position t, model sees correct tokens w1:t,\n     \u2022  Computes loss (\u2013log probability) for the next token wt+1\n\u2022     At next token position t+1 we ignore what model predicted\n      for w\u209c\u208a\u2081\n     \u2022  Instead we take the correct word w\u209c\u208a\u2081, add it to context, move on\n---\nTraining a transformer language model\n\nTrue next token  long        and        thanks        for        all           \u2026\n\nCE Loss          \u2212log ylong  \u2212log yand  \u2212log ythanks  \u2212log yfor  \u2212log yall     \u2026\nper token\n\n                 \u0177  back     \u0177  back    \u0177  back       \u0177  back    \u0177  back\n                    prop        prop       prop          prop       prop\n\nLLM                                                                            \u2026\n\nInput tokens  So  long  and  thanks  for    \u2026\n---\n LLMs are mainly trained on the web\n\nCommon crawl, snapshots of the entire web produced by\n the non- profit Common Crawl with billions of pages\nColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156\n billion tokens of English, filtered\n What's in it? Mostly patent text documents, Wikipedia, and\n news sites\n---\nThe Pile: a pretraining corpus\nComposition of the Pile by Category\nacademics    web                   books\n             Academic Internet Prose DialogueMisc\n\n                                                                 Bibliotik\n\nPile-CC                                                          PG-19       BC2\n\nPubMed Central    ArXiv\n                                                                             Subtitles\n                                                                                      dialog\n\n                  PMA    StackExchange                           Github      IRC EP\nFreeLaw           USPTO  Phil  NIH OpenWebText2    Wikipedia     DM Math     HN  YT\n---\nFiltering for quality and safety\n\nQuality is subjective\n\u2022     Many LLMs attempt to match Wikipedia, books, particular\n      websites\n\u2022     Need to remove boilerplate, adult content\n\u2022     Deduplication at many levels (URLs, documents, even lines)\nSafety also subjective\n\u2022     Toxicity detection is important, although that has mixed results\n\u2022     Can mistakenly flag data written in dialects like African American\n      English\n---\n Reexamining \"Fair Use\" in the Age of\n AI There are problems with scraping from the web\n Generative Al claims to produce new language and images, but when those ideas are based on copyrighted\n material, who gets the credit? A new paper from Stanford University looks for answers.\n Jun 5, 2023 | Andrew Myersfin 0\n\n                                                                                                       Authors Sue OpenAI Claiming Mass Copyright\n                                                                                                       Infringement of Hundreds of Thousands of Novels\n\n The Times Sues OpenAI and Microsoft\nOver A.I. Use of Copyrighted Work\nMillions of articles from The New York Times were used to train\n chatbots that now compete with it, the lawsuit said.\n---\nThere are problems with scraping from the web\n\nCopyright: much of the text in these datasets is copyrighted\n\u2022     Not clear if fair use doctrine in US allows for this use\n\u2022     This remains an open legal question across the world\nData consent\n\u2022     Website owners can indicate they don't want their site crawled\nPrivacy:\n\u2022     Websites can contain private IP addresses and phone numbers\nSkew:\n\u2022     Training data is disproportionately generated by authors from the\n      US which probably skews resulting topics and opinions\n---\nLarge       Pretraining Large Language\nLanguage    Models\nModels\n---\nLarge       Evaluating Large Language\nLanguage    Models\nModels\n---\n   P(w ) = P(w )P(w |w )P(w |w  ) . . . P(w |w\n We\u2019ve been talking about predicting one word at a time, computing the proba\n       1:n    1  2  1  3  1:2               n  1:n\u2212\nof the next token w from the prior context: P(w |w ). But of course as we\n    Better        n\n              LMs are better at predicting text\n hapter 3 the     iY      i  <i\n                chain rule allows us to move between computing the probability\n next token and =       P(wi |w<i )\n                computing the probability of a whole text:\n    Reminder of the chain rule:\n       P(w         i=1\n                1:n ) = P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P(wn |w1:n\u22121 )\npute the probability of text just by multiplying the cond\n                        n\n ch                = Y P(wi |w<i )\n    token in the text. The resulting (log) likelihood of a\n                        i=1\n mparing how good two language models are on that text\ne can compute the probability of text just by multiplying the conditional pr\n        So given a text w1:n we could just compare the log likelihood from two LMs:\nities for each token in the text. The resulting (log) likelihood of a text is a us\n                                   n\n tric for comparing how good two language   Y\n        log likelihood(w          ) = models are on that text:\n                               1:n  log            P(wi |w<i )\n                                            n\n                  log likelihood(w  ) = log Y i=1\n                               1:n           P(wi |w<i )\n---\nBut raw log-likelihood has problems\n\nProbability depends on size of test set\n\u2022  Probability gets smaller the longer the text\n\u2022  We would prefer a metric that is per-word,\n   normalized by length\n---\nPerplexity is normalized for length\n\nPerplexity is the inverse probability of the test set,\nnormalized by the number of words\n (The inverse comes from the original definition of\n perplexity from cross-entropy rate in information theory)\n\n Probability range is [0,1], perplexity range is [1,\u221e]\n---\n          set), normalized by the test set length in tokens. For a test set of n toke\n    Perplexity\nAs we     perplexity is\n          first saw in Chapter 3, one way to evaluate language models is\n  well they predict unseen text. Intuitively, good models are those that\n                              Perplexity          (w ) =      P (w )\u2212 1\n    So just as for n-gram grammars, we use perplexity                 n\n                                                  1:n         to measure how\n robabilities to unseen data (are less     q                  q    1:n\n                                           surprised when encountering the\n    well the LM predicts unseen text                     = s       1\n    The perplexity of a model \u03b8 on an unseen test             n\ntiate this intuition by using perplexity to measure set is the inverse\n                                                           the quality of a\n    probability that \u03b8 assigns to the test set,                  Pq (w1:n )\n el. Recall from page   ?? that the perplexity of    normalized by the test\n    set length.                                      a model q on an unseen\n erse probability that  q assigns to the test set, normalized by the test\n          To visualize how perplexity can be computed as a function of the proba\na test  For a test set of n tokens w       the perplexity is :\n        set of  n tokens w  , the perplexity is\n          LM computes for each new word, we can use the chain rule to expand th\n                            1:n    1:n\n          tion of probability of the test set: \u2212 1\n                Perplexity (w1:n ) = P (w1:n ) n\n                           q          q                   v\n                                     s                    u n\n                                           1              uY          1\n                                   =                      t\n                                   Perplexity   (w  ) =\n                                        n  q       1:n    n      P (w(10.7)\n                                           P (w1:n )             q    i |w<i )\n                                           q                  i=1\n---\nPerplexity\n\n\u2022     The higher the probability of the word sequence, the lower\n      the perplexity.\n\u2022     Thus the lower the perplexity of a model on the data, the\n      better the model.\n\u2022     Minimizing perplexity is the same as maximizing\n      probability\n\nAlso: perplexity is sensitive to length/tokenization so best used\nwhen comparing LMs that use the same tokenizer.\n---\nMany other factors that we evaluate, like:\n\nSize\nBig models take lots of GPUs and time to train, memory to store\nEnergy usage\nCan measure kWh or kilograms of CO2 emitted\nFairness\nBenchmarks measure gendered and racial stereotypes, or decreased\nperformance for language from or about some groups.\n\n",
        "quiz": [
            {
                "question_text": "What are large language models primarily designed to do?",
                "answers": [
                    {
                        "text": "Interact conversationally with people using natural language",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that large language models are computational agents designed to interact conversationally with people using natural language."
                    },
                    {
                        "text": "Perform complex mathematical calculations",
                        "is_correct": false,
                        "explanation": "The concept description does not mention mathematical calculations as a primary function of large language models."
                    },
                    {
                        "text": "Generate images based on textual descriptions",
                        "is_correct": false,
                        "explanation": "The concept description focuses on natural language interaction and text generation, not image generation."
                    },
                    {
                        "text": "Control physical robots in real-time",
                        "is_correct": false,
                        "explanation": "The concept description does not mention anything about controlling physical robots."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the fundamental intuition behind large language models?",
                "answers": [
                    {
                        "text": "Text contains enormous amounts of knowledge, and pretraining on lots of text with all that knowledge is what gives language models their ability to do so much",
                        "is_correct": true,
                        "explanation": "This is the fundamental intuition behind large language models as described in the content context."
                    },
                    {
                        "text": "Large language models are trained to predict the next word in a sequence by using counts computed from lots of text",
                        "is_correct": false,
                        "explanation": "While this is true for simple n-gram models, large language models are trained by learning to guess the next word through neural networks."
                    },
                    {
                        "text": "Large language models generate text by randomly selecting words without any probabilistic distribution",
                        "is_correct": false,
                        "explanation": "Large language models generate text by sampling from a probability distribution over possible next words, not by random selection."
                    },
                    {
                        "text": "Large language models are primarily used for classification tasks rather than text generation",
                        "is_correct": false,
                        "explanation": "While some language models like encoders are used for classification, large language models are primarily known for their text generation capabilities."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What type of model assigns probabilities to sequences of words and generates text by sampling possible next words?",
                "answers": [
                    {
                        "text": "A simple n-gram language model",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that a simple n-gram language model assigns probabilities to sequences of words and generates text by sampling possible next words."
                    },
                    {
                        "text": "A neural network with input as context and output as a distribution over possible next words",
                        "is_correct": false,
                        "explanation": "This describes a large language model, not the specific model that assigns probabilities to sequences of words and generates text by sampling possible next words."
                    },
                    {
                        "text": "A transformer model",
                        "is_correct": false,
                        "explanation": "Transformer models are a type of architecture used in large language models, but they are not specifically described as assigning probabilities to sequences of words and generating text by sampling possible next words."
                    },
                    {
                        "text": "A model trained by learning to guess the next word",
                        "is_correct": false,
                        "explanation": "This describes a large language model, not the specific model that assigns probabilities to sequences of words and generates text by sampling possible next words."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the output of a large language model given a context or prefix?",
                "answers": [
                    {
                        "text": "A distribution over possible next words",
                        "is_correct": true,
                        "explanation": "The concept description states that a large language model outputs a distribution over possible next words given a context or prefix."
                    },
                    {
                        "text": "A single predicted word",
                        "is_correct": false,
                        "explanation": "The model outputs a distribution, not a single word, which is then sampled to generate text."
                    },
                    {
                        "text": "A list of predefined responses",
                        "is_correct": false,
                        "explanation": "The model does not use predefined responses; it generates outputs based on learned probabilities."
                    },
                    {
                        "text": "A binary yes or no answer",
                        "is_correct": false,
                        "explanation": "The model outputs a probability distribution over words, not binary answers."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the three main architectures for large language models?",
                "answers": [
                    {
                        "text": "Decoder, Encoder, Encoder-Decoder",
                        "is_correct": true,
                        "explanation": "The three main architectures for large language models are explicitly listed as Decoder, Encoder, and Encoder-Decoder in the content."
                    },
                    {
                        "text": "Transformer, Recurrent Neural Network, Convolutional Neural Network",
                        "is_correct": false,
                        "explanation": "These are different types of neural network architectures, not the three main architectures for large language models."
                    },
                    {
                        "text": "Generative, Discriminative, Hybrid",
                        "is_correct": false,
                        "explanation": "These terms describe different types of models but are not the specific architectures mentioned for large language models."
                    },
                    {
                        "text": "BERT, GPT, T5",
                        "is_correct": false,
                        "explanation": "These are specific models or families of models, not the general architectures for large language models."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the primary function of a decoder in the context of large language models?",
                "answers": [
                    {
                        "text": "To generate a distribution over possible next words based on the input context",
                        "is_correct": true,
                        "explanation": "The concept description states that a decoder in LLMs takes a context or prefix as input and outputs a distribution over possible next words."
                    },
                    {
                        "text": "To translate text from one language to another",
                        "is_correct": false,
                        "explanation": "Translation is typically handled by encoder-decoder architectures, not just decoders."
                    },
                    {
                        "text": "To predict words from surrounding words on both sides",
                        "is_correct": false,
                        "explanation": "This is a function of encoders, not decoders."
                    },
                    {
                        "text": "To classify text into predefined categories",
                        "is_correct": false,
                        "explanation": "Classification tasks are usually performed by fine-tuned encoders, not decoders."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the main difference between decoders and encoders in language models?",
                "answers": [
                    {
                        "text": "Decoders generate text by predicting the next word in a sequence, while encoders predict words from surrounding words on both sides.",
                        "is_correct": true,
                        "explanation": "The concept_description and content_context describe decoders as generative models that predict the next word in a sequence, and encoders as models that predict words from surrounding words on both sides."
                    },
                    {
                        "text": "Decoders and encoders both predict the next word in a sequence.",
                        "is_correct": false,
                        "explanation": "The content_context specifies that encoders predict words from surrounding words on both sides, not just the next word in a sequence."
                    },
                    {
                        "text": "Encoders generate text by predicting the next word in a sequence.",
                        "is_correct": false,
                        "explanation": "The content_context states that decoders, not encoders, generate text by predicting the next word in a sequence."
                    },
                    {
                        "text": "Decoders and encoders are trained using the same method.",
                        "is_correct": false,
                        "explanation": "The content_context explains that decoders are trained by learning to guess the next word, while encoders are trained by predicting words from surrounding words on both sides."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the three stages of training in large language models?",
                "answers": [
                    {
                        "text": "Pretraining, Instruction Tuning, Preference Alignment",
                        "is_correct": true,
                        "explanation": "The three stages of training in large language models are explicitly stated as Pretraining, Instruction Tuning, and Preference Alignment in the provided content."
                    },
                    {
                        "text": "Training, Testing, Deployment",
                        "is_correct": false,
                        "explanation": "These are general stages in machine learning but are not specifically mentioned as the stages of training in large language models in the provided content."
                    },
                    {
                        "text": "Data Collection, Model Training, Evaluation",
                        "is_correct": false,
                        "explanation": "These are typical stages in developing machine learning models but are not the specific stages mentioned for large language models in the provided content."
                    },
                    {
                        "text": "Pretraining, Fine-tuning, Deployment",
                        "is_correct": false,
                        "explanation": "While these are common stages in model development, the specific stages for large language models as per the provided content are Pretraining, Instruction Tuning, and Preference Alignment."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the big idea that underlies the performance of language models?",
                "answers": [
                    {
                        "text": "The big idea is that text contains enormous amounts of knowledge and pretraining on lots of text with all that knowledge gives language models their ability to perform well.",
                        "is_correct": true,
                        "explanation": "This is directly stated in the provided content context under the section 'Fundamental intuition of large language models'."
                    },
                    {
                        "text": "The big idea is that language models are trained by learning to guess the next word in a sequence.",
                        "is_correct": false,
                        "explanation": "While this is a key aspect of language models, it is not the fundamental big idea that underlies their performance as described in the content."
                    },
                    {
                        "text": "The big idea is that language models use neural networks to generate text by sampling possible next words.",
                        "is_correct": false,
                        "explanation": "This describes a mechanism of language models but not the underlying big idea that explains their performance."
                    },
                    {
                        "text": "The big idea is that language models are trained on counts computed from lots of text.",
                        "is_correct": false,
                        "explanation": "This describes a method of training simple n-gram models, not the fundamental idea behind the performance of large language models."
                    }
                ],
                "topic": "Llms",
                "subtopic": "Main Content",
                "concepts": [
                    "Llms"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ],
        "summary": "No summary available."
    },
    "data/raw/NNs.pdf": {
        "metadata": {
            "file_name": "NNs.pdf",
            "file_type": "pdf",
            "content_length": 16065,
            "language": "en",
            "extraction_timestamp": "2025-11-27T18:21:58.194757+00:00",
            "timezone": "utc"
        },
        "content": "Simple Neural    Feedforward networks for\nNetworks and     simple classification\nNeural\nLanguage\nModels\n---\nUse cases for feedforward networks\n\nLet's consider 2 (simplified) sample tasks:\n 1.  Text classification\n 2.  Language modeling\n\nState of the art systems use more powerful neural\nclassifiers like BERT/MLM classifiers, but simple models\nwill introduce some important ideas\n\n                                                 43\n---\nClassification: Sentiment Analysis\n\nWe could do exactly what we did with logistic\nregression\nInput layer are binary features as before\nOutput layer is 0 or 1                   U   \u03c3\n\n                                         W\n\n                                         x\u2081    x\u2099\n---\n+ or \u2212 to a review document doc. We\u2019ll represent each input ob\nfeatures x     ...x  of the input shown in the following table; Fig. 5.2\nSentiment Features\n 1             6\nin a sample mini test document.\n\n Var                 Definition                             Value i\n x1                  count(positive lexicon) \u2208 doc)         3\n x                   count(negative lexicon) \u2208 doc)         2\n x2                  \u21e2 1  if \u201cno\u201d \u2208 doc                     1\n          3          0    otherwise\n x                   count(1st and 2nd pronouns \u2208 doc)      3\n x4                  \u21e2 1  if \u201c!\u201d \u2208 doc                      0\n          5          0    otherwise\n x6                  log(word count of doc)                 ln(66)\n\n                                                          45\n---\nFeedforward nets for simple classification\n\n Logistic      W \u03c3    2-layer               U    \u03c3\n Regression                      feedforward\n\n               x\u2081     x\u2099         network    W\n               f\u2081    f\u2082    f\u2099               x\u2081     x\u2099\n                                            f\u2081    f\u2082    f\u2099\nJust adding a hidden layer to logistic regression       46\n                     46\n\n\u2022     allows the network to use non-linear interactions between features\n\u2022     which may (or may not) improve performance.\n---\n Reminder: Multiclass Outputs\n\n What if you have more than two output classes?\n \u25e6  Add more output units (one for each class)\n \u25e6  And use a \u201csoftmax layer\u201d\n\nsoftmax(zi)  eZi  1\u2264i\u2264D      U\n             =2$\n             k\n\n                             W\n\n                             x\u2081               x\u2099\n                                                47\n---\nThe output layer\n\n\u0177 could have two nodes (one each for positive and\nnegative), or 3 nodes (positive, negative, neutral).\n\u2022     \u0177\u2081 estimated probability of positive sentiment\n\u2022     \u0177\u2082 probability of negative\n\u2022     \u0177\u2083 probability of neutral\n---\nent, \u02c6                             \u02c6\n     y2 the probability of negative and y3 the probability of neutral\nequations would be just what we saw above for a 2-layer network (a\n          Equations for NN classification with hand features\n ntinue to use the s to stand for any non-linearity, whether sigmo\n ).\n\n          x  = [x1 , x2 , ...xd ]  (each xi is a hand-designed feature)\n          h  = s (Wx + b)\n          z  = Uh\n   \u02c6\n   y         = softmax(z)\n\n10 shows a sketch of this architecture. As we mentioned earlier, ad\nlayer to our logistic regression classifier allows the network to repr\n---\ndessert  wordcount               x1            h1            ^\n         =3                                    h2            y1  p(+)\n\nwas            positive lexicon  x2                          ^\n               words = 1                       h3            y2  p(-)\n\n                                                             ^\ngreat  count of \u201cno\u201d             x3            \u2026             y3  p(neut)\n            = 0                             hdh\n\nInput words                      x     W       h             U  y\n                            [d\u2a091]      [dh\u2a09d]  [dh\u2a091]  [3\u2a09dh]   [3\u2a091]\n\n                            Input layer        Hidden layer     Output layer\n                            d=3 features                        softmax\n---\nVectoring for parallelizing inference\n\nWe would like to efficiently classify the whole test\nset of m observations.\nSo we vectorize: pack all the input features into X\n\u2022     Each row x(i) of X is a row vector with all the\n      features for example x(i)\n\u2022     Feature dimensionality is d, X is [m x d]\n---\nBecause we are now modeling each input as a row vector rather than a colum\ntor, we also need to slightly modify Eq. 6.19. X is of shape [m \u21e5 d ] and W is\n e [d  Slight changes to equations\n    h \u21e5 d ], so we\u2019ll reorder how we multiply X and W and transpose W so th\n ectly multiply to yield a matrix H of shape [m \u21e5 dh ]. 1\nThe    Each input is now a row vector\n     bias vector b from Eq. 6.19 of shape [1 \u21e5 dh ] will now have to be replicat\na matrix of shape [m \u21e5 dh ]. We\u2019ll need to similarly reorder the next step a\nspose \u2022    X is of shape [m x d]  \u02c6\n       U. Finally, our output matrix Y will be of shape [m \u21e5 3] (or more ge\nly [m \u2022    W is of shape [d x d]\n       \u21e5 d ], where d is the number of output classes), with each row i of o\n           o \u02c6    o   h            \u02c6 (i)\n ut matrix Y consisting of the output vector y  .\u2018 Here are the final equations f\n      \u2022    We'll need to do some reordering and transposing\n puting the output class distribution for an entire test set:\n      \u2022    Bias vector b that used to be [1 x d\u2095] is now [m x d\u2095]\n                            H = s (XW| + b)\n                            Z = HU|\n                            \u02c6\n                            Y = softmax(Z)                       (6.2\n---\nSimple Neural    Feedforward networks for\nNetworks and     simple classification\nNeural\nLanguage\nModels\n---\nSimple Neural    Embeddings as input to\nNetworks and     feedforward classifiers\nNeural\nLanguage\nModels\n---\n Even better: representation learning\n\nThe real power of deep learning comes   U \u03c3\nfrom the ability to learn features from\nthe data                                W\nInstead of using hand-built human-\nengineered features for classification  x\u2081    x\u2099\nUse learned representations like        e\u2081    e\u2082    e\u2099\nembeddings!\n\n                                                    55\n---\nEmbedding matrix E\n\n\u2022     An embedding is a vector of dimension [1 x d]\n      that represents the input token.\n\u2022     An embedding matrix E is a dictionary, one row\n      per token of vocab V\n\u2022     E has shape [|V| \u00d7 d]\n\u2022     Embedding matrices are central to NLP; they\n      represent input text in LLMs and all NLP tools\n---\nText classification from embeddings\n\n\u2022     Given tokenized input: dessert was great\n\u2022     Select the embedding vectors from E:\n     1.  Convert BPE tokens into vocabulary indices\n      \u2022  w = [3, 9824, 226]\n     2.  Use indexing to select the corresponding rows from E\n      \u2022  row 3, row 4000, row 10532\n---\n                                  Another way to think about selecting token embeddings from the\nAnother way to think of indexing from E\n                          matrix is to represent input tokens as one-hot vectors of shape [1 \u21e5 |V\n       one-hot vector     one dimension for each word in the vocabulary. Recall that in a one-h\n                          the elements are 0 except one, the element whose dimension is the w\n\n\u2022     Treat each in the vocabulary, which has value 1. So if the word \u201cdessert\u201d has in\n                         input word as one-hot vector\n                          vocabulary, x3 = 1, and xi = 0 8i = 3, as shown here:\n   \u2022   If dessert is index 3:     [0 0 1 0 0 0 0 ... 0 0 0 0]\n                                  1 2 3 4 5 6 7 ...      ... |V|\n\n\u2022     Multiply it by Multiplying by a one-hot vector that has only one non-zero element x\n                                  E to select out the embedding for\n                          selects out the relevant row vector for word i, resulting in the embeddin\n      dessert as depicted in Fig. 6.11.                  d\n\n       3                  |V|     \u2715 3 3  |V|              = 3 1  d             d  d\n     1 0 0 1 0 0 0 0 \u2026 0 0 0 0      1 0 0 1 0 0 0 0 \u2026 0 E  \u2715     E             =  1\n                                         0 0 0\n\n                                      |V|                     |V|\n                          Figure 6.11  Selecting the embedding vector for word V by multiplying th\n---\nCollecting embeddings from E for N words\n\n                              d\n                    |V|                 d\n\n   0 0 1 0 0 0 0 \u2026 0 0 0 0    \u2715  =\n   0 0 0 0 0 0 0 \u2026 0 0 1 0       E\n   1 0 0 0 0 0 0 \u2026 0 0 0 0\n\nN  0 0 0 0 1 0 \u2026                 |  |    N\n   0 \u2026 0 0 0 0                   V\n\nGiven window of N tokens, represented by N [1 \u00d7 d]\nembeddings\nNeed to return a single class (e.g., pos or neg)\n---\nText comes in different lengths\nGiven window of N tokens, represented by N [1 \u00d7 d]\nembeddings, return a single class (e.g., pos or neg)\n1.  Concatenate all the inputs into one long vector of\n    shape [1\u00d7dN], i.e. input is length N of longest review\n   \u2022     If shorter then pad with zero embeddings\n   \u2022     Truncate if you get longer reviews at test time\n2.  Pool the inputs into a single short [1 \u00d7 d] vector. A\n    single \"sentence embedding\" (the same\n    dimensionality as a word) to represent all the\n    words. Less info, but very efficient and fast.\n---\n   For example, for a text with N input words/tokens w1 , ..., wN , we want to turn\n  A pooling function is a way to turn a set of embeddings into a single embeddin\nhe N row embeddings e(w ), ..., e(w ) (each of dimensionality d ) into a single\n    Pooling          1              N\n  For example, for a text with N input words/tokens w1 , ..., wN , we want to tu\n mbedding also of dimensionality d .\ne N row embeddings e(w1 ), ..., e(wN ) (each of dimensionality d ) into a sing\n   There are various ways to pool. The simplest is mean-pooling: taking the mean\n bedding also of dimensionality d .\n y summing the embeddings and then dividing by N :\n    Intuition: exact position not so important for sentiment.\n  There are various ways to pool. The simplest is mean-pooling: taking the mea\n summing the embeddings and then dividing by N :\n                                      N\n    We'll just do some                X\n                          sort of averaging of all the vectors.\n                          xmean = 1    e(wi )                       (6.21)\n    Mean pooling:                   N N\n                                    1 X\n ere are the              xmean = N i=1 e(wi )                      (6.2\n                equations for this classifier assuming mean pooling:\n                                      i=1\n re are the       x = mean(e(w ), e(w ), . . . , e(w ))\n                equations for this classifier assuming mean pooling:\n                                    1    2           n\n                  h = s (xW + b)\n                 x  = mean(e(w1 ), e(w2 ), . . . , e(wn ))\n                  z = hU\n                 h  = s (xW + b)\n    \u02c6\n    y               = softmax(z)                                    (6.22)\n---\nPooling                           p(+)  p(-) p(neut)                  Output probabilities\n                                  ^     ^    ^\n                                  y1    y2   y3         y          [1\u2a093]  Output layer softmax\n\n                                                          U [d\u02b0\u2a093]          weights\n\n                                h1    h2    h3  \u2026 hdh   h [1\u2a09d\u2095]          Hidden layer\n\n                                                                   W [d\u2a09\u1d48\u2095] weights\n                                                        x          [1\u2a09d]  Input layer\n\n                                          +  pooling                      pooled embedding\n           embedding for \u201cdessert\u201d\n           embedding for \u201cwas\u201d                                        N\u2a09d   embeddings\n           embedding for \u201cgreat\u201d\n\n           E                         E                  E             |V|\u2a09d  E matrix\n           1    3                   |V|  1  524  |V|    1  902     |V|    N\u2a09|V|  shared across words\n           0 0  1               0 0                                              one-hot vectors\n                                         0 0  0 1  0 0\n           \u201cdessert\u201d = V3                \u201cwas\u201d = V524   0 0  0  1  0 0\n                                                        \u201cgreat\u201d = V902\n\n           dessert                            was            great         Input words\n---\nSimple Neural    Embeddings as input to\nNetworks and     feedforward classifiers\nNeural\nLanguage\nModels\n---\nSimple Neural    Neural language modeling with\nNetworks and     feedforward networks\nNeural\nLanguage\nModels\n---\nNeural Language Models (LMs)\n\nLanguage Modeling: Calculating the probability of the\nnext word in a sequence given some history.\n\u2022     We've seen N-gram based LMs\n\u2022     But neural network LMs far outperform n-gram\n      language models\nState-of-the-art neural LMs are based on more\npowerful neural network technology like Transformers\nBut simple feedforward LMs introduce many of the\nimportant concepts\n                                                  65\n---\nvast amounts of energy to train, and are less interpretable than\n    Simple feedforward Neural Language Models\n for some smaller tasks an n-gram language model is still the righ\nforward neural language model is a feedforward network that\n e  t  Task: predict next word w\n       a representation of some number of previous words (w  , w\n       given prior words w t    , w , w , \u2026    t \u22121\n s a probability distribution over possible next words. Thus\u2014lik\n               t-1                  t-2  t-3\n  the Problem: Now we\u2019re dealing with sequences of\n       feedforward neural LM approximates the probability of a w\n       arbitrary length.\n rior context  P(wt |w1:t \u22121 ) by approximating based on the N \u2212 1\n       Solution: Sliding windows (of fixed length)\n\n       P(wt |w1 , . . . , wt \u22121 ) \u2248 P(wt |wt \u2212N +1 , . . . , wt \u22121 )\n\nwing examples we\u2019ll use a 4-gram example, so we\u2019ll show a neur\n                                                                    66\n---\n    Inference in a Feedforward Language Model\n\n    p(wt=aardvark|w\u209c\u208b\u2083,w\u209c\u208b\u2082,w\u209c\u208b\u2081)     p(wt=do|\u2026) p(wt=fish|\u2026)  p(wt=zebra|\u2026)\n\n    ^  \u2026                           ^      \u2026      ^    \u2026 ^  \u2026 ^\noutput layer y  y1                 y34           y42  y35102   y|V|  1\u2a09|V|\n   softmax                                  U                  dh\u2a09|V|\n\n       hidden layer h    h1  h2    h3  \u2026 hdh                                      1\u2a09dh\n\n                                   W                                Nd\u2a09dh\n\n    embedding layer e                                                             1\u2a09Nd\nE is shared              E         E                             E  |V|\u2a09d\nacross words             1  35  |V|  1  992  |V|  1                 451  |V|      N\u2a09|V|\nInput layer\n  one-hot                0 0  1  0 0    0 0  0 1  0 0    0 0     0  1    0 0\n  vectors                \u201cfor\u201d = V35    \u201call\u201d = V992     \u201cthe\u201d = V451\n\n    ...\n    \u2026 and thanks                 for         all         the                ?     \u2026\n\n                              wt-3           wt-2        wt-1               wt         67\n---\n hot   Feedforward language model equations\n       input vectors for each input context word, are:\n\n       e        = [Ex\u209c\u2212\u2083 ; Ex\u209c\u2212\u2082 ; Ex\u209c\u2212\u2081 ]\n       h        = s (We + b)\n             z  = Uh\n   \u02c6\n   y            = softmax(z)                          (6.2\n       So \u0177    is the probability of the next word w being\nsemicolons to mean concatenation of vectors, so we form the\n       V 42                               t\ne by     = fish\n       concatenating the 3 embeddings for the three context vecto\n       42\nthis idea of using neural networks to do language modeling in\n---\nWhy Neural LMs work better than N-gram LMs\n\nTraining data:\nWe've seen: I have to make sure that the cat gets fed.\nNever seen: dog gets fed\nTest data:\nI forgot to make sure that the dog gets ___\nN-gram LM can't predict \"fed\"!\nNeural LM can use similarity of \"cat\" and \"dog\"\nembeddings to generalize and predict \u201cfed\u201d after dog\n---\nSimple Neural    Neural language modeling with\nNetworks and     feedforward networks\nNeural\nLanguage\nModels\n\n",
        "quiz": [
            {
                "question_text": "What is the primary purpose of feedforward neural networks in the context of simple classification?",
                "answers": [
                    {
                        "text": "To classify input data into predefined categories",
                        "is_correct": true,
                        "explanation": "The content explicitly states that feedforward neural networks are used for simple classification tasks, such as sentiment analysis, where the goal is to classify input data into predefined categories like positive, negative, or neutral."
                    },
                    {
                        "text": "To generate new text based on input patterns",
                        "is_correct": false,
                        "explanation": "This is a description of language modeling, not the primary purpose of feedforward neural networks in simple classification."
                    },
                    {
                        "text": "To predict continuous numerical values",
                        "is_correct": false,
                        "explanation": "This describes regression tasks, not classification tasks which are the focus of feedforward neural networks in this context."
                    },
                    {
                        "text": "To cluster data into groups based on similarity",
                        "is_correct": false,
                        "explanation": "Clustering is a task typically associated with unsupervised learning, not the supervised classification task described for feedforward neural networks."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the two sample tasks considered for feedforward networks?",
                "answers": [
                    {
                        "text": "Text classification and language modeling",
                        "is_correct": true,
                        "explanation": "The content explicitly lists these as the two sample tasks for feedforward networks."
                    },
                    {
                        "text": "Sentiment analysis and logistic regression",
                        "is_correct": false,
                        "explanation": "Sentiment analysis is a type of text classification, and logistic regression is a method, not a task."
                    },
                    {
                        "text": "Neural language models and classification",
                        "is_correct": false,
                        "explanation": "Neural language models and classification are broader categories, not specific tasks listed in the content."
                    },
                    {
                        "text": "Binary classification and multiclass classification",
                        "is_correct": false,
                        "explanation": "These are types of classification tasks, not the specific sample tasks mentioned in the content."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the output layer in a simple classification neural network?",
                "answers": [
                    {
                        "text": "A single node with a sigmoid activation function",
                        "is_correct": true,
                        "explanation": "The output layer in a simple classification neural network typically uses a single node with a sigmoid activation function to produce a binary output (0 or 1) for classification tasks."
                    },
                    {
                        "text": "Multiple nodes with a softmax activation function",
                        "is_correct": false,
                        "explanation": "This is used for multiclass classification, not simple binary classification."
                    },
                    {
                        "text": "A linear layer without any activation function",
                        "is_correct": false,
                        "explanation": "The output layer in a simple classification network typically includes an activation function to produce a probability."
                    },
                    {
                        "text": "A convolutional layer",
                        "is_correct": false,
                        "explanation": "Convolutional layers are used in convolutional neural networks, not in simple feedforward networks for classification."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the purpose of adding a hidden layer to logistic regression in neural networks?",
                "answers": [
                    {
                        "text": "To allow the network to use non-linear interactions between features",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that adding a hidden layer allows the network to use non-linear interactions between features."
                    },
                    {
                        "text": "To increase the number of input features",
                        "is_correct": false,
                        "explanation": "The purpose is not to increase the number of input features but to enable non-linear interactions between existing features."
                    },
                    {
                        "text": "To reduce the computational complexity of the model",
                        "is_correct": false,
                        "explanation": "Adding a hidden layer does not inherently reduce computational complexity; it may increase it."
                    },
                    {
                        "text": "To linearly transform the input features",
                        "is_correct": false,
                        "explanation": "The hidden layer enables non-linear transformations, not linear ones."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the role of the softmax layer in neural networks?",
                "answers": [
                    {
                        "text": "The softmax layer converts raw output scores into probabilities for each class.",
                        "is_correct": true,
                        "explanation": "The softmax layer is used to convert the raw output scores (logits) into probabilities that sum to 1, making it suitable for multi-class classification tasks."
                    },
                    {
                        "text": "The softmax layer is used to normalize the input features before they are processed by the hidden layers.",
                        "is_correct": false,
                        "explanation": "The softmax layer operates on the output layer, not the input features. It converts the raw output scores into probabilities."
                    },
                    {
                        "text": "The softmax layer is a type of activation function used in the hidden layers of a neural network.",
                        "is_correct": false,
                        "explanation": "The softmax layer is specifically used in the output layer for multi-class classification, not in the hidden layers."
                    },
                    {
                        "text": "The softmax layer is used to reduce the dimensionality of the input data.",
                        "is_correct": false,
                        "explanation": "The softmax layer does not reduce dimensionality; it converts raw output scores into probabilities for each class."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What does the output layer represent in a neural network for sentiment analysis?",
                "answers": [
                    {
                        "text": "The probability of the document having a specific sentiment",
                        "is_correct": true,
                        "explanation": "The output layer in a neural network for sentiment analysis represents the probability of the document having a specific sentiment, such as positive, negative, or neutral."
                    },
                    {
                        "text": "The input features of the document",
                        "is_correct": false,
                        "explanation": "The input features are represented in the input layer, not the output layer."
                    },
                    {
                        "text": "The hidden layer activations",
                        "is_correct": false,
                        "explanation": "The hidden layer activations are part of the hidden layer, not the output layer."
                    },
                    {
                        "text": "The weights of the neural network",
                        "is_correct": false,
                        "explanation": "The weights are parameters of the network and are not represented in the output layer."
                    }
                ],
                "topic": "Nns",
                "subtopic": "Main Content",
                "concepts": [
                    "Nns"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ],
        "summary": "No summary available."
    },
    "data/raw/Week9 - LGT.pdf": {
        "metadata": {
            "file_name": "Week9 - LGT.pdf",
            "file_type": "pdf",
            "content_length": 54823,
            "language": "en",
            "extraction_timestamp": "2025-11-27T18:25:24.796867+00:00",
            "timezone": "utc"
        },
        "content": "                    KING'S\n   Applications of  .\n                    College\n   LLM \u2013 Part I:    LONDON\n   Retrieval\n   Augmented\n   Generation\n   (RAG)\n\n   Week 9 - LGT     BVSH HOVSE\n\nG  Dr Lin Gui\n   Lin.1.gui@kcl.ac.uk\n---\nLearning outcomes\n\n\u26ab By the end of this topic, you will be able to:\n\n  \u26ab  Understand the core concepts of Retrieval-Augmented Generation and how it\n     differs from standard LLM approaches.\n\n  \u26ab  Build and configure a basic RAG pipeline using embeddings, retrievers, and\n     generators.\n\n  \u26ab  Evaluate and optimize RAG performance through effective data preparation,\n     chunking, and retrieval strategies.\n\n2\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n3\n---\n                                                        RAG overview\n\n                                                                               \u26ab                                  When answering questions\n                                                                                                                  or generating text, it first\n                                                                                                                  retrieves relevant\n                                                                                                                  information from a large\n                                                                                                                  number of documents, and\n                                                                                                                  then LLMs generates\n                                                                                                                  answers based on this\n                                                                                                                  information.\n\n                                                                               \u26ab                                  By attaching an external\n                                                                                                                  knowledge base, there is\n                                                                                                                  no need to retrain the\n                                                                                                                  entire large model for each\n                                                                                                                  specific task.\n\n                                                                               \u26ab                                  The RAG model is\n                                                                                                                  especially suitable for\n\n                                           Input        Query                       Indexing                      knowledge-intensive tasks.\nUser                                        How do you evaluate the fact       Documents\n                                            that OpenAI's CEO, Sam Altman,              Chunks Vectors\n\n        Output                              by the board in just three days,\n                                            and then was rehired by the                 embeddings\n                                            company, resembling a real-life\n                                            version of \"Game of Thrones\" in             Retrieval\n\nwithout RAG                                                                    Relevant Documents\n\nfuture events. Currently, I do not have     LLM         Generation\n\nand rehiring of OpenAI's CEO ..            Question:                           Chunk 1: \"Sam Altman Returns to                                4\nwith RAG\n\nthe company's future direction and        based on the following information:     Chunk 2: \"The Drama Concludes? Sam\n                                                                                  Altman to Return as CEO of OpenAl,\nand turns reflect power struggles and     Chunk 2:                                Board to Undergo Restructuring\"\n                                          Chunk 3 :\nOpenAl...                                 Combine Context                         OpenAl Comes to an End: Who Won\n             Answer                       and Prompts                             and Who Lost?\"\n---\n   Symbolic Knowledge or Parametric Knowledge\n\n    \u26ab Ways to optimize LLMs.\n\n    \u26ab Prompt Engineering    This week\n\n    \u26ab Instruct / Fine-tuning\n\n    \u26ab  Retrieval-Augmented\n       Generation\n\n    Week 7    Week 8\n\nExternal Knowledge\n     Required\n    High     Modular RAG\n\n             multiple modules    Retriever Fine-tuning\nAdvanced RAG                     Collaborative Fine-tuning\n\noptimization                     All of the above\n Naive RAG                       RAG             Generator Fine-tuning\n\n   XoT Prompt     Prompt Engineering  Fine-tuning          5\n e.g. CoT, ToT\nFew-shot Prompt\n    Low           Standard Prompt                      Model Adaptation\n           Low                                       High  Required\n---\nRAG vs Fine-tuning\n\n Data Processing\n                ddling.    datasets, and limited datasets may not result\n\n 6\n\n higher latency.    retrieval, resulting in lower latency.\n---\nRAG Application\n\n\u26ab Scenarios where RAG is applicable:\n\n  \u26ab  Long-tail distribution of data\n\n  \u26ab  Frequent knowledge updates\n\n  \u26ab  Answers requiring verification and traceability\n\n  \u26ab  Specialized domain knowledge\n\n  \u26ab  Data privacy preservation\n\nQuestion Answering     Fact checking    Dialog systems    Summarisation\n\nMachine translation    Code generation    Sentiment Analysis    Commonsense\n                                                                reasoning\n\n                                                                           7\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  Foundation of information Retrieval\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 8\n---\n   Foundation of information Retrieval\n\n   \u26ab What is information Retrieval?\n\n     \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n        returns those items to the user, typically in list form sorted per computed\n        relevance#\n\n   \u26ab Three main questions in information retrieval:\n\n     \u26ab  How to map the text into features (Embedding method)\n\n     \u26ab  How to measure the similarity between features (IR Modelling)\n\n     \u26ab  How to do it efficiently (Indexing)\n\n[#] Qiaozhu Mei and Dragomir Radev, \u201cInformation Retrieval,\u201d The Oxford Handbook of Computational Linguistics,\n2\u207f\u1d48 edition, Oxford University Press, 2016.    9\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n10\n---\nDiscrete representation\n\n\u26ab  In discrete representation, for both query and document, we assign each word a\n   specific dimension. If a word appears query/document, then value of the\n   corresponding dimension is:\n\n    \u26ab  In Binary representation: 1\n\n    \u26ab  In TF (term frequency) based representation: t (how many times this word\n       appears within the query/documents)\n\n    \u26ab  In TF-IDF (inverse document frequency) based representation: tlog(n/x)\n\n       \u26ab  Here, t is term frequency, n is number of documents, x is the number of\n          documents which contains this term.\n\n11\n---\nDiscrete representation (example)\n\n\u26ab We have the following documents:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n\u26ab After pre-processing:\n\n  \u26ab  D1 = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d.\n\n  \u26ab  D2 = \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n  \u26ab  D3 = \u201cshipment\u201d, \u201cgold\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n                                                  12\n---\nDiscrete representation (example)\n\n \u26ab Building vocabulary:\n\n   \u26ab V = \u201cshipment\u201d, \u201cgold\u201d, \u201cdamage\u201d, \u201cfire\u201d, \u201cdelivery\u201d, \u201csilver\u201d, \u201carrive\u201d, \u201ctruck\u201d.\n\n \u26ab  Detect the feature for each document. If the feature occurs, the corresponding\n    value is \u20181\u2019, otherwise \u20180\u2019 (binary feature):\n\n           shipment  gold  damage  fire          delivery  silver  arrive  truck\n     D1     1        1     1       1             0         0       0       0\n     D2     0        0     0       0             1         1       1       1\n     D3     1        1     0       0             0         0       1       1\n\n 13\n---\nDiscrete representation (example)\n\n\u26ab  Definition \u2013 term frequency (TF):\n\n    \u26ab  \ud835\udc61 - how many times the term appears in the document\n\n\u26ab  Example:\n\n    \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n    \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n    \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n              shipment  gold  damage  fire  delivery  silver    arrive  truck\n        D1     1        1     1       1     0              0     0      0\n        D2     0        0     0       0     1              2     1      1\n        D3     1        1     0       0     0              0     1      1\n\n                                                                             14\n---\nDiscrete representation (example)\n\n\u26ab Definition \u2013 inverse document frequency (IDF):\n\n  \u26ab  \ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc5b/\ud835\udc65) \u2013 n is number of documents, x is the number of documents which\n     contains this term\n\n\u26ab Example:\n\n  \u26ab  D1 = \u201cShipment of gold damaged in a fire\u201d.\n\n  \u26ab  D2 = \u201cDelivery of silver arrived in a silver truck\u201d.\n\n  \u26ab  D3 = \u201cShipment of gold arrived in a truck\u201d\n\n          shipment     gold  damage  fire  delivery  silver     arrive  truck\n          0.176     0.176    0.477   0.477  0.477    0.477      0.176   0.176\n                       Inverse document frequency vector\n\n                                                                             15\n---\nDiscrete representation (example)\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1      1         1        1      1         0        0           0         0\n D2      0         0        0      0         1        2           1         1\n D3      1         1        0      0         0        0           1         1\n                           Term frequency matrix\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n        0.176     0.176    0.477   0.477    0.477     0.477      0.176     0.176\n                           Inverse document frequency vector\n\n       shipment   gold     damage  fire     delivery  silver     arrive    truck\n D1     0.176     0.176    0.477   0.477     0        0           0         0\n D2      0        0         0      0        0.477     0.954      0.176     0.176\n D3     0.176     0.176     0      0         0        0          0.176     0.176\n\n                            TF-IDF Matrix\n                                                                                16\n---\nEmbedding Method\n\n\u26ab How to map the text into features (vectors)?\n\n  \u26ab  Discrete representation\n\n     \u26ab Convert the input query/document into vectors based on the lexicon\n\n  \u26ab  Continuous representation\n\n     \u26ab Using representation learning to convert the input text into vectors\n\n17\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n18\n---\nContinuous representation\n\n\u26ab  Continuous representation - using representation learning to convert the input\n   text into vectors\n\n   \u26ab  Define the relation first, then using optimiser to update the embedding to\n      approximate the relation.\n\n19\n---\n   Dense Passage Retrieval\n\n   \u26ab  Encode questions and text passages into continuous vectors (embeddings) and\n      retrieve passages using vector similarity instead of keyword overlapping.\n\n   \u26ab  Train directly on question\u2013passage pairs, using in-batch negatives to improve\n      efficiency.\n\n                 Question q    Passage p                                In each batch, there are multiple\n\n                 BERTQ         BERTp                Passage encoder     question\u2013answer pairs, both\n      Question encoder                                                  matched and unmatched. Matched\n                                                                        pairs should have similar\n\n                          OOOOOOOO  0OOOOOOO                            representations, while unmatched\n                                                                        pairs should have representations\n                          $h_q$                                         that are far apart.\n                                                Training phase\n\n      Similarity score: dot product  (q, =  Fine-tune two encoders\nhttps://aclanthology.org/2020.emnlp-main.550.pdf                                                 20\n---\n   ReContriever\n\n   \u26ab  What if we don\u2019t have annotated data (Matched and unmatched QA-pair).\n\n   \u26ab  Using pseudo-examples: For each passage/document p, create an augmented\n      version p\u2032. Then treat (p, p\u2032) as a positive pair:\n\n       \u26ab  Masking words (random word masking)\n\n       \u26ab  Span deletion\n\n       \u26ab  Back-translation Sentence\n\n       \u26ab  Reordering Adding noise\n\n       \u26ab  Perturbations Cropping (taking a subset of sentences)\n\nhttps://aclanthology.org/2023.findings-acl.695.pdf    21\n---\n  Using API\n\n   \u26ab  There are many APIs could do this job, for example, Mistral AI:\n\n                       YMISTRAL EMBED API\n\n                       8OPEN IN COLAB\n\n   \u26ab  Example: link    How to Generate Embeddings\n                       To generate text embeddings using Mistral Al's embeddings APl, we can make a request to the APl endpoint and specify the\n                       embedding model mistra1-embed , along with providing a list of input texts. The APl will then return the corresponding\n                       embeddings as numerical vectors, which can be used for further analysis or processing in NLP applications.\n\n   \u26ab Some other options:    PYTHON TYPESCRIPT      CURL                                                           OUTPUT\n                            import os\n                            from mistralai import Mistral\n          Sentence Bert     api_key = os.environ[\"MISTRAL_API_KEY\"]\n     \u26ab                      model = \"mistral-embed\"\n                            client = Mistral(api_key=api_key)\n\n     \u26ab    SimCSE            embeddings_batch_response = client.embeddings.create(\n                            model=model,\n                            inputs=[\"Embed this sentence.\", \"As well as this one.\"],\n\n     \u26ab    \u2026\u2026                )\n                            The output is an embedding object with the embeddings and the token usage information.\n\n                            Let's take a look at the length of the first embedding:\n\n                            PYTHON TYPESCRIPT CURL\n                            len(embeddings batch response.data[0].embedding)\n\nhttps://docs.mistral.ai/capabilities/embeddings    22\n---\nIR Modelling\n\n\u26ab What is information Retrieval?\n\n  \u26ab  The system searches collections for items relevant to the user\u2019s query. It then\n     returns those items to the user, typically in list form sorted per computed\n     relevance\n\n\u26ab Three main questions in information retrieval:\n\n  \u26ab  How to map the text into features (Embedding method)\n\n  \u26ab  How to measure the similarity between features (IR Modelling)\n\n  \u26ab  How to do it efficiently (Indexing)\n\n23\n---\nIR Modelling\n\n\u26ab  In IR modelling, we use different metric to measure the similarity/distance\n   between the query and given documents. The target is to find the top-k relevant\n   documents based on the given query.\n\n    \u26ab  Cosine similarity (for both Discrete & Continuous representation)\n\n    \u26ab  Jaccard distance (for Discrete representation only)\n\n    \u26ab  BM25 (for Discrete representation only)\n\n24\n---\nCosine similarity\n\n\u26ab Cosine similarity\n                           \u03c3\ud835\udc5b \ud835\udc65\ud835\udc56 \ud835\udc66\ud835\udc56\n                   \ud835\udc36\ud835\udc5c\ud835\udc60 \ud835\udc65, \ud835\udc66 = \u03c3\ud835\udc5b  \ud835\udc56=1 \u03c3\ud835\udc5b\n                                   \ud835\udc56=1(\ud835\udc65\ud835\udc56 )2  \ud835\udc56=1(\ud835\udc66\ud835\udc56 )2\n\n\u26ab Considering\n  \u2212  D1 = [1,1,1,1,0,0,0,0]\n  \u2212  D3 = [1,1,0,0,0,0,1,1]\n                                 \ud835\udc36\ud835\udc5c\ud835\udc60(D1,D3)=1/2\n\n25\n---\nJaccard similarity\n\n\u26ab Only considering if there is over lapping or not. We don\u2019t care about the value.\n\n\u26ab For example:            C1    sim(cl,c2)    C1  C2\n\n  \u26ab  \ud835\udc45\ud835\udc65 = [2,0,3,3]     C2\n\n  \u26ab  \ud835\udc45\ud835\udc66 = [1,1,0,5]     C3                    JACCARD SIMILARITY\n                                                  2    0.5\n                                              4  2+1+1\n\n\u26ab Jaccard similarity: \ud835\udc60\ud835\udc56\ud835\udc5a \ud835\udc65, \ud835\udc66 = \ud835\udc79\ud835\udc99\u2229\ud835\udc79\ud835\udc9a\n                                    \ud835\udc79\ud835\udc99\u222a\ud835\udc79\ud835\udc9a\n\n                                               26\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              hyperparameters         Average length of all docs\n                                                                                  27\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b      \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1      \ud835\udc56               1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n     Maybe\u2026a bit confusing                            Average length of all docs\n     Can you speak in English?  hyperparameters\n                                                                                  28\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b             \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| ) Length of D\n                      \ud835\udc56=1             \ud835\udc56        1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n Relax\u2026it is pretty simple actually    hyperparameters    Average length of all docs\n\n                                                                                  29\n---\nBM25\n\n \u26ab  BM25 is a lexicon based retrieval method that ranks a set of documents based on\n    the query terms appearing in each document, regardless of their proximity within\n    the document.\n\n \u26ab  Given a query \ud835\udc44, containing keywords \ud835\udc5e1, \ud835\udc5e2, \u2026 , \ud835\udc5e\ud835\udc5b , the BM25 score of a\n    document \ud835\udc37 is:  IDF term in Q (is it an important word?)\n\n    \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc37, \ud835\udc44 = \ud835\udc5b    \ud835\udc53 (\ud835\udc5e\ud835\udc56 , \ud835\udc37)(\ud835\udc581 + 1)\n                      \u0dcd \ud835\udc3c\ud835\udc37\ud835\udc39(\ud835\udc5e\ud835\udc56 ) \ud835\udc53 \ud835\udc5e , \ud835\udc37 + \ud835\udc58 (1 \u2212 \ud835\udc4f + \ud835\udc4f |\ud835\udc37| )\n                      \ud835\udc56=1    \ud835\udc56                 1                 \ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc51\ud835\udc59\n\n                              The \u2018percentage\u2019 of querying words in D\n\n                                                                                  30\n---\nIndexing\n\n\u26ab Next question, how to do it efficiently (Indexing)\n\n\u26ab  Suppose we have 1k queries, and there are 1 billion documents in knowledge\n   based, how many times of comparison we need?\n\n\u26ab  1k x 1b\n    It is a really huge number.\n    In real world scenario, it could be even larger\n    If there is only one important task in information retrieval, it must be\n    \u201cindexing\u201d\n\n31\n---\nIndexing - Discrete representation\n\n\u26ab  Inverted index\n\n\u26ab  Since the discrete representation is sparse (most dims are zero), we can build\n   inverted index. For each word, we build a link list to store all the documents\n   contain this word.\n\n\u26ab  For the given query, the complexity is now only related to the #unique words in\n   the query. (In most queries, the size is just few words)\n   doclD                          geo-scopelD              geo-scopelD   docID\n     1                            Europe                   Europe        1 2 7\n     2                                Europe               France        3\n     3                                France               Portugal      5\n     4                                England              England       4\n     5                                Portugal             Quebec        6\n     6                                Quebec               Spain         8\n     7                                Europe\n     8                                Spain\n                     Forward Index                         Inverted Index\n                                                                                 32\n---\n   Indexing - Continuous representation\n\n   \u26ab  In continuous representation, it might be a bit complex. There is no sparse\n      representation anymore.\n\n   \u26ab  We can use the following method to speed up the searching.\n\n       \u26ab  Vector compression \u2013 reduce the size of vectors\n\n       \u26ab  Hierarchical clustering \u2013 in each layer only search the nearest cluster\n                                          Clustering the documents first, and then,\n                                          Only consider the nearest centroid during the searching\n          voronoi cells  xq  Centroids                        voronoi cells  xq    Centroids\n                         o\n                             o                                                     o\n                  e\n                         o\n                  o          \u00a9    o                           9                  o\n                                  -\n                       o                                           o\n         Pause (k)\n\nhttps://www.pinecone.io/learn/series/faiss/faiss-tutorial/                                       33\n---\nContents\n\n\u26ab  RAG overview\n\n\u26ab  Foundation of information Retrieval\n\n\u26ab  RAG Paradigms Shifting\n\n\u26ab  Key Technologies and Evaluation\n\n\u26ab  Applications\n\n34\n---\nNaive RAG\n\n\u26ab  Step 1 \u2013 indexing\n\n    \u26ab  Divide the document into even chunks, each chunk being a piece of the\n       original text.\n\n    \u26ab  Using the encoding model to generate an embedding for each chunk.\n\n    \u26ab  Store the Embedding of each block in the vector database.\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  Retrieve the k most relevant documents using vector similarity search.\n\n\u26ab  Step 3 \u2013 Generation\n\n    \u26ab  The original query and the retrieved text are combined and input into a LLM\n       to get the final answer\n                                                                             35\n---\n  Naive RAG\n\n    \u26ab Step 1 \u2013 indexing\n\n    \u26ab Step 2 \u2013 Retrieval\n\n    \u26ab Step 3 \u2013 Generation\n\n                                    Offline\n\nDocuments  Document Chunks  Vector Database\n    8                                      36\n   User  Query    Related DocumentChunks\n                            Frozen\n    Augmented Prompt        LLM\n---\nAdvanced RAG\n\n  \u26ab Step 1 \u2013 indexing\n\n  \u26ab + index optimization\n\n  \u26ab + pre-retrieval process\n\n  \u26ab Step 2 \u2013 Retrieval\n\n  \u26ab +post-retrieval process\n\n  \u26ab Step 3 \u2013 Generation\n\n  URLS  PDFs  Database\n     Documents               Document Chunks       Vector Database\n                             Fine-grained Data Cleaning\n                             Sliding Window /Small2Big\n                             Add File Structure\n                             Query Rewrite/Clarifcation\n  User    Query              Retriever Router                          37\n                              Pre-retrieval    Related Document Chunks\n\n  Prompt              LLM                        Rerank  Filter  Prompt Compression\n                                                         Post-retrieval\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Sliding windows\n\n    \u26ab  + index optimization       Fine-grained segmentation\n\n    \u26ab  + pre-retrieval process    Adding metadata\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n38\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Sliding windows\n\n      \u26ab  + index optimization       Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process    Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n                                    Document\n\n         0                200 100   300 200  400 300  500\n\n  Split the doc into chunks, and ensure there is over lapping between chunks (WHY?)\n                                                                                   39\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                                      Sliding windows\n\n      \u26ab  + index optimization          Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                            Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation\n\n                          Document     Section 1             Paragraph 1.1\n\n                                       Section 2             Paragraph 1.2\n\n                          Searching on fine-grained text     Paragraph 1.3\n                                                                           40\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing                               Sliding windows\n\n      \u26ab  + index optimization                        Fine-grained segmentation\n\n      \u26ab  + pre-retrieval process                     Adding metadata\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation                             Web page    Publishing date\n\n                                                                    Title\n     The metadata is the aspects of each chunk.\n     It will help both retriever and generator to                Parents node\n     improve the performance.\n\n  41\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process    Summarization\n\n\u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n    \u26ab  +post-retrieval process    Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n\n42\n---\nAdvanced RAG\n\n   \u26ab  Step 1 \u2013 indexing                    Retrieve routes\n\n       \u26ab  + index optimization\n\n       \u26ab  + pre-retrieval process          Summarization\n\n   \u26ab  Step 2 \u2013 Retrieval                   Rewriting\n\n       \u26ab  +post-retrieval process          Confidence judgment\n\n   \u26ab  Step 3 \u2013 Generation                  Instead of one flat \u201cretrieve chunks by embeddings\u201d step, you can:\n\n                                           Searching doc first\n\n  Retrieve routes = multiple retrieval     Query    Document    Chunk\n  paths that a RAG system can choose                            Searching chunks\n  from, depending on query intent, data                         within the doc\n  type, or document structure.\n                                                                                                             43\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing                       Retrieve routes\n\n    \u26ab  + index optimization\n\n    \u26ab  + pre-retrieval process             Summarization\n\n\u26ab  Step 2 \u2013 Retrieval                      Rewriting\n\n    \u26ab  +post-retrieval process             Confidence judgment\n\n\u26ab  Step 3 \u2013 Generation\n                                           Document\n                                                           Summarise first\n                                  Query    Summarisation\n\n                                  Searching in smmarisation\n                                  instead of full documents\n                                                                          44\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing               Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process     Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval              Rewriting\n\n      \u26ab  +post-retrieval process     Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n        Benefits:                    Query\n        a more explicit query             Rewrite the query first\n        a more keyword-rich query    Rewrite the\n        a more structured query      query      Searching\n        multiple diverse sub-queries\n                                     Searching by re-written query\n                                                                  45\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Retrieve routes\n\n      \u26ab  + index optimization\n\n      \u26ab  + pre-retrieval process    Summarization\n\n  \u26ab  Step 2 \u2013 Retrieval             Rewriting\n\n      \u26ab  +post-retrieval process    Confidence judgment\n\n  \u26ab  Step 3 \u2013 Generation\n\n  Query     Document             LLM            Confidence checking    Output\n\n            Confirm the Confidence before output\n             By similarity scores\n             By LLM Confidence scores                                        46\n---\nAdvanced RAG\n\n\u26ab  Step 1 \u2013 indexing           Re-order\n\n    \u26ab  + index optimization    Filter content retrieval\n\n    \u26ab  + pre-retrieval process\n\n\u26ab  Step 2 \u2013 Retrieval\n\n    \u26ab  +post-retrieval process\n\n\u26ab  Step 3 \u2013 Generation\n\n47\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing           Re-order\n\n      \u26ab  + index optimization    Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process\n\n  \u26ab  Step 3 \u2013 Generation         Evidence#1  Evidence#2  Evidence#3  Question\n\n  LLMs is sensitive with the input order\n  The early input chunks has higher weights\n  How to organize the searched evidence for final output is\n  important\n\n  48\n---\nAdvanced RAG\n\n  \u26ab  Step 1 \u2013 indexing              Re-order\n\n      \u26ab  + index optimization       Filter content retrieval\n\n      \u26ab  + pre-retrieval process\n\n  \u26ab  Step 2 \u2013 Retrieval\n\n      \u26ab  +post-retrieval process    Evidence#1  Evidence#2  Evidence#3  Question\n\n  \u26ab  Step 3 \u2013 Generation\n\n                                                           Evidence#1  Evidence#3  Question\n\n  To avoid possible hallucination, filtering the irrelevant\n  evidences.\n\n                                                                                           49\n---\nModular RAG\n\n   Na\u00ef\n\u26ab  ve RAG\n\n      Read           Retrieve    Generate\n\n\u26ab  DSP\n\n   Demonstrate       Search      Predict    Generate\n\n\u26ab  Rewrite-Retrieve-Read\n\n      Rewrite        Retrieve    Read\n\n\u26ab  Retrieve-then-read\n\n   Retrieve          Read        Generate\n\n50\n---\n   Different RAG Paradigms\n\n    Modules\n    8 C 8 E2    Search\nUser Query  Documents    User Query  Documents    Routing    Predict\n\n    Indexing    Query Routing    Indexing    Rewrite  RAG  Rerank\n\n  Read\n                               Fusion\n Memory\n\n    \\Post-Retrieval    Patterns\n \u2192|l|+                               51\nSummary                        Retrieve\n\n    Output    Output\n\n    Naive RAG    Advanced RAG    Modular RAG\n---\nKey problems in RAG\n\n \u26ab  How to retrieve\n\n \u26ab  When to retrieve\n\n \u26ab  How to use the retrieved information\n\n 52\n---\nHow to retrieve\n\n \u26ab By using the information on different structuration levels\n\n \u26ab  Token level         It excels in handling long-tail and cross-domain issues with high\n                        computational efficiency, but it requires significant storage.\n\n \u26ab  Phrase level\n\n \u26ab  Chunk level         The search is broad, recalling a large amount of information, but with\n                        low accuracy, high coverage but includes much redundant information.\n\n \u26ab  Entity level\n\n \u26ab  Knowledge level     Richer semantic and structured information, but the retrieval efficiency\n                        is lower and is limited by the quality of KG.\n\n 53\n---\n   When to retrieve\n\n    \u26ab Two questions:\n\n    \u26ab When we need to retrieve information to support the QA\n\n    \u26ab How many times we need to retrieve the information\n\n    \u26ab Solution#1: Conducting once search during the reasoning process.\n\n    High efficiency, but low relevance of the\n    retrieved documents\n\n    Retrieved document d.\n              Jobs cofounded     Jobs was raisedd;    Jobs is thex    apple\n  Retriever    Apple in his      by adopted...        CEO of        pearnot\n             parents' garage\n Document    Input                 Steve Jobs         Jobs is the     apple    apple\nRetrieval    Reformulation       passed away...       CEO of        pearnot    pearnot    54\nTest Context X    Black-box      Jobs cofoundedJobs is the            apple\n Jobs is the    LM                  Apple...          CEO of           pear\n   CEO of _                                           Ensemble          not\n    Apple\n---\n  When to retrieve\n\n   \u26ab  Two questions:\n\n   \u26ab  When we need to retrieve information to support the QA\n\n   \u26ab  How many times we need to retrieve the information\n\n   \u26ab Solution#2: Adaptively conduct the search.\n\n   Balancing efficiency and information\n   might not yield the optimal solution\n\n Search results:Dx               Retriever\n [1]:Search results:Dq2\n [2]:[1]:Search results:Dq3\n [2]:[1]: ...\nP     [2]: ..                             x\n     x Generate a summary about Joe Biden.\nAy1 Joe Biden attended           q2                55\n Q2[Search(Joe Biden University)]\n y2tthe University of Pennsylvania, where he earned\n q3[Search(Joe Biden degree)]    q3\n y3 a law degree.\n---\n When to retrieve\n\n  \u26ab  Two questions:\n\n  \u26ab  When we need to retrieve information to support the QA\n\n  \u26ab  How many times we need to retrieve the information\n\n  \u26ab Solution#3: Retrieve once for every N tokens generated.\n\n  A large amount of information with low\n  efficiency and redundant information.\n\n    Masked Language Modelling:\n    Bermuda Triangle is in the    western part\n  <MASK> of the Atlantic Ocean.\nPretraining    Atlas\nFew-shot\n          Fact checking:\nBermuda Triangle is in the western                                False    56\n      part of the Himalayas.            The Bermuda\n                                    Triangle is an urban\n                                    legend focused on a\n                                      loosely-defined\n       Question answering:             region in the       Western part of the\n  Where is the Bermuda Triangle?    western part of the    North Atlantic Ocean\n                                       North Atlantic\n                                           Ocean.\n---\nContents\n\n \u26ab  RAG overview\n\n \u26ab  RAG Paradigms Shifting\n\n \u26ab  Key Technologies and Evaluation\n\n \u26ab  Applications\n\n 57\n---\nKey Technologies\n\n\u26ab  Data indexing optimization\n\n\u26ab  Structured Corpus\n\n\u26ab  Retrieval Source Optimization\n\n\u26ab  KG as a Retrieval Data Source\n\n\u26ab  Query Optimization\n\n\u26ab  Embedding Optimization\n\n\u26ab  Fine-tuning on RAG\n\n58\n---\n Data indexing optimization\n\n  \u26ab Chunk optimization\n\n  \u26ab Small-2-big: Embedding at sentence level expand the window during generation\n\n  process.\n\n                   Embed Sentence \u2192 Link to Expanded Window\n\n                              Continuous observation of the Atlantic meridional\n                              overturning circulation (AMOC) has improved the\n                              understanding of its variability (Frajka-Williams et al.,    What the LLM Sees\n                              2019), but there is low confidence in the quantification\n                              of AMOC changes in the 20th century because of low\n                   Embeddingagreement in quantitative reconstructed and simulated\n                              trends. Direct observational records since the\n                   Lookup     mid-2000s remain too short to determine the relative\nQuestion:                     contributions of internal variability, natural                                59\n                              forcing and anthropogenic forcing to AMOC change\nWhat are the                  (high confidence). Over the 21st century, AMOC wil\nconcerns                      very likely decline for all SSP scenarios but will not\n                              involve an abrupt collapse before 2100. 3.2.2.4 Sea Ice\nsurrounding the               Changes\nAMOC?                         Sea ice is a key driver of polar marine life, hosting        What the LLM Sees\n                              unique ecosystems and affecting diverse marine\n                              organisms and food webs through its impact on light\n                              penetration and supplies of nutrients and organic\n                              matter (Arrigo, 2014)\n---\nData indexing optimization\n\n\u26ab  Chunk optimization\n\n\u26ab  Sliding window: sliding chunk covers the entire text, avoiding semantic ambiguity.\n\n                          Maintain overlap for\n                          contextual continuity\n\nLoaded large\ndocument  Dividing into  Merging units into\n          compact units  larger chunks\n\n60\n---\nData indexing optimization\n\n \u26ab  Chunk optimization\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 61\n---\nStructured Corpus\n\n \u26ab  Adding meta-data: adding meta-data in the query searching to improve retrieval\n    accuracy, provide context during chunking, and enables filtering\n\n                             Indexed documents\n                                                                        Filtered subset of\n                                                                        documents    Most relevant\n    Did we implement any new                                                           documents\n       policies in 2021?     year: 2020, content: ...\n\n                             year: 2020, content:                       year: 2021, content.:..\n    year = 2021              year: 2021, content: .    Select relevant  year: 2021, content...    Vector similarity    year: 2021, content:.\n                                                       documents                                  search\n                             year: 2021, content:..                     year: 2021, content...                       year: 2021, content:...\n    Metadata filter          year: 2021, content: .\n\n Filter the irrelevant docs\n\n                           Ensure each chunk contains the metadata\n\n                                                                  62\n---\nRetrieval Source Optimization\n\n \u26ab  Adding meta-data\n\n \u26ab  Two-stage method: Retrieve documents through summaries, then retrieve text\n    blocks from the documents.\n\n     \u26ab  Step 1 \u2014 Search Summaries (Coarse Retrieval) You maintain a summary index,\n        where each summary represents a larger document, chapter, or cluster.\n\n     \u26ab  Step 2 \u2014 Search Related Chunks (Fine Retrieval) Once you find the top\n        summaries, you only search inside their associated chunks.\n\n 63\n---\nKG as a Retrieval Data Source\n\n\u26ab  Extract entities from the user's input query, then construct a subgraph to form\n   context, and finally feed it into the large model for generation.\n\n    \u26ab  Use LLM (or other models) to extract key entities from the question.\n\n    \u26ab  Retrieve subgraphs based on entities, delving to a certain depth, such as 2\n       hops or even more.\n\n    \u26ab  Utilize the obtained context to generate answers through LLM.Two-stage\n       method: Retrieve documents through summaries, then retrieve text blocks\n       from the documents.\n\n64\n---\n  KG as a Retrieval Data Source\n\n  \u26ab  Extract entities from the user's input query, then construct a subgraph to form\n     context, and finally feed it into the large model for generation.\n     P Meta Summary Entities        x k    Layer[i+1]\n                                    Summary Entities    Summarization by LLM  Layer[i]\n                                    Normal Entities     GMM Clustering        Layer[i-1]\n     Documents                      Hilndex: Indexing with Hierarchical Knowledge\n\n                                    6Communities        Global                                            Community Report\n                  Query             Key Entity          Bridge\n                                    Reasoning Paths                              Reasoning Paths                          Generation by LLM\n                                                        xk\n                                    Hatten KG             Local    Key Entity\n                                                                  Descriptions\n                                                        HiRetrieval: Retrieval with Hierarchical Knowledge\n\nhttps://arxiv.org/pdf/2503.10150                                                                                                           65\n---\n   Query Optimization\n\n    \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n       the Query can yield better retrieval results.\n\n\u26ab  Rewrite query:\n                 Input     Input\n                           Black-box LLM\n\n                 Retriever    Rewriter\n\nInput    Example\nSmall PrLM  Input:\n            What profession does Nicholas Ray and\nRewriter            Elia Kazan have in common?\n\n                                  Query             Quuery              Query: Nicholas Ray profession\n                     Documents    Web Search        Web Search            Query: Elia Kazan profession\n                                  Retriever         Retriever           Elia Kazan was an American film and\n                                                                         theatre director, producer,\n       Black-box LLM                                Documents            screenwriter and actor, described\n           Reader                 Documents                               Nicholas Ray American author and\n                                                                          director, original name Raymond\n                                                                          Nicholas Kienzle, born August 7,\n                     Output       Black-box LLM     Black-box LLM         1911, Galesville, Wisconsin, U.S.\n                                  Reader            Reader               Correct (reader                   director\n                                  Output            Reward Output        Hit (retriever\n                     (a) Retrieve-then-read (b)Rewrite-retrieve-read    (c) Trainable rewrite-retrieve-read\n\n https://arxiv.org/pdf/2305.14283    66\n---\n Query Optimization\n\n  \u26ab  Questions and answers do not always possess high semantic similarity; adjusting\n     the Query can yield better retrieval results.\n\n  \u26ab Clarify the query:                                        Ambiguous Question (AQ)\n                                                    \"What country has the most medals         o  M\n                                                             in Olympic history?\"             88\n\n                                                      Tree of Clarifications\n                                                      Question                       Pruned   Information\n                                                   Clarification                              Retrieval\n                                                                       *\n                                                   DQ1            DQ2   DQ3\n                                                                       \"What country has\n                                                                       the most total medals\n                                                                       in Olympic history?\"\n                                                   Question       Question\n                                                   Clarification  Clarification\n                                                   *                   *                      Passages\n                                                   DQ11 DQ12 DQ13 DQ21 DQ22 DQ23 DQ24\n                                                   \"What country has the  \"What country has\n                                                   most medals in winter  the most gold medals\n                                                     Olympic history?\"  in Olympic history?\"\n\n                                                                                     Long Form Answer\n\n                                                     Answer                          \"The United States has the\n                                                   Generation                        most total medals. .\n                                                                                        Norway has won most\n\nhttps://aclanthology.org/2023.emnlp-main.63.pdf                                      medals in winter Olympic.\"    67\n---\n Embedding Optimization\n\n  \u26ab Better embedding always indicate a better retrieval results:\n\n             \u26ab    Selecting a more suitable embedding method\n                                                                0Retriever &  HotpotQA      Dataset\n                  Fine-tuning the embedding model               Framework      EM F1        2Wiki      NQ   WebQ\n             \u26ab                                                  [BM25                       EM F1 EM F1 EM F1\n                                                                               |25.4, 37.2[16.6 21.1[26.0 32.8 [22.2 31.2\n                                                                2+SuRe      38.8 53.523.8.31.036.6 47.934 4 48.5\n                                                                 +EmbQA (ours) 42.0 55.8|27.4 36.642.2 54.438.2 52.1\n                                                                DPR            20.6 21.7[10.8 13.5[25.0 34.2[23.8 34.4\n                                                                 +SuRe         25.0 31.9 14.2 16.038.8 52.336.0 49.6\n                                                                 +EmbQA (ours) |29.8 36.3 16.8 21.0    38.0 52.0\n                                                                                                   43.0 54.4\n                                                                 Contriever   [22.6 35.4\n                                                                                     [16.6 20.7[25.8 32.8\n                                                                                                             25.2 34.2\n                                                                 +SuRe          33.8 50.6 21.0 29.3 39.0 52.834.4 48.5\n                                                                 +EmbQA (ours) 36.6 52.7 26.4 34.2 42.2 53.6\n                  Try different embedding methods in the RAG    [BM25         [21.2 29.2               36.0 49.6\n                                                                20+SuRe        32.2 46.1 [13.8 21.7 18.8 25.319.0 26.1\n                                                                                            17.8 30.1\n                                                                                                    35.2 45.131.6 45.7\n                                                                  +EmbQA (ours) 34.8 44.3 18.6 30.5 35.8 46.035.8 48.1\n                                                                [DPR               7.8 11.0 3.8 4.5[22.2,26.718.8 27.7\n                                                                +Sure          15.0 21.8 6.4 8.540.0 51.8\n                                                                 +EmbQA (ours) 16.2 23.3 7.6 9.6             32.6 47.7\n                                                                                                   |40.2 49.433.4 46.0\n                                                                Contriever     19.4 28.6[13.6 20.7[21.8 27.4117.8,244\n                                                                 +SuRe         28.0 41.6 17.2 25.4 39.8 51.630.2 45.0\n                                                                 +EmbQA (ours)29.8 42.3 17.4 26.2 40.6 51.8 31.6 43.0\n                                                                [BM25          [28.6 37.1[20.2 24.1 [24.0 29.4 [22.6 31.4\n                                                                20+Sure        43.6 54.7 28.4 34.1 41.6 49.0 36.6 47.3\n                                                                 +EmbQA (ours) 44.6 55.628.8 33.8 42.4 49.2 38.2 48.7\n                                                                [DPR           8.8 9.8 5.6 7.1\n                                                                                                  [29.2 32.6[25.6 31.1\n                                                                 +Sure         21.8 27.3 12.2 16.1\n                                                                                                   45.4 54.6\n                                                                                                             38.4 49.6\n                                                                 +EmbQA (ours) 22.6 29.1 13.8 17.345.8 54.7\n  AD                                                                                                      38.6 50.1\n  Facts                                                         Contriever     27.0 34.0[17.6 20.0 26.6 31.9 21.0 29.1\n\n0                                                                +Sure         38.8 50.323.8 30.4 44.0 52.9 36.4 48.1\n                                                                +EmbQA (ours)39.0 50.2\n            General-Purpose                                                              24.4 30.9 45.2 50.5 37.0 48.6\n     C-Pack  Text Embedding                                                                                              68\nhttps://arxiv.org/pdf/2503.01606\n\n  C-MTEB  C-MTP  C-TEM  Recipe\n---\nEmbedding Optimization\n\n \u26ab Better embedding always indicate a better retrieval results:\n\n   \u26ab  Selecting a more suitable embedding method\n\n   \u26ab  Fine-tuning the embedding model\n\n      An in-context learning based method to generate prompt\n\n                                                                                       generate Query-Doc pair    Fine-tuning with pseudo data\n\n 1           A few query and\n             relevant document\n             examples\n             for each doc     You are an award\n             in documents     winning relevance                                 GPT-x\n                              expert. Suggest          Large Language Model     BARD\n                              relevant queries for                              Flan-T5\nDocuments                     this article $article                                                               69\n                              queries:                 Synthetic queries for documents\n             LLM Query Generation Prompt\n                                                       \"Labeled data\"                  9\n---\nFine-tuning on RAG\n\n\u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n  \u26ab  Retriever fine-tuning\n\n  \u26ab  Generator fine-tuning\n\n70\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n      \u26ab  Retriever fine-tuning\n\n      \u26ab  Generator fine-tuning\n\n                              A small LM\n\n    Using the attention scores\n    annotate which documents\n    the LM \u201cprefers\u201d.\n\n                                                DecSource LM\n                                                    Fusion-in-Decoder\n                                                Enc Enc. Enc\n    Source Task                                     Q+D1 Q+D2Q+DN\n                                                    Retrieve\n                                                      N Docs\n  Positives         Negatives                   Pre-Trained Retriever    71\nGround Truth U    ANCE Sampling\n    -Top-K FiDAtt                               Target LMs Target Tasks\n    https://aclanthology.org/2023.acl-long.136.pdf\n                                     Generic                GCMETRY\n                                     Plug-In                  WAKT\n   Augmentation-Adapted Retriever                           RISTORY\n                                                            LITERATRE\n                                                            SCIENCE\n                                                              MATH\n---\n   Fine-tuning on RAG\n\n    \u26ab It is also possible to train the RAG framework by fine-tuning, including:\n\n    \u26ab Retriever fine-tuning\n\n    \u26ab Generator fine-tuning\n\n    Product Search                 Premium hiking bag.            Unstructured Data                                             Structured Data (Negative)\n    Black large capacity hiking    Size: Max Color: Black                                     Structured Data (Positive)\n    bag, made of canvas.           Material: canvas               These erasable mood pencils     [MASK1] changing [MASK2]       Tire Specifications: Material:\n\nCode Search\nGiven two numbers, the\n\nlargest number is returned\nQuery\n]\nPretrained Language Model\n                               are made of quality wood       [MASK3] with [MASK4]            Rubber Tire Size: 16X6.50-8\n                               and color temperature          Function: removing wrong        Tire Type: Tubeless Rim\ndef compare(a, b):             coating, have non-fading        writing Material: [MASK2]      Width: 5.375\" Tread Depth:\nreturn max(a, b)          colors.                              Changing Size: 18 x 0.5 cm     7.1mm\" Pattern: P332\nStructured Data (Positive)\n Structured Data (Negative)                                               T5\n                               Structured Data Alignment (Loss: C sDA)      Masked Entity Prediction (Loss: LP)\n\n    Training    Push Away                           Prediction                                          erasers\n                                                    [MASK1] Color\n                Ground Truth                        [MASK2] mood\n                                Align               [MASK3] pencils,\n    .                                               [MASK4]\n\n    Embedding Space    Optimized Embedding Space    Add an entity prediction loss in the fine-tuning           72\n---\nK\nING'S\nCollege\nLONDON\n\nThank you\n\nDr Lin Gui\nLin.1.Gui@kcl.ac.uk\nwww.kcl.ac.uk/people/lin-gui\n\n",
        "quiz": [
            {
                "question_text": "What is the primary focus of Week9 - LGT?",
                "answers": [
                    {
                        "text": "Retrieval-Augmented Generation (RAG) and its applications",
                        "is_correct": true,
                        "explanation": "The content context explicitly states that the primary focus of Week9 - LGT is on Retrieval-Augmented Generation (RAG) and its applications."
                    },
                    {
                        "text": "Advanced techniques in prompt engineering",
                        "is_correct": false,
                        "explanation": "While prompt engineering is mentioned, it is not the primary focus of Week9 - LGT according to the content context."
                    },
                    {
                        "text": "Fine-tuning large language models",
                        "is_correct": false,
                        "explanation": "Fine-tuning is listed as a way to optimize LLMs but is not the primary focus of Week9 - LGT."
                    },
                    {
                        "text": "Symbolic knowledge and parametric knowledge",
                        "is_correct": false,
                        "explanation": "These concepts are mentioned but are not the primary focus of Week9 - LGT."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "Who is the instructor for Week9 - LGT?",
                "answers": [
                    {
                        "text": "Dr. Lin Gui",
                        "is_correct": true,
                        "explanation": "The content context explicitly lists Dr. Lin Gui as the instructor for Week 9 - LGT."
                    },
                    {
                        "text": "BVSH HOVSE",
                        "is_correct": false,
                        "explanation": "BVSH HOVSE is mentioned in the content context but not as the instructor."
                    },
                    {
                        "text": "KING'S College London",
                        "is_correct": false,
                        "explanation": "KING'S College London is mentioned as the institution but not as the instructor."
                    },
                    {
                        "text": "LLM \u2013 Part I: LONDON",
                        "is_correct": false,
                        "explanation": "LLM \u2013 Part I: LONDON is mentioned as part of the topic but not as the instructor."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the email address of the instructor for Week9 - LGT?",
                "answers": [
                    {
                        "text": "Lin.1.gui@kcl.ac.uk",
                        "is_correct": true,
                        "explanation": "The email address provided in the content context for Dr. Lin Gui."
                    },
                    {
                        "text": "gui.lin@kcl.ac.uk",
                        "is_correct": false,
                        "explanation": "This is a plausible but incorrect variation of the email address format."
                    },
                    {
                        "text": "lin.gui@kings.ac.uk",
                        "is_correct": false,
                        "explanation": "This is a plausible but incorrect variation using a different domain."
                    },
                    {
                        "text": "dr.lin.gui@kcl.ac.uk",
                        "is_correct": false,
                        "explanation": "This is a plausible but incorrect variation with an added prefix."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the main topic covered in Week9 - LGT?",
                "answers": [
                    {
                        "text": "Retrieval-Augmented Generation (RAG)",
                        "is_correct": true,
                        "explanation": "The content explicitly describes the main topic as Retrieval-Augmented Generation (RAG), including its overview, foundation, paradigms, and applications."
                    },
                    {
                        "text": "Standard LLM approaches",
                        "is_correct": false,
                        "explanation": "While standard LLM approaches are mentioned as a comparison, they are not the main topic of Week9 - LGT."
                    },
                    {
                        "text": "Information Retrieval",
                        "is_correct": false,
                        "explanation": "Information Retrieval is a subtopic under RAG, not the main topic of the week."
                    },
                    {
                        "text": "Prompt Engineering",
                        "is_correct": false,
                        "explanation": "Prompt Engineering is mentioned as a way to optimize LLMs but is not the main topic of Week9 - LGT."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the three key learning outcomes for Week9 - LGT?",
                "answers": [
                    {
                        "text": "Understand the core concepts of Retrieval-Augmented Generation and how it differs from standard LLM approaches.",
                        "is_correct": true,
                        "explanation": "This is directly listed as one of the key learning outcomes for Week9 - LGT."
                    },
                    {
                        "text": "Build and configure a basic RAG pipeline using embeddings, retrievers, and generators.",
                        "is_correct": false,
                        "explanation": "This is another key learning outcome, but not the one asked for in the question."
                    },
                    {
                        "text": "Evaluate and optimize RAG performance through effective data preparation, chunking, and retrieval strategies.",
                        "is_correct": false,
                        "explanation": "This is another key learning outcome, but not the one asked for in the question."
                    },
                    {
                        "text": "Understand the core concepts of prompt engineering and fine-tuning.",
                        "is_correct": false,
                        "explanation": "This is not mentioned as a key learning outcome for Week9 - LGT."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the primary technique discussed in Week9 - LGT for optimizing LLMs?",
                "answers": [
                    {
                        "text": "Retrieval-Augmented Generation",
                        "is_correct": true,
                        "explanation": "The concept description explicitly mentions Retrieval-Augmented Generation as a technique for optimizing LLMs."
                    },
                    {
                        "text": "Prompt Engineering",
                        "is_correct": false,
                        "explanation": "While mentioned in the context, it is not highlighted as the primary technique for optimizing LLMs in the provided content."
                    },
                    {
                        "text": "Instruct / Fine-tuning",
                        "is_correct": false,
                        "explanation": "This is listed as a way to optimize LLMs but not the primary technique discussed in the context."
                    },
                    {
                        "text": "Collaborative Fine-tuning",
                        "is_correct": false,
                        "explanation": "This is mentioned as an advanced RAG optimization technique, but not the primary technique discussed."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What are the two ways to optimize LLMs mentioned in Week9 - LGT?",
                "answers": [
                    {
                        "text": "Prompt Engineering and Retrieval-Augmented Generation",
                        "is_correct": true,
                        "explanation": "The content explicitly mentions 'Prompt Engineering' and 'Retrieval-Augmented Generation' as ways to optimize LLMs."
                    },
                    {
                        "text": "Instruct / Fine-tuning and Symbolic Knowledge",
                        "is_correct": false,
                        "explanation": "While 'Instruct / Fine-tuning' is mentioned, 'Symbolic Knowledge' is not presented as a method to optimize LLMs."
                    },
                    {
                        "text": "Modular RAG and Collaborative Fine-tuning",
                        "is_correct": false,
                        "explanation": "These are advanced optimization techniques but are not the primary methods mentioned for optimizing LLMs in the content."
                    },
                    {
                        "text": "Parametric Knowledge and Naive RAG",
                        "is_correct": false,
                        "explanation": "'Parametric Knowledge' is not mentioned as a method to optimize LLMs, and 'Naive RAG' is a type of RAG, not a standalone optimization method."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the main application of Retrieval-Augmented Generation (RAG) discussed in Week9 - LGT?",
                "answers": [
                    {
                        "text": "Enhancing language models with external knowledge retrieval for knowledge-intensive tasks",
                        "is_correct": true,
                        "explanation": "The concept description explicitly states that RAG is suitable for knowledge-intensive tasks and involves retrieving relevant information from documents before generating answers."
                    },
                    {
                        "text": "Improving the speed of language model training",
                        "is_correct": false,
                        "explanation": "The content does not mention anything about improving the speed of language model training as an application of RAG."
                    },
                    {
                        "text": "Reducing the need for human intervention in text generation",
                        "is_correct": false,
                        "explanation": "While RAG does involve automated retrieval and generation, the content does not specifically highlight reducing human intervention as its main application."
                    },
                    {
                        "text": "Enhancing the creativity of language models",
                        "is_correct": false,
                        "explanation": "The content focuses on knowledge-intensive tasks and external knowledge retrieval, not on enhancing creativity."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the process of Retrieval-Augmented Generation (RAG) as described in Week9 - LGT?",
                "answers": [
                    {
                        "text": "Retrieval-Augmented Generation (RAG) first retrieves relevant information from a large number of documents and then generates answers based on this information.",
                        "is_correct": true,
                        "explanation": "This is the correct description of the RAG process as provided in the content context."
                    },
                    {
                        "text": "Retrieval-Augmented Generation (RAG) involves retraining the entire large model for each specific task.",
                        "is_correct": false,
                        "explanation": "The content context explicitly states that RAG does not require retraining the entire model for each task."
                    },
                    {
                        "text": "Retrieval-Augmented Generation (RAG) only uses pre-existing knowledge stored within the model.",
                        "is_correct": false,
                        "explanation": "The content context describes RAG as attaching an external knowledge base, which contradicts this option."
                    },
                    {
                        "text": "Retrieval-Augmented Generation (RAG) is suitable for tasks that do not require external knowledge.",
                        "is_correct": false,
                        "explanation": "The content context states that RAG is especially suitable for knowledge-intensive tasks, making this option incorrect."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            },
            {
                "question_text": "What is the benefit of using an external knowledge base in RAG as discussed in Week9 - LGT?",
                "answers": [
                    {
                        "text": "It avoids the need to retrain the entire large model for each specific task.",
                        "is_correct": true,
                        "explanation": "The content explicitly states that attaching an external knowledge base in RAG eliminates the need to retrain the entire large model for each specific task."
                    },
                    {
                        "text": "It increases the computational complexity of the model.",
                        "is_correct": false,
                        "explanation": "The content does not mention any increase in computational complexity as a benefit of using an external knowledge base in RAG."
                    },
                    {
                        "text": "It requires more data storage for the model parameters.",
                        "is_correct": false,
                        "explanation": "The content does not discuss data storage requirements as a benefit or drawback of using an external knowledge base in RAG."
                    },
                    {
                        "text": "It improves the model's ability to generate creative fiction.",
                        "is_correct": false,
                        "explanation": "The content focuses on knowledge-intensive tasks, not creative fiction, as the benefit of using an external knowledge base in RAG."
                    }
                ],
                "topic": "Week9 - Lgt",
                "subtopic": "Main Content",
                "concepts": [
                    "Week9 - Lgt"
                ],
                "difficulty": "easy",
                "explanation": ""
            }
        ],
        "summary": "The document \"Week9 - LGT.pdf\" from King's College London discusses Retrieval-Augmented Generation (RAG), a technique that combines information retrieval with generative models. RAG enhances LLMs by first retrieving relevant information from external sources and then generating responses based on this data, eliminating the need for retraining the entire model for specific tasks. The course aims to help students understand RAG, build basic RAG pipelines, and optimize their performance."
    }
}