{
    "LLMs.pdf": {
        "metadata": {
            "file_name": "LLMs.pdf",
            "file_type": "pdf",
            "content_length": 16700,
            "language": "en"
        },
        "content": "Large       Introduction to Large Language\nLanguage    Models\nModels\n---\nLarge language models\n\nComputational agents that can interact\nconversationally with people using natural language\nLLMS have revolutionized the field of NLP and AI\n---\nLanguage models\n\n\u2022       Remember the simple n-gram language model\n   \u2022     Assigns probabilities to sequences of words\n   \u2022     Generate text by sampling possible next words\n   \u2022     Is trained on counts computed from lots of text\n\u2022       Large language models are similar and different:\n   \u2022     Assigns probabilities to sequences of words\n   \u2022     Generate text by sampling possible next words\n   \u2022     Are trained by learning to guess the next word\n---\n Fundamental intuition of large language models\n\nText contains enormous amounts of knowledge\nPretraining on lots of text with all that knowledge is\n what gives language models their ability to do so\n much\n---\nWhat does a model learn from pretraining?\n\n\u2022     With roses, dahlias, and peonies, I was\n      surrounded by flowers\n\u2022     The room wasn't just big it was enormous\n\u2022     The square root of 4 is 2\n\u2022     The author of \"A Room of One's Own\" is Virginia\n      Woolf\n\u2022     The doctor told me that he\n---\n    What is a large language model?\n    A neural network with:\n    Input: a context or prefix,\n    Output: a distribution over possible next words\n\n                                                   p(w|context)\n                               output              all     .44\n\n                                                the        .33\n                                               your        .15\n    Transformer (or other decoder)             that        .08\n\n input                                ?\ncontext  So  long  and    thanks  for\n---\nLLMs can generate!\n\nA model that gives a probability distribution over next words can generate\nby repeatedly sampling from the distribution\n\n                  p(w|context)\n                  output                  all    .44\n\n                            the                  .33\n                                         your    .15\nTransformer (or other decoder)    that           .08\n                                  \u2026               \u2026\n\nSo  long  and  thanks  for  all                  p(w|context)\n                                   output        the   .77\n\n                                                 your  .22\n                                              our      .07\n          Transformer (or other decoder)       of      .02\n                                                 \u2026     \u2026\n\n          So  long  and  thanks    for  all  the\n---\nThree architectures for large language models\nw w                                          w\n\nw w w w w\n\nw  w     w w w   w  w  w w w   w  w w\n\n   Decoder                        Encoder-Decoder\nDecoders         Encoders      Encoder-decoders\n                    Encoder\n\nGPT, Claude,     BERT family,  Flan-T5, Whisper\nLlama            HuBERT\nMixtral\n---\nDecoders                                    w w w w        w\n\n                                            w  w  w w      w  w\nWhat most people think of when we say LLM Decoder\n\u2022     GPT, Claude, Llama, DeepSeek, Mistral\n\u2022     A generative model\n\u2022     It takes as input a series of tokens, and iteratively\n      generates an output token one at a time.\n\u2022     Left to right (causal, autoregressive)\n---\nEncoders              w     w w w w\n\n\u2022  Masked Language Models (MLMs)\n                      w     w  w w w  w  w  w  w     w\n\u2022  BERT family              Decoder      Encoder\n\n\u2022  Trained by predicting words from surrounding\n   words on both sides\n\u2022  Are usually finetuned (trained on supervised data)\n   for classification tasks.\n---\nEncoder-Decoders    w w w\nw w  w  w w\n\n\u2022          w  w  w  w w  w  w  w  w w  w  w  w\n    Trained to map from one sequence to another\n\u2022   Very      Decoder       Encoder       Encoder-Decoder\n           popular for:\n   \u2022     machine translation (map from one language to\n         another)\n   \u2022     speech recognition (map from acoustics to words)\n---\nLarge       Introduction to Large Language\nLanguage    Models\nModels\n---\nLarge       Pretraining Large Language\nLanguage    Models\nModels\n---\n    Three stages of training in LLMs\n\n    Instruction Data    Preference Data\n\nPretraining    Label sentiment of this sentence:    Human: How can I embezzle money?\n    The movie wasn\u2019t that great\n    Data    Summarize: Hawaii Electric urges       Assistant: Embezzling is a\n        caution as crews replace a utility pole    felony, I can't help you\u2026\n             overnight on the highway from\u2026         Assistant: Start by creating\n\n    Translate English to Chinese:                   fake expense reports...\n    When does the flight arrive?\n\n    1. Pretraining    2. Instruction    3. Preference\n                         Tuning            Alignment\n\nPretrained    Instruction    Aligned LLM\n   LLM         Tuned LLM\n---\nPretraining\n\nThe big idea that underlies all the amazing\nperformance of language models\n\nFirst pretrain a transformer model on enormous\namounts of text\nThen apply it to new tasks.\n---\nSelf-supervised training algorithm\n\nWe train them to predict the next word!\n1. Take a corpus of text\n2. At each time step t\n i.      ask the model to predict the next word\n ii.     train the model using gradient descent to minimize the\n         error in this prediction\n\n \"Self-supervised\" because it just uses the next word as the\n label!\n---\nIntuition of language model training: loss\n\n\u2022         Same loss function: cross-entropy loss\n     \u2022     We want the model to assign a high probability to true\n           word w\n     \u2022     = want loss to be high if the model assigns too low a\n           probability to w\n\u2022         CE Loss: The negative log probability that the model\n          assigns to the true next word w\n     \u2022     If the model assigns too low a probability to w\n     \u2022     We move the model weights in the direction that assigns a\n           higher probability to w\n---\n                    L   = \u2212        y [w] log \u02c6\nl to minimize the   CE             t             yt [w]\n                    error in predicting the true next word in the training sequ\n       Cross                 w\u2208V\n                 -entropy loss for language modeling\ncross-entropy as the loss function.\necall that the cross-entropy loss measures the difference between a pred\n of language modeling, the correct distribution        yt comes from kn\nbility distribution and the correct distribution.\n       CE loss: difference between the correct probability distribution and the predicted\n . This is represented as a one-hot vector corresponding to the v\n       distribution       X\nentry for the actual next word is 1, and all the other entries are\n                       L  = \u2212      y [w] log \u02c6\nentropy loss           CE          t    yt [w]                                           (\n                       for language modeling is determined by the proba\n                               w\u2208V\n igns to the correct next word (all other words get multiplied by\n ase   The correct distribution y knows the next word, so is 1 for the actual next\n   of language modeling, the correct distribution y comes from knowin\nthe CE loss in (10.5)        t                    t\n       word and 0 for the   can be simplified as the negative log prob\nord. This is                others.\n                 represented as a one-hot vector corresponding to the vocabu\n       So in this sum, all terms get multiplied by zero except one: the logp the\n igns to the next word in the training sequence.\nthe entry for the actual next word is 1, and all the other entries are 0. T\n       model assigns to the correct next word, so:\n ss-entropy loss for language modeling is determined by the probability\n                       L    ( \u02c6                   \u02c6\nl assigns to the       CE    yt , yt ) = \u2212 log yt [wt +1 ]\n                       correct next word (all other words get multiplied by zero\n t the CE loss in (10.5) can be simplified as the negative log probabilit\n---\nTeacher forcing\n\n\u2022     At each token position t, model sees correct tokens w1:t,\n     \u2022  Computes loss (\u2013log probability) for the next token wt+1\n\u2022     At next token position t+1 we ignore what model predicted\n      for w\u209c\u208a\u2081\n     \u2022  Instead we take the correct word w\u209c\u208a\u2081, add it to context, move on\n---\nTraining a transformer language model\n\nTrue next token  long        and        thanks        for        all           \u2026\n\nCE Loss          \u2212log ylong  \u2212log yand  \u2212log ythanks  \u2212log yfor  \u2212log yall     \u2026\nper token\n\n                 \u0177  back     \u0177  back    \u0177  back       \u0177  back    \u0177  back\n                    prop        prop       prop          prop       prop\n\nLLM                                                                            \u2026\n\nInput tokens  So  long  and  thanks  for    \u2026\n---\n LLMs are mainly trained on the web\n\nCommon crawl, snapshots of the entire web produced by\n the non- profit Common Crawl with billions of pages\nColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156\n billion tokens of English, filtered\n What's in it? Mostly patent text documents, Wikipedia, and\n news sites\n---\nThe Pile: a pretraining corpus\nComposition of the Pile by Category\nacademics    web                   books\n             Academic Internet Prose DialogueMisc\n\n                                                                 Bibliotik\n\nPile-CC                                                          PG-19       BC2\n\nPubMed Central    ArXiv\n                                                                             Subtitles\n                                                                                      dialog\n\n                  PMA    StackExchange                           Github      IRC EP\nFreeLaw           USPTO  Phil  NIH OpenWebText2    Wikipedia     DM Math     HN  YT\n---\nFiltering for quality and safety\n\nQuality is subjective\n\u2022     Many LLMs attempt to match Wikipedia, books, particular\n      websites\n\u2022     Need to remove boilerplate, adult content\n\u2022     Deduplication at many levels (URLs, documents, even lines)\nSafety also subjective\n\u2022     Toxicity detection is important, although that has mixed results\n\u2022     Can mistakenly flag data written in dialects like African American\n      English\n---\n Reexamining \"Fair Use\" in the Age of\n AI There are problems with scraping from the web\n Generative Al claims to produce new language and images, but when those ideas are based on copyrighted\n material, who gets the credit? A new paper from Stanford University looks for answers.\n Jun 5, 2023 | Andrew Myersfin 0\n\n                                                                                                       Authors Sue OpenAI Claiming Mass Copyright\n                                                                                                       Infringement of Hundreds of Thousands of Novels\n\n The Times Sues OpenAI and Microsoft\nOver A.I. Use of Copyrighted Work\nMillions of articles from The New York Times were used to train\n chatbots that now compete with it, the lawsuit said.\n---\nThere are problems with scraping from the web\n\nCopyright: much of the text in these datasets is copyrighted\n\u2022     Not clear if fair use doctrine in US allows for this use\n\u2022     This remains an open legal question across the world\nData consent\n\u2022     Website owners can indicate they don't want their site crawled\nPrivacy:\n\u2022     Websites can contain private IP addresses and phone numbers\nSkew:\n\u2022     Training data is disproportionately generated by authors from the\n      US which probably skews resulting topics and opinions\n---\nLarge       Pretraining Large Language\nLanguage    Models\nModels\n---\nLarge       Evaluating Large Language\nLanguage    Models\nModels\n---\n   P(w ) = P(w )P(w |w )P(w |w  ) . . . P(w |w\n We\u2019ve been talking about predicting one word at a time, computing the proba\n       1:n    1  2  1  3  1:2               n  1:n\u2212\nof the next token w from the prior context: P(w |w ). But of course as we\n    Better        n\n              LMs are better at predicting text\n hapter 3 the     iY      i  <i\n                chain rule allows us to move between computing the probability\n next token and =       P(wi |w<i )\n                computing the probability of a whole text:\n    Reminder of the chain rule:\n       P(w         i=1\n                1:n ) = P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P(wn |w1:n\u22121 )\npute the probability of text just by multiplying the cond\n                        n\n ch                = Y P(wi |w<i )\n    token in the text. The resulting (log) likelihood of a\n                        i=1\n mparing how good two language models are on that text\ne can compute the probability of text just by multiplying the conditional pr\n        So given a text w1:n we could just compare the log likelihood from two LMs:\nities for each token in the text. The resulting (log) likelihood of a text is a us\n                                   n\n tric for comparing how good two language   Y\n        log likelihood(w          ) = models are on that text:\n                               1:n  log            P(wi |w<i )\n                                            n\n                  log likelihood(w  ) = log Y i=1\n                               1:n           P(wi |w<i )\n---\nBut raw log-likelihood has problems\n\nProbability depends on size of test set\n\u2022  Probability gets smaller the longer the text\n\u2022  We would prefer a metric that is per-word,\n   normalized by length\n---\nPerplexity is normalized for length\n\nPerplexity is the inverse probability of the test set,\nnormalized by the number of words\n (The inverse comes from the original definition of\n perplexity from cross-entropy rate in information theory)\n\n Probability range is [0,1], perplexity range is [1,\u221e]\n---\n          set), normalized by the test set length in tokens. For a test set of n toke\n    Perplexity\nAs we     perplexity is\n          first saw in Chapter 3, one way to evaluate language models is\n  well they predict unseen text. Intuitively, good models are those that\n                              Perplexity          (w ) =          P (w )\u2212 1\n    So just as for n-gram grammars, we use perplexity                     n\n                                                  1:n               to measure how\n robabilities to unseen data (are less     q                      q    1:n\n                                           surprised when encountering the\n    well the LM predicts unseen text                     = s           1\n    The perplexity of a model \u03b8 on an unseen test                 n\ntiate this intuition by using perplexity to measure set is the inverse\n                                                                       the quality of a\n    probability that \u03b8 assigns to the test set,                      Pq (w1:n )\n el. Recall from page   ?? that the perplexity of    normalized by the test\n    set length.                                                  a model q on an unseen\n erse probability that  q assigns to the test set, normalized by the test\n          To visualize how perplexity can be computed as a function of the proba\na test  For a test set of n tokens w       the perplexity is :\n        set of  n tokens w  , the perplexity is\n          LM computes for each new word, we can use the chain rule to expand th\n                            1:n    1:n\n          tion of probability of the test set: \u2212 1\n                Perplexity (w1:n ) = P (w1:n ) n\n                           q          q                          v\n                                     s                           u n\n                                           1                     uY       1\n                                   =                             t\n                                   Perplexity    ~~(w~~   ) =\n                                        n  q              1:n    n   P (w  ~~(10.7)~~  \n                                           P (w1:n )                 q    i |w<i )\n                                           q                        i=1\n---\nPerplexity\n\n\u2022     The higher the probability of the word sequence, the lower\n      the perplexity.\n\u2022     Thus the lower the perplexity of a model on the data, the\n      better the model.\n\u2022     Minimizing perplexity is the same as maximizing\n      probability\n\nAlso: perplexity is sensitive to length/tokenization so best used\nwhen comparing LMs that use the same tokenizer.\n---\nMany other factors that we evaluate, like:\n\nSize\nBig models take lots of GPUs and time to train, memory to store\nEnergy usage\nCan measure kWh or kilograms of CO2 emitted\nFairness\nBenchmarks measure gendered and racial stereotypes, or decreased\nperformance for language from or about some groups.\n\n"
    },
    "Transformers.pdf": {
        "metadata": {
            "file_name": "Transformers.pdf",
            "file_type": "pdf",
            "content_length": 48841,
            "language": "en"
        },
        "content": " Introduction to Transformers\n\nTransformers\n---\nLLMs are built out of transformers\n\nTransformer: a specific kind of network architecture, like a\nProvided proper attribution is provided, Google hereby grants permission to\nfancier feedforward network, but based on attention\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\n\nAttention Is All You Need\n\n2023        Ashish Vaswani\u21e4      Noam Shazeer\u21e4    Niki Parmar\u21e4      Jakob Uszkoreit\u21e4\n            Google Brain         Google Brain     Google Research   Google Research\nAug         avaswani@google.com  noam@google.com  nikip@google.com  usz@google.com\n            Llion Jones\u21e4         Aidan N. Gomez\u21e4 \u2020         \u0141ukasz Kaiser\u21e4\n2          Google Research    University of Toronto          Google Brain\n[cs.CL]    llion@google.com    aidan@cs.toronto.edu    lukaszkaiser@google.com\n                                 Illia Polosukhin\u21e4 \u2021\n                                 illia.polosukhin@gmail.com\n---\nA very approximate timeline\n\n1990 Static Word Embeddings\n2003 Neural Language Model\n2008 Multi-Task Learning\n2015 Attention\n2017 Transformer\n2018 Contextual Word Embeddings and Pretraining\n2019 Prompting\n---\n Attention\n\nTransformers\n---\n    Instead of starting with the big picture\n    Let's consider the embeddings for an individual word from a particular layer\nNext token  long  and  thanks  for    all\nNext token  long  and  thanks  for    all\nLanguage  logits    logits    logits    logits    logits    \u2026\nModeling\n Language  logits U    logits U    logits  U  logits  U  logits  U  \u2026\n  Head\n Modeling    U         U                   U  U                  U\n  Head\n\n                   \u2026  \u2026  \u2026  \u2026  \u2026\n    Stacked         ~~\u2026~~    \u2026  \u2026  \u2026  \u2026\n    Stacked                            \u2026\n    Transformer                        \u2026\n    Transformer\n    Blocks\n    Blocks\n\n                                                                                                                     \u2026\n                 ~~x1~~       ~~x2~~       ~~x3~~       ~~x4~~      x5                                               \u2026\n                x1           x2           x3           x4           x5\n Input           ~~+~~       1   ~~+~~     ~~2~~     ~~+~~     ~~3~~     ~~+~~     ~~4~~     ~~+~~     ~~5~~         \u2026\n Input          +            1    +       2         +         3                   +         4         +           5  \u2026\n    Encoding    E                 E                 E                   E                      E\nEncoding        E                 E                 E                             E                   E\nInput tokens    So                long              and                 thanks                 for\nInput tokens    So                long              and                 thanks                 for\n---\nProblem with static embeddings (word2vec)\n\nThey are static! The embedding for a word doesn't reflect how its\nmeaning changes in context.\n\nThe chicken didn't cross the road because it was too tired\n\nWhat is the meaning represented in the static embedding for \"it\"?\n---\nContextual Embeddings\n\n\u2022       Intuition: a representation of meaning of a word\n        should be different in different contexts!\n\u2022       Contextual Embedding: each word has a different\n        vector that expresses different meanings\n        depending on the surrounding words\n\u2022       How to compute contextual embeddings?\n     \u2022  Attention\n---\nContextual Embeddings\n\nThe chicken didn't cross the road because it\n\nWhat should be the properties of \"it\"?\n\nThe chicken didn't cross  the road because it was  too  tired\nThe chicken didn't cross  the road because it was  too  wide\n\nAt this point in the sentence, it's probably referring to either the chicken or the street\n---\nIntuition of attention\n\nBuild up the contextual embedding from a word by\nselectively integrating information from all the\nneighboring words\nWe say that a word \"attends to\" some neighboring\nwords more than others\n---\nIntuition of attention:\n\ncolumns corresponding to input tokens\ntest\n    chicken            because\n    didn\u2019t                    tired\nLayer k+1  The         cross road  was too\n                       the         it\n\nself-attention distribution\n\nchicken                    because\ndidn\u2019t                            tired\nLayer k                    cross road  was too\n           The             the         it\n---\nAttention definition\n\nA mechanism for helping compute the embedding for\na token by selectively attending to and integrating\ninformation from surrounding tokens (at the previous\nlayer).\n\nMore formally: a method for doing a weighted sum of\nvectors.\n---\n    Attention is left-to-right\n\n    a1    a2    a3    a4    a5\n\nSelf-Attention  attention    attention    attention    attention    attention\n    Layer\n\n    x1                       x2           x3           x4           x5\n---\n ords to other words? Since our representations for\n    Simplified version of attention: a sum of prior words\n    Verson 1:         score(xi , x j ) = xi \u00b7 x j                     (1\ne use of our old friend the  dot product that we used\n    weighted by their similarity with the current word\nty in Chapter 6, and also played a role in attention in\nsult of a dot product is a scalar value ranging from \u2212\u2022 to \u2022, the la\n sult   Given a sequence of token embeddings:\n e      of this comparison between words             i and j as a\n   more similar the vectors that are being compared. Continuing with o\n quation  x\u2081  x\u2082 x\u2083   x\u2084     x\u2085 x\u2086 x\u2087    xi\nhe        to add attention to the computation of this\n     first step in computing y3 would be to compute three scores: x3\n x   \u00b7 x  Produce: a = a weighted sum of x through x   (and x )\n          . Then to make effective use of these scores, we\u2019ll normalize t\n     3    3    i                         1             7    i\n               Weighted by their similarity to x\n  ax to create a vector of weights,  a , that indicates the proporti\nRS AND         L ARGE L ANGUAGE M ODELSi j i\nof each input to the input element i that is the current focus of attentio\n rson 1:       score(xi , x j ) = xi \u00b7 x j                  (10.4)\nch weighted by its  a value.\nt is a         ai j  = softmax(score(xi , x j )) \u2200 j \u2264 i              (1\n               scalar value ranging from         \u2212\u2022 to \u2022, the larger\n               a     = X exp(score(x , x ))\n vectors that        a x             i        j             (10.7)\n                 are being  ~~compared. ~~         Continuing with our\n                 i   = Pi i j j                    \u2200 j \u2264 i            (1\n                     j\u2264i      exp(score(xi , xk ))\n---\nIntuition of attention:\n columns corresponding to input tokens\n\n test    chicken    because\n         didn\u2019t\n Layer k+1  The  cross road    too tired\n                    the        it was\n\n self-attention distribution\n\n chicken                    because\n didn\u2019t                            tired\n Layer k  The               cross road\n                            the       it was too\n\n          x1 x2 x3 x4 x5 x6 x7        xi\n---\nAn Actual Attention Head: slightly more complicated\n\nHigh-level idea: instead of using vectors (like xi and x\u2084)\ndirectly, we'll represent 3 separate roles each vector xi plays:\n\u2022     query: As the current element being compared to the\n      preceding inputs.\n\u2022     key: as a preceding input that is being compared to the\n      current element to determine a similarity\n\u2022     value: a value of a preceding element that gets weighted\n      and summed\n---\nAttention intuition columns corresponding to input \u1d57\u1d52\u1d4f\u1d49\u207f\u02e2\n query\n chicken    because\n didn\u2019t            tired\n Layer k+1  The  cross road  was too\n                 the         it\n\n self-attention distribution\n\n                 chicken    because\n                 didn\u2019t             tired\n Layer k                    cross road  was too\n            The             the         it\n\n            x1 x2 x3 x4 x5 x6 x7        xi\n            values\n---\nIntuition of attention:\n                columns corresponding to input tokens\n                                                query\n\n                  chicken                      because\n                       didn\u2019t                          tired\nLayer k+1  The               cross       road          was too\n                                   the               it\n\nself-attention distribution\n\n                  chicken                      because\n                       didn\u2019t                                 tired\nLayer k                      cross       road          was too\n           The                     the               it\n\nkeys       x1 x2 x3 x4 x5 x6 x7 xi\nvalues     k      k     k     k     k     k     k     k\n           v      v     v     v     v     v     v     v\n---\n ine a similarity weight. We\u2019ll refer to this role as a k\n    An Actual Attention Head: slightly more complicated\nlly, as a value of a preceding element that gets weigh\n pute the output for the current element.\n    We'll use matrices to project each vector xi into a\n    representation of its role as query, key, value:\nhese three different roles, transformers introduce w\nW V\u2022    query: WQ\n    . These weights will project each input vector x\n   \u2022    key: W\u1d37                                         i\n s a key, query, or value:\n   \u2022    value: W\u2c7d\n\n        qi = xi W Q ;  ki = xi W K ;  vi = xi W V\n\njections, when we are computing the similarity of t\n---\no capture these three different roles, transformers introduce w\n    K  An Actual Attention Head: slightly more complicated\nW , and W V . These weights will project each input vector xi i\nof its role as a key, query, or value:\n       Given these 3 representation of xi\n              qi = xi W Q ;  ki = xi W K ;  vi = xi W V\n       To compute similarity of current element x with\n these projections, when we are computing the similarity of th\n x     some prior element x                 i\n    with some prior element x , we\u2019ll use the dot product betw\n    i  W      j                j\n nt\u2019s  e\u2019ll use dot product between   q and k .\n       query vector q and the preceding element\u2019s key vector k\n       And instead   i                i     j                 j\n ult of a            of summing up x , we'll sum up v\n              dot product can be an arbitrarily large (positive or negat\n                                j                    j\n entiating large values can lead to numerical issues and loss of g\nng. To avoid this, we scale the dot product by a factor related to\n---\n    i                                 i\n by summing the values of the prior elements, each weig\ns   Final equations for one attention head\n   key to the query from the current element:\n\n    qi = xi WQ ; k j      = x j W\u1d37 ; v j = x j W\u2c7d\n                          qi \u00b7 k j\n    score(xi , x j ) =      \u221adk\n\n                 ai j     = softmax(score(xi , x j )) \u2200 j \u2264 i\n                 ai       = X ai j v j\n                          j\u2264i\n---\n    Calculating the value of a3\n\n    5. Weigh each value vector\n                                         \ud6fc3,1\n    4. Turn into \ud6fci,j weights via softmax\n\n    3. Divide score by \u221adk  \u221adk              \u00f7\n\n 2. Compare x3\u2019s query with\nthe keys for x1, x2, and x3\n\n                                         Wk  k\n   1. Generate                           Wq  q\nkey, query, value\n     vectors    x1                       Wv  v\n\n    \u00d7\n\n    Output of self-attention  a\u2083\n\n6. Sum the weighted\n   value vectors\n\n    \u00d7\n    \ud6fc3,2    \ud6fc3,3\n\n    \u221adk \u00f7    \u221adk  \u00f7\n\n           Wk  k           Wk  k\n\n           Wq  q           Wq  q\n\n    x2     Wv  v    x3     Wv  v\n---\nActual Attention: slightly more complicated\n\n\u2022     Instead of one attention head, we'll have lots of them!\n                                                      9.2   \u2022   T RANSFORMER B LOCKS         7\n\u2022     Intuition: each head might be attending to the context for different purposes\n     shows an intuition.\n     \u2022  Different linguistic relationships or patterns in the context\n\n        qc = xi WQc ;   kc = x j WKc ;  vc         = x j WVc ;  \u2200 c  1 \u2264 c \u2264 h               (9.14)\n        i               j               j\n                                                   qc \u00b7 kc\n                             c                        i     j\n                             score (xi , x j ) =     \u221adk                                     (9.15)\n\n                                        a c        = softmax(scorec (xi , x j )) \u2200 j \u2264 i     (9.16)\n                                            i j       X\n                                          headc    =        a c vc                           (9.17)\n                                        i                   i j  j\n                                                      j\u2264i\n                                        ai         = (head1 \u2295 head2 ... \u2295 headh )WO          (9.18)\n        MultiHeadAttention(xi , [x1 , \u00b7 \u00b7 \u00b7 , xN ]) = ai                                     (9.19)\n---\n    Multi-head attention\n\n                           ai\n\n    Project down to d      W\u1d3c [hdv x d] [1 x d]\n\n    Concatenate Outputs    \u2026                                    [1 x hdv ]\n\n     Each head             Head 1[1 x dv ]  Head 2 [1 x dv ]    Head 8\nattends differently        W\u1d37 W\u2c7d WQ         W\u1d37 W\u2c7d WQ       \u2026    W\u1d37 W\u2c7d WQ\n     to context            1  1      1      2  2      2         8         8  8\n\n    \u2026 xi-3 xi-2  xi-1                          xi  [1 x d]\n                                                      ai\n---\nSummary\n\nAttention is a method for enriching the representation of a token by\nincorporating contextual information\nThe result: the embedding for each word will be different in different\ncontexts!\nContextual embeddings: a representation of word meaning in its\ncontext.\nWe'll see in the next lecture that attention can also be viewed as a\nway to move information from one token to another.\n---\n Attention\n\nTransformers\n---\n  The Transformer Block\n\nTransformers\n---\n    Reminder: transformer language model\n\n    Next token  long  and  thanks  for    all\n\nLanguage        logits    logits    logits    logits    logits    \u2026\nModeling        U         U         U         U         U\n  Head\n\n    Stacked    \u2026  \u2026  \u2026  \u2026  \u2026  \u2026\nTransformer\n   Blocks\n\n    x1    x2    x3    x4    x5              \u2026\n\n Input    +  1  +  2  +  3  +  4  +  5      \u2026\nEncoding    E    E    E     E     E\n\n    Input tokens  So  long  and  thanks  for\n---\n    The residual stream: each token gets passed up and\n    modified\n\n    hi-1    hi    hi+1\n\n    +\n\n     Feedforward\n\n    \u2026    Layer Norm    \u2026\n         +\nMultiHead\nAttention\n\n          Layer Norm\n\n    xi-1    xi    xi+1\n---\network be larger than the model dimensionality d .  (For example in the orig\n ansformer model, d = 512 and d  = 2048.)\n    We'll need nonlinearities, so a feedforward layer\n    ff\n    FFN(xi ) = ReLU(xi W1 + b1 )W2 + b2              (9\n\n ayer Norm    h                  h                  h\n              At two stages in the transformer block we normalize the vector\n              i-1                i                  i+1\nt al., 2016). This process, called layer norm (short for layer normalization), is\n                                 +\n\n                                      Feedforward\n\n              \u2026                       Layer Norm       \u2026\n                                 +\nMultiHead\nAttention\n                                      Layer Norm\n\n                   xi-1          xi                 xi+1\n---\n    Layer norm: the vector xi is normalized twice\n\n    hi-1    hi    hi+1\n\n    +\n\n     Feedforward\n\n    \u2026    Layer Norm    \u2026\n         +\nMultiHead\nAttention\n\n          Layer Norm\n\n    xi-1    xi    xi+1\n---\n ken. Thus the input to layer norm is a single vector of dimensionality                                  d\n                                         v\n utput is that vector normalized,        u\n                                 d  again of dimensionality d . The first step in\n                         X u                   d\n    Layer                1                     X\n                   Norm                  t 1\nrmalization is to calculate the mean,          \u03bc , and standard deviation, s , over the\n                   \u03bc     =    s     x                 2         (9.21)\n s of the vector to        d        = i        d      (xi \u2212 \u03bc )                                           (9.22)\n                      be normalized. Given an embedding vector x of dimen-\ny d , these values are     vi=1                   i=1\n                         calculated  ~~as follows.~~  \n     Layer norm is a variation u\n                           of the z-score from statistics, applied to a single vec- tor in a hidden layer\n                               u    d\n these values, the                  X\n                       vector components are normalized by subtracting the mean\n                   s     = t 1        d (x     \u2212 \u03bc )2              (9.22)\n ach and dividing                1 X i\n                      by the standard deviation. The result of this computation is\nvector with zero         \u03bc = dd i=1      xi                        (9.21)\n                   mean and a standard deviation of one.\nlues, the                        vi=1\n              vector components      ~~are ~~  normalized by subtracting the mean\n                                 u       d (x \u2212 \u03bc )\n                                 u 1 X\n                                    \u02c6\n                                 t x =                                                                    (9.23)\n ividing by the standard deviation.             ~~The ~~  result of this computation is\n                         s =                   (x \u2212 \u03bc )2           (9.22)\n th zero mean and a standard d                    is\n                                    deviation of one.\n                                         i=1\nly, in the standard implementation of layer normalization, two learnable param-\n ese values, the              (x \u2212 \u03bc )\n g and b             vector components are normalized by subtracting the mean\n          , representing gain and offset values, are introduced.\n                         \u02c6\n h and                   x =                                       (9.23)\n           dividing by the standard deviation. The result of this computation is\nctor with zero mean                 s                 (x \u2212 \u03bc )\n                        and a standard deviation of one.\n                         LayerNorm(x) = g                     + b\ntandard implementation of layer normalization, two learnable param-                                       (9.24)\n, representing                      (x \u2212 \u03bc )              s\n              gain and offset values, are introduced.\n---\n                   Putting it all together    The function computed by a transforme\n     Putting together a single transformer block\n                   pressed by breaking it down with one equation for each compo\n                   using t (of shape [1 \u00d7 d ]) to stand for transformer and supersc\n                   each computation inside the block:\n  hi-1    hi                      hi+1\n                                         t1   = LayerNorm(xi )\n          +                              i                              \u21e5    \u21e4\n                                         t2   = MultiHeadAttention(t1 ,      x1 , \u00b7 \u00b7 \u00b7 , x1  )\n                   Feedforward           i                    i              1  N\n                    Layer Norm           t3   = t2 + xi\n\u2026                                     \u2026  i    i\n          +                              t4   = LayerNorm(t3 )\n                    MultiHead            i              i\n                    Attention            t5   = FFN(t4 )\n                    Layer Norm           i    5  3i\n                                         hi   = ti + ti\n     xi-1    xi                   xi+1\n                   Notice that the only component that takes as input information\n                   (other residual streams) is multi-head attention, which (as we see\n---\n    A transformer is a stack of these blocks\n    so all the vectors are of the same dimensionality d\n\n    hi-1    hi    hi+1\n\n    +\n\n                    Feedforward\n\n    Block 2    \u2026  +  Layer Norm    \u2026\n\n                     MultiHead\n                     Attention\n\n                     Layer Norm\n\n    xi-1    xi    xi+1\n\n    hi-1    hi    hi+1\n\n    +\n\n     Feedforward\n\n    Block 1    \u2026  +  Layer Norm    \u2026\nMultiHead\nAttention\n\nLayer Norm\n\n    xi-1    xi    xi+1\n---\n     Residual streams and attention\n\n    Notice that all parts of the transformer block apply to 1 residual stream (1\n     token).\n    Except attention, which takes information from other tokens\n     Elhage et al. (2021) show that we can view attention heads as literally moving\n     information from the residual stream of a neighboring token into the current\n     stream .\n\nToken A      Token B\nresidual     residual\n    stream    stream\n---\n  The Transformer Block\n\nTransformers\n---\n Parallelizing Attention\n Computation\nTransformers\n---\n  9.3  Parallelizing computation using X\n       \u2022  PARALLELIZING COMPUTATION USING A SINGLE MATRIX X  11\n       For attention/transformer block we've been computing a single\n ension).\n       output at a single time step i in a single residual stream.\n allelizing attention  Let\u2019s first see this for a single attention head and then turn\n       But we can pack the N tokens of the input sequence into a single\n ultiple heads, and then add in the rest of the components in the transformer\n       matrix X of size [N \u00d7 d].\n k. For one head we multiply X by the key, query, and value matrices WQ of\n       Each row of X is the embedding of one token of the input.\n e [d \u00d7 dk ], WK of shape [d \u00d7 dk ], and WV of shape [d \u00d7 dv ], to produce matrices\n       X can have 1K-32K rows, each of the dimensionality of the\n f shape [N \u00d7 dk ], K \u2208 RN \u00d7d\u1d4f , and V \u2208 RN \u00d7d\u1d5b , containing all the key, query, and\n       embedding d (the model dimension)\n e vectors:\n           Q = XWQ ;            K = XWK ; V = XWV                 (9.31)\nen these matrices we can compute all the requisite query-key comparisons simul-\n ously by multiplying Q and K| in a single matrix multiplication. The product is\n---\nQK\u1d40\n\nNow can do a single matrix multiply to combine Q and K\u1d40\n\nq1\u2022k1 q1\u2022k2 q1\u2022k3 q1\u2022k4\n\n q2\u2022k1  q2\u2022k2  q2\u2022k3  q2\u2022k4\nN\n q3\u2022k1  q3\u2022k2  q3\u2022k3  q3\u2022k4\n\nq4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4\n\nN\n---\n  The N \u00d7 N QK| matrix showing how it computes all q \u00b7 k comparisons\nx   Parallelizing attention    i                         j\n    multiple.\n    \u2022 Scale the scores, take the softmax, and then\n e have this QK| matrix, we can very efficiently scale these scores,\n , and  multiply the result by V resulting in a matrix of\n        then multiply the result by V resulting in a matrix of shape N\n        shape N \u00d7 d\nbedding representation for each token in the input. We\u2019ve reduced\n        \u2022 An attention vector for each input token\nattention step for an entire sequence of N tokens for one head to\n omputation:       \u2713                \u2713 QK| \u25c6\u25c6\n\n                A = softmax mask    \u221adk     V             (9\n\n ut the future  You may have noticed that we introduced a mask func\n---\n lf-attention step for an entire sequence of N tokens for one head\nng   Masking out the future\n     computation:    \u2713     \u2713 QK| \u25c6\u25c6\n\n                     A = softmax mask  \u221adk   V\n\n g   \u2022 What is this mask function?\n   out the future    You may have noticed that we introduced a mask f\n .32  QK\u1d40 has a score for each query dot every key,\n      above. This is because the self-attention computation as we\u2019ve de\n      including those that follow the query.\nroblem: the calculation in  QK| results in a score for each quer\n      \u2022 Guessing the next word is pretty simple if you\n key value,  including those that follow the query. This is inapprop\n      already know it!\nng of language modeling: guessing the next word is pretty simple\nknow it!     To fix this, the elements in the upper-triangular portion\n re zeroed out (set to \u2212\u2022), thus eliminating any knowledge of wo\nn the sequence.      This is done in practice by adding a mask matrix\n---\n ctor embedding representation for each token in the input. We\u2019ve reduced the\n ire self-attention step for an entire sequence of N tokens for one head to the\nlowing Masking out the future\n   computation:\n                                 \u2713  \u2713 QK| \u25c6\u25c6\n                 A = softmax        mask  \u221adk      V    (9.32)\n\n sking out the future  You may have noticed that we introduced a mask function\n q. 9.32 above. This is because the self-attention computation as we\u2019ve described\n    Add \u2013\u221e to cells in upper triangle                   q1\u2022k1 \u2212\u221e \u2212\u221e \u2212\u221e\nas a problem: the calculation in           QK| results in a score for each query value\n very key value,       including those that follow the query. This is inappropriate in\n    The softmax will turn it to 0                       q2\u2022k1 q2\u2022k2 \u2212\u221e \u2212\u221e\n setting of language modeling: guessing the next word is pretty simple if you\n                                                   N\neady know it!    To fix this, the elements in the                      \u2212\u221e\n                                                      upper-triangular portion of the\n                                                        q3\u2022k1 q3\u2022k2 q3\u2022k3\n trix are zeroed out (set to \u2212\u2022), thus eliminating any knowledge of words that\nlow in the sequence.   This is done in practice by      q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4\n                                                             adding a mask matrix M in\n ich Mi j = \u2212\u2022 \u2200 j > i (i.e. for the upper-triangular portion) and Mi j = 0 otherwise.\n . 9.9 shows the resulting masked QK| matrix. (we\u2019ll see            N\n                                                                  in Chapter 11 how to\n---\n ctor embedding representation for each token in the input. We\u2019ve reduced the\n ire self-attention step for an entire sequence of N tokens for one head to the\nlowing Another point: Attention is quadratic in length\n   computation:\n                                 \u2713  \u2713 QK| \u25c6\u25c6\n                 A = softmax        mask  \u221adk      V    (9.32)\n\n sking out the future   You may have noticed that we introduced a mask function\n q. 9.32 above. This is because the self-attention computation as we\u2019ve described\n                                 q1\u2022k1 \u2212\u221e \u2212\u221e       \u2212\u221e\nas a problem: the calculation in           QK| results in a score for each query value\n very key value,       including those that follow the query. This is inappropriate in\n setting of language modeling:   q2\u2022k1 q2\u2022k2 \u2212\u221e    \u2212\u221e\n                        N       guessing the next word is pretty simple if you\neady know it!    To fix this, the elements in the upper-triangular portion of the\n                                 q3\u2022k1 q3\u2022k2 q3\u2022k3 \u2212\u221e\n trix are zeroed out (set to \u2212\u2022), thus eliminating any knowledge of words that\nlow in the sequence.             q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4\n                                 This is done in practice by adding a mask matrix M in\n ich Mi j = \u2212\u2022 \u2200 j > i (i.e. for the upper-triangular portion) and Mi j = 0 otherwise.\n . 9.9 shows the resulting masked QK|        N\n                                               matrix. (we\u2019ll see in Chapter 11 how to\n---\n Q  Q  X  X  K  K  X  X  V  V\n Attention again  V  V\n X  Input  Q WK WK  Key  Key  K  W  W\n Query  Input  X  Input Input  X Value Value  V\n Query Q  XToken 1  K  Token 1  X  V\n Token 1  Token 1  Token 1\n Token 1  Token 1\n Token 1  Token 1\n Token 1  XX  Q  X  Q Q  Q  X X  X K  K VK  K  X X  WV  V  V\n Input  Input  K  Input  Key  W  Key  Input  X  V\n Input  Key  Input Input\n Query Query  W Input  Query W  Key  Input  K  W  Value  Value Value  V  Value\n Token 1\n Query  Token 2  Token 1 Token 2  Token 1  Token 1  V\n Q  Token 2  W\n Token 2  K  Token 2  V  Token 1\n Input  Token 1  =  Token 2 K\n x  = Token 1  W  Key  Token 2\n Token 2  Token 1  Input  Token 2\n x  Token 1  Input\n Token 2  Q  Q  x  =  W W  Value\n =  Input  =\n W  Query  W  x  Token 1\n Token 1  Input  Input  Input  W  Key  Key  Input  Input  Value\n W  W  Value\n Input  Token 1  Query  Query  Token 1  Key  Token 1  Token 1\n Token 1  Token 1  Token 1  Input  Token 1  Token 1  Input  Token 1\n Token 1  Token 1  Token 1Token 1  Value\n Input Query  Key  Key  Input  Token 1\n Input  Key  Input  Token 1\n Query Query  Input  Token 1  Token 1  Input  Value  Value  Value\n Token 2\n Query  Input  Token 2  Token 2  x  =  Token 2  Key  Token 2  Token 2\n Input\n Input  Token 3  Input  =\n Token 3  Token 2  x\n x  =  Token 3  Token 3  Key\n Input  Token 2  Input\n Token 3  Token 2  Input  Value\n Query =  Input  Token 3  Key  Input  Token 3  Value\n Token 2\n Token 3  x  Query  Token 3\n Token 2  Value\n Token 3  Query  x  =\n =  Token 2  Token 2  Token 2  Token 2\n Input  Token 2  Token 2  Token 2\n Token 2  =  =  Token 2\n Key  Token 2\n Token 2  InputToken 2  x  x  Token 2  Input  Token 2\n Token 2  Token 2  =\n =  x  = Value\n x  x =  =  x  x  Token 2\n Query Token 2  Key  x  =\n x Input\u1d35\u207f\u1d56\u1d58\u1d57  =  Key  Key  Input  Input\n Token 3  Input  Input  Value\n Query  Input  Token 3  Token 3  Value  Value\n Query Query  Input  Input  Input  Key Key  Token 3  Input  Token 3\n Input  Token 3  Query  Key  Input  Value\n Token 3  Query  Token 3Input  Token 3  Input  Value Value\n Token 4  Token 4  Token 3\n Query  Token 4  Token 4\n Token 3  d x d\n Token 3  Token 4  d x d  Token 3  Token 4  Token 3d x d  Token 4\n Token 3  d x d  Token 3\n Token 4  Token 3  Token 3  Token 4  Token 3\n Token 4  Token 3  Token 3  Token 3  Token 3\n Input  Token 3  k InputToken 3  Key  Token 3  Token 3\n k  v\n Token 3  v  Input  Token 3\n Input  Query  Key  Value\n k  Token 4  Input  Input  Input  Key  Value  Input\n Query  Input  Token 4  Token 4  Value\n Input  Query  Input  d x d  Key Key  Token 4 Input  d x d  Token 4\n N x d\n Token 4  N x Input  Input  Value\n Token 4  Query  d  Value\n Token 4  Query  Token 4  Token    ~~4~~    d x d  Token 4  Token 4\n d x d  Token 4\n d x d  k  d x d  d x d\n Token 4  Token 4  Token 4  v  Token 4\n N x N x d  Token 4  N x d  k  N x d\n d  Token 4  N x d  k  Token 4  N x d\n Token 4  N x d\n d x d  d x d  k  Token 4\n k  Token 4  N x d Token 4  v\n k  d x d  v  Token 4  d  d x d  Token 4\n Token 4  x d  Token 4\n k  Token 4  v\n k  d x d  k  k  k  v  v\nk  N x d  d x d  k  N x d  N x d  v\n N x d\n N x d  N x d  k  N x d\n k  N x d  N x d\n N x d  N x d  N x d  k  N x d  N x d\n N x d  N x d  N x d  k  k  k  N x d  N x d  N x d  v v\n N x d  N x d  N x d  k  N x d\n k  N x d  N x d  k  N x d  N x d\n v\n k k  N x d  v  v\n\n KT  KT  T  T  T  T  T  T  V  A\n Q T QK  K  QK  masked  V  T  A  V  A\n QK  QK  masked\n T  Q  K  T T  QKT  QK  QKT  QK masked  V  A\n K  T  T  T  masked\n QQ  QK  K K  QK  QK  T  V  T  T  A  V  V  A  A\n masked  QK  masked\n q1  x  =  QK  QK  masked\n k1  k2  k3  k4  q1\u2022k1 q1\u2022k2 q1\u2022k3 q1\u2022k4  q1\u2022k1  \u2212\u221e  \u2212\u221e  \u2212\u221e  v1  a1\n q1\u2022k1\n =  =  \u2212\u221e  \u2212\u221e  \u2212\u221e  \u2212\u221e  q1\u2022k1\n k1   ~~k2~~    k3  k4 x  q1\u2022k1  =  \u2212\u221e  \u2212\u221e  v1  v1  a1  a1\n k2  k3  q1  q1\u2022k4\n k4  q1\u2022k1  q1\u2022k2 q1\u2022k3  q1\u2022k1\n q1\u2022k2 q1\u2022k3 q1\u2022k4  q1\u2022k1  q1\u2022k1\n q1\u2022k1  q1\u2022k1\n x  k2  k3  k4  q1\u2022k1  \u2212\u221e  \u2212\u221e  \u2212\u221e  v1  a1\n k1  q1\u2022k1  q1\u2022k4\n q1  =  q1\u2022k2 q1\u2022k3  q1\u2022k1\n x  q1\u2022k1\n = q1  k1  k2  k3   ~~k4~~    =  q1\u2022k1  \u2212\u221e  \u2212\u221e  \u2212\u221e  x v1  =  a1\n mask  q1\u2022k1 q1\u2022k2 q1\u2022k3 q1\u2022k4  \u2212\u221e  \u2212\u221e  \u2212\u221e\n \u2212\u221e  \u2212\u221e  \u2212\u221e  q1\u2022k1  v1  a1\n q1\u2022k1\n k1  k2  k3  k4  q1\u2022k1\n k1   k2  k3  k4  q2  q1\u2022k1 q1\u2022k2 q1\u2022k3 q1\u2022k4  v1 =  a1\n q1\u2022k1  q1\u2022k4  q1\u2022k1\n q1\u2022k2 q1\u2022k3  q1\u2022k1  q1\u2022k1\n q1\u2022k1  q1\u2022k1  \u2212\u221e   \u2212\u221e\n q1\u2022k1  q2\u2022k1 q2\u2022k2 q2\u2022k3 q2\u2022k4  x q2\u2022k1 q2\u2022k2  =  v2  a2\n mask  =  x  =  x  =\n q2  =  =  x  =\n mask  \u2212\u221e  \u2212\u221e  \u2212\u221e  x  =\n q2\u2022k1 q2\u2022k2 q2\u2022k3 q2\u2022k4  \u2212\u221e  v2  a2\n mask  q2q2\u2022k1 q2\u2022k2 q2\u2022k3 q2\u2022k4  q2\u2022k1 q2\u2022k1 q2\u2022k2  x =  v2 =\u2212\u221e  \u2212\u221e  a2  v2  a2\n q2\u2022k1 q2\u2022k2 q2\u2022k3 q2\u2022k4\n q2  q2\u2022k2  q2\u2022k1 q2\u2022k2  \u2212\u221e  \u2212\u221e\n q2\u2022k1 q2\u2022k2 q2\u2022k3 q2\u2022k4  =  v2  a2\n q3  =  q2\u2022k1 q2\u2022k2  \u2212\u221e  \u2212\u221e  v2  a2\n q2\u2022k1 q2\u2022k2 q2\u2022k3 q2\u2022k4  q2\u2022k1  \u2212\u221e  v3  a3\n \u2212\u221e  \u2212\u221e  q2\u2022k2\n q3\u2022k1 q3\u2022k2 q3\u2022k3 q3\u2022k4  v2  q3\u2022k1 q3\u2022k2a2\n q2\u2022k1 q2\u2022k2  q2\u2022k3 q2\u2022k4  q2\u2022k1 q2\u2022k2  q3\u2022k3\n q3  q3  q4  d  x N  \u2212\u221e  v3  a3\n q3\u2022k1 q3\u2022k2 q3\u2022k3 q3\u2022k4  \u2212\u221e  \u2212\u221e  v3  a3\n q3  q3\u2022k1 q3\u2022k2 q3\u2022k3 q3\u2022k4  q3\u2022k1 q3\u2022k2  v3  a3\n q3\u2022k1 q3\u2022k2 q3\u2022k3 q3\u2022k4  \u2212\u221e  q3\u2022k1  v3  q3\u2022k3  a3  v4  a4\n k  q3\u2022  q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4  q3\u2022k2 q3\u2022k3\n q3\u2022k1 q3\u2022k2 q3\u2022k3 q3\u2022k4  k1 q3\u2022k2 q3\u2022k3  q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4\n q3\u2022k1 q3\u2022k2 q3\u2022k3  \u2212\u221e  v3  a3\n q3\u2022k1 q3\u2022k2 q3\u2022k3 q3\u2022k4  q3\u2022k1 q3\u2022k2 q3\u2022k3\n q3\u2022k1 q3\u2022k2 q3\u2022k3 q3\u2022k4  q3\u2022k1 q3\u2022k2 q3\u2022k3  \u2212\u221e  v3  a3\n d  x d  x N\n q4  q4  N  N d  x N\n d  x d  x N  x d  v4  a4\n N  k  q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4  v4N x d  N x d\n q4  k  q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4  q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4  a4\n d  x N  k  k  N x N  q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4  v4  a4\n q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4  q4\u2022k1  N x N\n q4 v4  a4\n k  v4  \u2022k2 q4\u2022k3 q4\u2022k4  v  v\n k  q4\u2022k1 q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4  q4\u2022k1  a4\n q4\u2022k2 q4\u2022k3 q4\u2022k4  q4\u2022k1 q4\u2022k2 q4\u2022k2 q4\u2022k3 q4\u2022k4\n k  q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4  q4\u2022k1 q4\u2022k2  q4\u2022k3 q4\u2022k4  v4  a4\n q4\u2022k3 q4\u2022k4\n N x d  N x d  N x d  N x d\n N x d  N x N  N x d  N x d\n k  N x N  N x d  N x d\n k  k  N x N N x N  N x N  v  v\n N x N  v  v  v   v\n N x d  N x d  N x d\n N x N  N x d  N x dN x d\n N x N x N  N x N\n N  N x N x N  v  v\n N  v  v  v  v\n---\n    Parallelizing Multi-head Attention\n    9.4  \u2022  T HE INPUT: EMBEDDINGS FOR TOKEN AND POSITION\n\ne self-attention output A of shape [N \u00d7 d ].\n\n            Qi = XWQi ; Ki = XWKi ;         Vi = XWVi    (9.\n            i  i        i                   \u2713 Qi Ki | \u25c6  i\n    headi = SelfAttention(Q , K , V ) =     softmax  \u221adk  V  (9.\n\n         MultiHeadAttention(X) = (head1 \u2295 head2 ... \u2295 headh )WO (9.\n\n tting it all together with the parallel input matrix X  The function compu\nparallel by an entire layer of N transformer block over the entire N input tok\nn be expressed as:\n---\n xpressed as:\n  Putting it all together with the parallel input matrix X  The function computed\n     Parallelizing Multi-head Attention\n  in parallel by an entire layer of N transformer block over the entire N input tokens\n             O = LayerNorm(X + MultiHeadAttention(X))       (9.36)\n  can be expressed as:\n             H = LayerNorm(O + FFN(O))                      (9.37)\n\nan break     O = LayerNorm(X + MultiHeadAttention(X))                       (9.36)\n             it down with one equation for each component computation, using\nhape [N \u00d7 d ]) to H   = LayerNorm(O + FFN(O))                               (9.37)\n             stand for transformer and superscripts to demarcate each\n tation inside the block:\n  Or we can break it down with one equation for each component computation, using\n  T          or\n     (of shape [N \u00d7 d ]) to stand for transformer and superscripts to demarcate each\n               T\u00b9        = MultiHeadAttention(X)            (9.38)\n  computation inside the block:\n               T\u00b2        = X + T\u00b9                           (9.39)\n               T\u00b3        = LayerNorm(T\u00b2 )                   (9.40)\n                         T\u00b9    = MultiHeadAttention(X)                      (9.38)\n               T\u2074        = FFN(T\u00b3 )                         (9.41)\n                         T\u00b2    = X + T\u00b9                                     (9.39)\n               T\u2075        = T\u2074 + T\u00b3                          (9.42)\n                         T\u00b3    = LayerNorm(T\u00b2 )                             (9.40)\n               H = LayerNorm(T\u2075 )                           (9.43)\n                         T\u2074    = FFN(T\u00b3 )                                   (9.41)\n---\n Parallelizing Attention\n Computation\nTransformers\n---\n  Input and output: Position\n                 embeddings and the Language\nTransformers     Model Head\n---\nToken and Position Embeddings\n\nThe matrix X (of shape [N \u00d7 d]) has an embedding for\neach word in the context.\nThis embedding is created by adding two distinct\nembedding for each input\n\u2022     token embedding\n\u2022     positional embedding\n---\nToken Embeddings\n\nEmbedding matrix E has shape [|V | \u00d7 d ].\n\u2022     One row for each of the |V | tokens in the vocabulary.\n\u2022     Each word is a row vector of d dimensions\n\nGiven: string \"Thanks for all the\"\n1. Tokenize with BPE and convert into vocab indices\nw = [5,4000,10532,2224]\n2. Select the corresponding rows from E, each row an embedding\n\u2022     (row 5, row 4000, row 10532, row 2224).\n---\nPosition Embeddings\n\nThere are many methods, but we'll just describe the simplest: absolute\nposition.\nGoal: learn a position embedding matrix Epos of shape [1 \u00d7 N ].\nStart with randomly initialized embeddings\n\u2022  one for each integer up to some maximum length.\n\u2022  i.e., just as we have an embedding for token fish, we\u2019ll have an\n   embedding for position 3 and position 17.\n\u2022  As with word embeddings, these position embeddings are learned along\n   with other parameters during training.\n---\nEach x is just the sum of word and position embeddings\n\n                 Transformer Block\n\nX = Composite\nEmbeddings\n(word + position)\n                 +  +  +  +       +\nWord             Janet  will  back  the  bill\nEmbeddings\nPosition         1            2  3       4   5\nEmbeddings       Janet  will     back  the  bill\n---\n    Language modeling head\n\n    y1  y2                                    \u2026      y|V|  Word probabilities  1 x |V|\n\n    Language Model Head                                    Softmax over vocabulary V\n    takes hLN and outputs a    u1  u2         \u2026      u|V|  Logits  1 x |V|\n    distribution over vocabulary V    Unembedding          Unembedding layer   d x |V|\n\n                                      layer = E\u1d40\n\n    h\u1d38\u2081    h\u1d38\u2082                           h\u1d38N    1 x d\n  Layer L\nTransformer\n   Block                              \u2026\n\n    w1  w2    wN\n---\n  Language modeling head\n\n  Unembedding layer: linear layer projects from hLN      (shape [1 \u00d7 d])       to logit vector\n\n  y1  y2         \u2026 y|V|  Word probabilities  1 x |V|    Why \"unembedding\"? Tied to E\u1d40\n                         Softmax over vocabulary V\n  u1  u2         \u2026 u|V|  Logits  1 x |V|\n      Unembedding        Unembedding layer   d x |V|     Weight tying, we use the same weights for\n      layer = E\u1d40\n            h\u1d38N    1 x d                                 two different matrices\n\n\u2026                           Unembedding layer maps from an embedding to a\n                            1x|V| vector of logits\n            wN\n---\n     This linear layer can be learned, but more commonly we tie this matri\ntranspose of) the embedding matrix E.                          Recall that in weight tying, we\nsame Language modeling head\n     weights for two different matrices in the model. Thus at the input sta\ntransformer the embedding matrix (of shape [|V | \u00d7 d ]) is used to map from a\n                                                          Logits, the score vector u\nvector over the vocabulary (of shape [1 \u00d7 |V |]) to an embedding (of shape\n                                                          One score for each of the |V |\nAnd then in the language model head, E\u1d40 , the transpose of the embedding m\nshape [d \u00d7 |V |]) is used to map back possible words in the vocabulary V .\n     y1  y2  \u2026    y|V|  Word probabilities  1 x |V|       from an embedding (shape [1 \u00d7 d ]) to\nover the vocabulary (shape [1\u00d7|V                          Shape 1 \u00d7 |V |.\n                        Softmax over vocabulary V      |]). In the learning process, E will be opti\nbe good at \u2026\n             doing both of these mappings. We therefore sometimes call the t\n     u1  u2       u|V|  Logits  1 x |V|                        Softmax turns the logits into\nE\u1d40 the Unembedding\n         unembedding layer because it is performing this reverse mapping.\n         layer = E\u1d40     Unembedding layer   d x |V|            probabilities over vocabulary.\n     A softmax layer turns the logits u                        Shape 1 \u00d7 |V |.\n               h\u1d38N     1 x d                              into the probabilities y over the voca\n\n\u2026                                                      u  = h\u1d38 E\u1d40\n                                                          N\n               wN                                      y  = softmax(u)\n---\nThe final transformer  Token probabilities\n                       softmax\nmodel    Language\n         Modeling\n           Head\n\ny1  y2  \u2026 y|V|    wi+1\n                  Sample token to\n                  generate at position i+1\nlogits    u1  u2  \u2026  u|V|\n\n                  U\n\n    \u2026                                                                   hLi\n                                                                   feedforward\n                                                         Layer L    layer norm\n    Token probabilities    y1  y2                        y|V|       attention    wi+1\n                                                                    layer norm\n                                                                      \u2026 h\u1d38\u207b\u00b9\u2071 = x\u1d38\u2071  Sample token to\n                                                                        h2i = x3i\n\n    Language  softmax                                              feedforward  generate at position i+1\n                                                         Layer 2    layer norm\n    Modeling                                                        attention\n                           logits    u1  u2  \u2026 u|V|                 layer norm\n    Head                                                                h1i = x2i\n                                                                   feedforward\n                                             U           Layer 1    layer norm\n                                                                    attention\n                                                                    layer norm\n\n                                                  hLi     Input       + x1i i\n\n                                         feedforward     Encoding     E\n                                                         Input token  wi\n---\n  Input and output: Position\n                 embeddings and the Language\nTransformers     Model Head\n---\nLarge    Dealing with Scale\nLanguage\nModels\n---\nScaling Laws\n\nLLM performance depends on\n\u2022  Model size: the number of parameters not counting\n   embeddings\n\u2022  Dataset size: the amount of training data\n\u2022  Compute: Amount of compute (in FLOPS or etc\nCan improve a model by adding parameters (more layers,\nwider contexts), more data, or training for more iterations\nThe performance of a large language model (the loss) scales\nas a power-law with each of these three\n---\n or example, Kaplan et al. (2020) found the following three relationsh\n  as a function of the number of non-embedding parameters N , the datas\n    Scaling Laws\nd the compute budget C, for models training with limited parameters, d\n    Loss L as a function of # parameters N, dataset size D, compute budget C (if other\n  pute budget, if in each case the other two properties are held constant:\n    two are held constant)  L(N ) = \u2713 Nc \u25c6aN\n\n                                            \u2713 N \u25c6a\n    L(D) =                                  Dc    D\n                                            \u2713 D \u25c6a\n                           L(C) =           Cc  C\n                                            C\n he number of (non-embedding) parameters N can be roughly computed\n    Scaling laws can be used early in training to predict what the loss would be if we were\n    to add more data or increase model size.\n(ignoring biases, and with d as the input and output dimensionality\n---\nmber of (non-embedding) parameters    N can be roughly computed as\n    Number of non-embedding parameters N\nring biases, and with   d as the input and output dimensionality of\nttn as the self-attention layer size, and dff the size of the feedforward lay\n         N         \u21e1 2 d nlayer (2 dattn + dff )\n                   \u21e1 12 nlayer d 2                                            (10\n                   (assuming dattn = dff /4 = d )\n-3, with n = 96 layers and dimensionality d = 12288, has 12 \u21e5 9\n 175 billion parameters.\n    Thus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 \u00d7 96 \u00d7\n    122882 \u2248 175 billion parameters.\nlues of  Nc , Dc , Cc , aN , aD , and aC depend on the exact transfor\nre, tokenization, and vocabulary size, so rather than all the precise va\n                                      2\n---\n      KV Cache\n10.5.2  KV Cache\nWe saw in Fig. ?? and in Eq. ?? (repeated below) how the attention vector can be\nvery efficiently computed in parallel for training, via two matrix multiplications:\n      In training, we can compute attention very efficiently in parallel:\n                \u2713 QK| \u25c6\n                A = softmax  pdk  V                                      (10.13)\n\n      But not at inference! We generate the next tokens one at a time!\n     Unfortunately we can\u2019t do quite the same efficient computation in inference as\nin    For a new token x, need to multiply by WQ , W\u1d37, and W\u2c7d to get query, key,\n     training. That\u2019s because at inference time, we iteratively generate the next tokens\none at values\n      a time. For a new token that we have just generated, call it xi , we need to\n      But don't want to recompute the key and value vectors for all the prior\ncompute its query, key, and values by multiplying by WQ , WK , and WV respec-\ntively. tokens x\n      But it would be a waste of computation time to recompute the key and value\n             <i\n      Instead,  store key and value vectors in memory in the KV cache, and\nvectors for all the prior tokens x<i ; at prior steps we already computed these key\nand   then we can just grab them from the cache\n      value vectors! So instead of recomputing these, whenever we compute the key\nand value vectors we store them in memory in the KV cache, and then we can just\ngrab them from the cache when we need them. Fig. 10.7 modifies Fig. ?? to show\n---\n  N x d N x d\n  KV N x d\u1d4f  k\n  Cache\n\n  Q  KT KT\nQ  Q\n\n q1  q1 x  x  q1   x =\n  k1  k2  k3  k4\n  k1  k2  k3   k4\n\n q2  q2  mask  q2\n\n q3  q3  q3\n\n  d d  x N\n  x N\n q4  q4  k  q4\n  k\n\n x d  N x d\n  N x d  k\n  k  k\n\n    Q\n\n    x\n\n    q4\n\n    1 x dk\n\n    k1\n\n    N x d    k    N x d\n    N x d    N x d    k  N x d    N x d\n    N x d             k    N x d    N x d\n             k             N x d    v    v    v\n\n           T T T                         T T                                T\n           K  QK                         T                             V           V           A     A         V     A\n              QK                         QKQK        masked            QK          masked\n                                         QK   masked\n\n    =               =                    \u2212\u221e   \u2212\u221e     \u2212\u221e          \u2212\u221e         \u2212\u221e           \u2212\u221e  \u2212\u221e\n         k2   k3    k4                        \u2212\u221e     \u2212\u221e                v1          v1          a1     a1       v1    a1\n         q1\u2022k1            q1\u2022k4\n    q1\u2022k1         q1\u2022k2 q1\u2022k3     q1\u2022k1  q1\u2022k1       q1\u2022k4\n                    q1\u2022k4                q1\u2022k2 q1\u2022k3                   q1\u2022k1\n           q1\u2022k2 q1\u2022k3                 q1\u2022k1q1\u2022k1                      q1\u2022k1\n                                       q1\u2022k1q1\u2022k1                      q1\u2022k1\n                                       q1\u2022k1\n\n                                  =      =                       =x    x                 =  =        x               =\n                                                     \u2212\u221e \u2212\u221e       \u2212\u221e                      \u2212\u221e  \u2212\u221e\n                                  q2\u2022k1 q2\u2022k2 q2\u2022k3     \u2212\u221e                v2       v2          a2     a2       v2    a2\n    q2   q2\u2022k1 q2\u2022k2 q2\u2022k3 q2\u2022k4                        q2\u2022k4\n         \u2022k1 q2\u2022k2 q2\u2022k3 q2\u2022k4         q2\u2022k1q2\u2022k1 q2\u2022k2                q2\u2022k1 q2\u2022k2\n                                              q2\u2022k2\n\n                                                           \u2212\u221e    \u2212\u221e       v3       v3        \u2212\u221e       a3       v3    a3\n         q3\u2022k1 q3\u2022k2 q3\u2022k3 q3     q3\u2022k1 q3\u2022k2 q3\u2022k3 q3\u2022k4                                      a3\n    q3\u2022k1 q3\u2022k2 q3\u2022k3 q3\u2022k4       \u2022k4  q3\u2022k1q3\u2022k1 q3\u2022k2 q3\u2022k3          q3\u2022k1 q3\u2022k2 q3\u2022k3\n                                              q3\u2022k2 q3\u2022k3\n\n    dk x N                        q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4                 v4       v4          a4     a4       v4    a4\n    q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4                                            q4\u2022k1\n         q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4       q4\u2022k1q4\u2022k1                           q4\u2022k2 q4\u2022k3 q4\u2022k4\n                                              q4\u2022k2 q4\u2022k3 q4\u2022k4\n                                                     q4\u2022k2 q4\u2022k3 q4\u2022k4\n\n                                                                       N x d                N x d           N x d     N x d\n             N x N                            N x N                             N x d                N x d\n                  N x N                       N x N                             N x N\n                                                        N x N                   v        v           v      v        v\n\n    QKT    V    A\n    KT     v1\n\n    k1  k2  k3  k4  =    x                           v2    =\n\n                                                     v3\n        dk x N           q4\u2022k1 q4\u2022k2 q4\u2022k3 q4\u2022k4     v4     a4\n\n    1 x N    N x dv    1 x dv\n---\nLarge    Dealing with Scale\nLanguage\nModels\n\n"
    }
}