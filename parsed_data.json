{
    "LLMs.pdf": "Large       Introduction to Large Language\nLanguage    Models\nModels\n---\nLarge language models\n\nComputational agents that can interact\nconversationally with people using natural language\nLLMS have revolutionized the field of NLP and AI\n---\nLanguage models\n\n\u2022       Remember the simple n-gram language model\n   \u2022     Assigns probabilities to sequences of words\n   \u2022     Generate text by sampling possible next words\n   \u2022     Is trained on counts computed from lots of text\n\u2022       Large language models are similar and different:\n   \u2022     Assigns probabilities to sequences of words\n   \u2022     Generate text by sampling possible next words\n   \u2022     Are trained by learning to guess the next word\n---\n Fundamental intuition of large language models\n\nText contains enormous amounts of knowledge\nPretraining on lots of text with all that knowledge is\n what gives language models their ability to do so\n much\n---\nWhat does a model learn from pretraining?\n\n\u2022     With roses, dahlias, and peonies, I was\n      surrounded by flowers\n\u2022     The room wasn't just big it was enormous\n\u2022     The square root of 4 is 2\n\u2022     The author of \"A Room of One's Own\" is Virginia\n      Woolf\n\u2022     The doctor told me that he\n---\n    What is a large language model?\n    A neural network with:\n    Input: a context or prefix,\n    Output: a distribution over possible next words\n\n                                                   p(w|context)\n                               output              all     .44\n\n                                                the        .33\n                                               your        .15\n    Transformer (or other decoder)             that        .08\n\n input                                ?\ncontext  So  long  and    thanks  for\n---\nLLMs can generate!\n\nA model that gives a probability distribution over next words can generate\nby repeatedly sampling from the distribution\n\n                  p(w|context)\n                  output                  all    .44\n\n                            the                  .33\n                                         your    .15\nTransformer (or other decoder)    that           .08\n                                  \u2026               \u2026\n\nSo  long  and  thanks  for  all                  p(w|context)\n                                   output        the   .77\n\n                                                 your  .22\n                                              our      .07\n          Transformer (or other decoder)       of      .02\n                                                 \u2026     \u2026\n\n          So  long  and  thanks    for  all  the\n---\nThree architectures for large language models\nw w                                          w\n\nw w w w w\n\nw  w     w w w   w  w  w w w   w  w w\n\n   Decoder                        Encoder-Decoder\nDecoders         Encoders      Encoder-decoders\n                    Encoder\n\nGPT, Claude,     BERT family,  Flan-T5, Whisper\nLlama            HuBERT\nMixtral\n---\nDecoders                                    w w w w        w\n\n                                            w  w  w w      w  w\nWhat most people think of when we say LLM Decoder\n\u2022     GPT, Claude, Llama, DeepSeek, Mistral\n\u2022     A generative model\n\u2022     It takes as input a series of tokens, and iteratively\n      generates an output token one at a time.\n\u2022     Left to right (causal, autoregressive)\n---\nEncoders              w     w w w w\n\n\u2022  Masked Language Models (MLMs)\n                      w     w  w w w  w  w  w  w     w\n\u2022  BERT family              Decoder      Encoder\n\n\u2022  Trained by predicting words from surrounding\n   words on both sides\n\u2022  Are usually finetuned (trained on supervised data)\n   for classification tasks.\n---\nEncoder-Decoders    w w w\nw w  w  w w\n\n\u2022          w  w  w  w w  w  w  w  w w  w  w  w\n    Trained to map from one sequence to another\n\u2022   Very      Decoder       Encoder       Encoder-Decoder\n           popular for:\n   \u2022     machine translation (map from one language to\n         another)\n   \u2022     speech recognition (map from acoustics to words)\n---\nLarge       Introduction to Large Language\nLanguage    Models\nModels\n---\nLarge       Pretraining Large Language\nLanguage    Models\nModels\n---\n    Three stages of training in LLMs\n\n    Instruction Data    Preference Data\n\nPretraining    Label sentiment of this sentence:    Human: How can I embezzle money?\n    The movie wasn\u2019t that great\n    Data    Summarize: Hawaii Electric urges       Assistant: Embezzling is a\n        caution as crews replace a utility pole    felony, I can't help you\u2026\n             overnight on the highway from\u2026         Assistant: Start by creating\n\n    Translate English to Chinese:                   fake expense reports...\n    When does the flight arrive?\n\n    1. Pretraining    2. Instruction    3. Preference\n                         Tuning            Alignment\n\nPretrained    Instruction    Aligned LLM\n   LLM         Tuned LLM\n---\nPretraining\n\nThe big idea that underlies all the amazing\nperformance of language models\n\nFirst pretrain a transformer model on enormous\namounts of text\nThen apply it to new tasks.\n---\nSelf-supervised training algorithm\n\nWe train them to predict the next word!\n1. Take a corpus of text\n2. At each time step t\n i.      ask the model to predict the next word\n ii.     train the model using gradient descent to minimize the\n         error in this prediction\n\n \"Self-supervised\" because it just uses the next word as the\n label!\n---\nIntuition of language model training: loss\n\n\u2022         Same loss function: cross-entropy loss\n     \u2022     We want the model to assign a high probability to true\n           word w\n     \u2022     = want loss to be high if the model assigns too low a\n           probability to w\n\u2022         CE Loss: The negative log probability that the model\n          assigns to the true next word w\n     \u2022     If the model assigns too low a probability to w\n     \u2022     We move the model weights in the direction that assigns a\n           higher probability to w\n---\n                    L   = \u2212        y [w] log \u02c6\nl to minimize the   CE             t             yt [w]\n                    error in predicting the true next word in the training sequ\n       Cross                 w\u2208V\n                 -entropy loss for language modeling\ncross-entropy as the loss function.\necall that the cross-entropy loss measures the difference between a pred\n of language modeling, the correct distribution        yt comes from kn\nbility distribution and the correct distribution.\n       CE loss: difference between the correct probability distribution and the predicted\n . This is represented as a one-hot vector corresponding to the v\n       distribution       X\nentry for the actual next word is 1, and all the other entries are\n                       L  = \u2212      y [w] log \u02c6\nentropy loss           CE          t    yt [w]                                           (\n                       for language modeling is determined by the proba\n                               w\u2208V\n igns to the correct next word (all other words get multiplied by\n ase   The correct distribution y knows the next word, so is 1 for the actual next\n   of language modeling, the correct distribution y comes from knowin\nthe CE loss in (10.5)        t                    t\n       word and 0 for the   can be simplified as the negative log prob\nord. This is                others.\n                 represented as a one-hot vector corresponding to the vocabu\n       So in this sum, all terms get multiplied by zero except one: the logp the\n igns to the next word in the training sequence.\nthe entry for the actual next word is 1, and all the other entries are 0. T\n       model assigns to the correct next word, so:\n ss-entropy loss for language modeling is determined by the probability\n                       L    ( \u02c6                   \u02c6\nl assigns to the       CE    yt , yt ) = \u2212 log yt [wt +1 ]\n                       correct next word (all other words get multiplied by zero\n t the CE loss in (10.5) can be simplified as the negative log probabilit\n---\nTeacher forcing\n\n\u2022     At each token position t, model sees correct tokens w1:t,\n     \u2022  Computes loss (\u2013log probability) for the next token wt+1\n\u2022     At next token position t+1 we ignore what model predicted\n      for w\u209c\u208a\u2081\n     \u2022  Instead we take the correct word w\u209c\u208a\u2081, add it to context, move on\n---\nTraining a transformer language model\n\nTrue next token  long        and        thanks        for        all           \u2026\n\nCE Loss          \u2212log ylong  \u2212log yand  \u2212log ythanks  \u2212log yfor  \u2212log yall     \u2026\nper token\n\n                 \u0177  back     \u0177  back    \u0177  back       \u0177  back    \u0177  back\n                    prop        prop       prop          prop       prop\n\nLLM                                                                            \u2026\n\nInput tokens  So  long  and  thanks  for    \u2026\n---\n LLMs are mainly trained on the web\n\nCommon crawl, snapshots of the entire web produced by\n the non- profit Common Crawl with billions of pages\nColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156\n billion tokens of English, filtered\n What's in it? Mostly patent text documents, Wikipedia, and\n news sites\n---\nThe Pile: a pretraining corpus\nComposition of the Pile by Category\nacademics    web                   books\n             Academic Internet Prose DialogueMisc\n\n                                                                 Bibliotik\n\nPile-CC                                                          PG-19       BC2\n\nPubMed Central    ArXiv\n                                                                             Subtitles\n                                                                                      dialog\n\n                  PMA    StackExchange                           Github      IRC EP\nFreeLaw           USPTO  Phil  NIH OpenWebText2    Wikipedia     DM Math     HN  YT\n---\nFiltering for quality and safety\n\nQuality is subjective\n\u2022     Many LLMs attempt to match Wikipedia, books, particular\n      websites\n\u2022     Need to remove boilerplate, adult content\n\u2022     Deduplication at many levels (URLs, documents, even lines)\nSafety also subjective\n\u2022     Toxicity detection is important, although that has mixed results\n\u2022     Can mistakenly flag data written in dialects like African American\n      English\n---\n Reexamining \"Fair Use\" in the Age of\n AI There are problems with scraping from the web\n Generative Al claims to produce new language and images, but when those ideas are based on copyrighted\n material, who gets the credit? A new paper from Stanford University looks for answers.\n Jun 5, 2023 | Andrew Myersfin 0\n\n                                                                                                       Authors Sue OpenAI Claiming Mass Copyright\n                                                                                                       Infringement of Hundreds of Thousands of Novels\n\n The Times Sues OpenAI and Microsoft\nOver A.I. Use of Copyrighted Work\nMillions of articles from The New York Times were used to train\n chatbots that now compete with it, the lawsuit said.\n---\nThere are problems with scraping from the web\n\nCopyright: much of the text in these datasets is copyrighted\n\u2022     Not clear if fair use doctrine in US allows for this use\n\u2022     This remains an open legal question across the world\nData consent\n\u2022     Website owners can indicate they don't want their site crawled\nPrivacy:\n\u2022     Websites can contain private IP addresses and phone numbers\nSkew:\n\u2022     Training data is disproportionately generated by authors from the\n      US which probably skews resulting topics and opinions\n---\nLarge       Pretraining Large Language\nLanguage    Models\nModels\n---\nLarge       Evaluating Large Language\nLanguage    Models\nModels\n---\n   P(w ) = P(w )P(w |w )P(w |w  ) . . . P(w |w\n We\u2019ve been talking about predicting one word at a time, computing the proba\n       1:n    1  2  1  3  1:2               n  1:n\u2212\nof the next token w from the prior context: P(w |w ). But of course as we\n    Better        n\n              LMs are better at predicting text\n hapter 3 the     iY      i  <i\n                chain rule allows us to move between computing the probability\n next token and =       P(wi |w<i )\n                computing the probability of a whole text:\n    Reminder of the chain rule:\n       P(w         i=1\n                1:n ) = P(w1 )P(w2 |w1 )P(w3 |w1:2 ) . . . P(wn |w1:n\u22121 )\npute the probability of text just by multiplying the cond\n                        n\n ch                = Y P(wi |w<i )\n    token in the text. The resulting (log) likelihood of a\n                        i=1\n mparing how good two language models are on that text\ne can compute the probability of text just by multiplying the conditional pr\n        So given a text w1:n we could just compare the log likelihood from two LMs:\nities for each token in the text. The resulting (log) likelihood of a text is a us\n                                   n\n tric for comparing how good two language   Y\n        log likelihood(w          ) = models are on that text:\n                               1:n  log            P(wi |w<i )\n                                            n\n                  log likelihood(w  ) = log Y i=1\n                               1:n           P(wi |w<i )\n---\nBut raw log-likelihood has problems\n\nProbability depends on size of test set\n\u2022  Probability gets smaller the longer the text\n\u2022  We would prefer a metric that is per-word,\n   normalized by length\n---\nPerplexity is normalized for length\n\nPerplexity is the inverse probability of the test set,\nnormalized by the number of words\n (The inverse comes from the original definition of\n perplexity from cross-entropy rate in information theory)\n\n Probability range is [0,1], perplexity range is [1,\u221e]\n---\n          set), normalized by the test set length in tokens. For a test set of n toke\n    Perplexity\nAs we     perplexity is\n          first saw in Chapter 3, one way to evaluate language models is\n  well they predict unseen text. Intuitively, good models are those that\n                              Perplexity          (w ) =          P (w )\u2212 1\n    So just as for n-gram grammars, we use perplexity                     n\n                                                  1:n               to measure how\n robabilities to unseen data (are less     q                      q    1:n\n                                           surprised when encountering the\n    well the LM predicts unseen text                     = s           1\n    The perplexity of a model \u03b8 on an unseen test                 n\ntiate this intuition by using perplexity to measure set is the inverse\n                                                                       the quality of a\n    probability that \u03b8 assigns to the test set,                      Pq (w1:n )\n el. Recall from page   ?? that the perplexity of    normalized by the test\n    set length.                                                  a model q on an unseen\n erse probability that  q assigns to the test set, normalized by the test\n          To visualize how perplexity can be computed as a function of the proba\na test  For a test set of n tokens w       the perplexity is :\n        set of  n tokens w  , the perplexity is\n          LM computes for each new word, we can use the chain rule to expand th\n                            1:n    1:n\n          tion of probability of the test set: \u2212 1\n                Perplexity (w1:n ) = P (w1:n ) n\n                           q          q                          v\n                                     s                           u n\n                                           1                     uY       1\n                                   =                             t\n                                   Perplexity    ~~(w~~   ) =\n                                        n  q              1:n    n   P (w  ~~(10.7)~~  \n                                           P (w1:n )                 q    i |w<i )\n                                           q                        i=1\n---\nPerplexity\n\n\u2022     The higher the probability of the word sequence, the lower\n      the perplexity.\n\u2022     Thus the lower the perplexity of a model on the data, the\n      better the model.\n\u2022     Minimizing perplexity is the same as maximizing\n      probability\n\nAlso: perplexity is sensitive to length/tokenization so best used\nwhen comparing LMs that use the same tokenizer.\n---\nMany other factors that we evaluate, like:\n\nSize\nBig models take lots of GPUs and time to train, memory to store\nEnergy usage\nCan measure kWh or kilograms of CO2 emitted\nFairness\nBenchmarks measure gendered and racial stereotypes, or decreased\nperformance for language from or about some groups.\n\n"
}